[
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "18  Referencias",
    "section": "",
    "text": "Anderson, E. (1935). The irises of the Gaspe Peninsula. Bulletin of the American Iris Society, 59, 2–5.\nBaker, M. (2016). 1,500 scientists lift the lid on reproducibility. Nature, 533(7604), 452–454. https://doi.org/10.1038/533452a\nBelsley, D. A., Kuh, E., & Welsch, R. E. (1980). Regression Diagnostics: Identifying Influential Data and Sources of Collinearity. Wiley. https://doi.org/10.1002/0471725153\nBryan, J. (2018). Happy Git and GitHub for the useR. https://happygitwithr.com/\nChambers, J. (2008). Software for data analysis: Programming with R (1st ed.). Springer. https://doi.org/10.1007/978-0-387-75936-4\nCleveland, W. S. (1993). Visualizing Data. Hobart Press.\nCohen, J., Cohen, P., West, S. G., & Aiken, L. S. (2003). Applied multiple regression/correlation analysis for the behavioral sciences (3rd ed.). Lawrence Erlbaum Associates Publishers.\nCui, B. (2020). Automate Data Exploration and Treatment [R package DataExplorer version 0.8.2]. R-Project.org. https://cran.r-project.org/package=DataExplorer\nDraper, N. R., & Smith, H. (1998). Applied Regression Analysis (3rd ed.). Wiley. https://doi.org/10.1002/9781118625590\nField, A. P. (2013). Discovering statistics using IBM SPSS statistics: and sex and drugs and rock’n’roll (4th edition). Sage.\nFriendly, M. (2008). A Brief History of Data Visualization. In: Handbook of Data Visualization. Springer Handbooks Comp.Statistics. Springer, Berlin, Heidelberg. https://doi.org/10.1007/978-3-540-33037-0_2\nGentleman, R., & Temple Lang, D. (2007). Statistical analyses and reproducible research. Journal of Computational and Graphical Statistics, 16(1), 1–23. https://doi.org/10.1198/106186007X178663\nGrolemund, G., & Wickham, H. (2017). R for data science. O’Reilly Media. https://r4ds.had.co.nz/\nHernández, F., Usuga, O., & Mazo, M. (12 de agosto de 2024). Modelos de Regresión con R. Github.io. https://fhernanb.github.io/libro_regresion/\nHmelo-Silver, C. E., Duncan, R. G., & Chinn, C. A. (2007). Scaffolding and achievement in problem-based and inquiry learning: A response to Kirschner, Sweller, and Clark (2006). Educational Psychologist, 42(2), 99–107. https://doi.org/10.1080/00461520701263368\nIhaka, R., & Gentleman, R. (1996). R: A Language for Data Analysis and Graphics. Journal of Computational and Graphical Statistics, 5(3), 299–314. https://doi.org/10.1080/10618600.1996.10474713\nKolb, D. (1984). Experiential learning: Experience as the source of learning and development. Prentice Hall.\nKutner, M. H., Nachtsheim, C. J., Neter, J., & Li, W. (2005). Applied Linear Statistical Models (5th ed.). McGraw-Hill, Irwin, New York.\nLópez, E., & González, B. (2016). Diseño y análisis de experimentos. Internet Archive. https://archive.org/details/DiseoYAnlisisDeExperimentos2016\nMontgomery, D.C., Peck, E.A. and Vining, G.G. (2012) Introduction to Linear Regression Analysis. Vol. 821, John Wiley & Sons, Hoboken.\nMurrell, P. (2018). R Graphics (3rd ed.). Chapman and Hall/CRC. https://doi.org/10.1201/9780429422768\nNational Academies of Sciences, Engineering, and Medicine. (2019). Reproducibility and replicability in science. National Academies Press. https://doi.org/10.17226/25303\nR Core Team. (2023). R: A Language and Environment for Statistical Computing. R Foundation for Statistical Computing. https://www.r-project.org/\nRosales Castillo, J. M. (2005). Micropropagación de Calahuala Phlebodium psedoaureum (Cav.) Lellinger con tres tipos de explantes en diferentes medios de cultivo in vitro. Tesis Ing. Agr. Guatemala, Universidad de San Carlos de Guatemala, Facultad de Agronomía. 51 p.\nTabachnick, B., & Fidell, L. (2013). Using Multivariate Statistics (6th ed.). Boston, MA: Pearson.\nThe Turing Way Community. (2023). The Turing Way: A handbook for reproducible, ethical and collaborative research. https://the-turing-way.netlify.app\nTufte, E. (2001). The Visual Display of Quantitative Information (2nd ed.). Graphics Press.\nTukey, J. W. (1977). Exploratory Data Analysis. Addison-Wesley.\nVenables, W. N., & Ripley, B. D. (2002). Modern Applied Statistics with S (4th ed.). Springer. https://doi.org/10.1007/978-0-387-21706-2\nWickham, H. (2016). ggplot2: Elegant graphics for data analysis (2. ed.). Springer Cham. https://doi.org/10.1007/978-3-319-24277-4\nWilkinson, L. (2005). The grammar of graphics (2nd ed.). Springer. https://doi.org/10.1007/0-387-28695-0\nWilkinson, M. D. et al. (2016). The FAIR Guiding Principles for scientific data management and stewardship. Scientific Data 3, 160018. https://doi.org/10.1038/sdata.2016.18\nXie, Y., Allaire, J.J., & Grolemund, G. (2018). R Markdown: The Definitive Guide (1st ed.). Chapman and Hall/CRC. https://doi.org/10.1201/9781138359444",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Referencias</span>"
    ]
  },
  {
    "objectID": "01_cbasicos.html",
    "href": "01_cbasicos.html",
    "title": "1  Conceptos básicos de R",
    "section": "",
    "text": "1.1 ¿Qué es R?\nR es un lenguaje de programación y un entorno computacional especializado en el análisis estadístico, la visualización de datos y la investigación científica. Su desarrollo fue iniciado en 1996 por Ross Ihaka y Robert Gentleman, quienes lo concibieron como una herramienta flexible y robusta para realizar análisis reproducibles y generar visualizaciones de alta calidad (Ihaka & Gentleman, 1996). Desde su creación, R ha evolucionado hasta convertirse en una de las plataformas más utilizadas en los ámbitos científico, académico y profesional, gracias a su capacidad de adaptación, su naturaleza de código abierto y el respaldo de una comunidad global activa.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Conceptos básicos de R</span>"
    ]
  },
  {
    "objectID": "01_cbasicos.html#qué-es-r",
    "href": "01_cbasicos.html#qué-es-r",
    "title": "1  Conceptos básicos de R",
    "section": "",
    "text": "1.1.1 Características principales de R\nEl lenguaje R se distingue principalmente por su orientación al análisis estadístico y científico de datos, abarcando desde pruebas estadísticas básicas, como t de Student y análisis de varianza (ANOVA), hasta modelos avanzados de regresión y análisis multivariado. Una de sus fortalezas más reconocidas es la capacidad de generar visualizaciones de datos de alta calidad mediante paquetes especializados, como ggplot2, que permiten explorar, interpretar y comunicar patrones complejos de manera efectiva (Wickham, 2016).\nR es un software de código abierto, lo que significa que es gratuito y su desarrollo es impulsado por una comunidad internacional de usuarios y desarrolladores. Esta característica fomenta la colaboración, la transparencia y la mejora continua del entorno. La extensibilidad de R es notable, ya que cuenta con un repositorio central (CRAN) que, hasta 2023, alberga más de 19,000 paquetes, los cuales amplían sus capacidades para abordar tareas especializadas como análisis genómico, minería de texto, modelado espacial, entre otros (R Core Team, 2023).\nOtra característica fundamental de R es su contribución a la reproducibilidad científica. El uso de scripts y cuadernos de trabajo permite documentar cada paso del análisis, facilitando la replicación y verificación de resultados por parte de otros investigadores (Peng, 2011). Además, R es altamente interoperable, permitiendo la integración con otros lenguajes de programación como Python, C++ y SQL, así como la importación y exportación de datos en múltiples formatos, incluyendo CSV, Excel, JSON y bases de datos relacionales (R Core Team, 2023).\n\n\n1.1.2 ¿Por qué es especial R?\nR trasciende su función como herramienta de cálculo estadístico, constituyéndose en un entorno integral para la manipulación, análisis y visualización de datos, así como para la automatización de flujos de trabajo analíticos. Su flexibilidad y capacidad de personalización lo convierten en una opción preferente para investigadores, analistas y profesionales de diversas disciplinas, quienes pueden adaptar el entorno a sus necesidades específicas (Grolemund & Wickham, 2017).\nLa vitalidad de la comunidad de usuarios y desarrolladores de R es un factor clave en su evolución. Esta comunidad no solo contribuye al desarrollo de nuevos paquetes y recursos, sino que también promueve la difusión de buenas prácticas y la formación continua, asegurando que R se mantenga a la vanguardia en el análisis de datos y análisis estadístico (R Core Team, 2023).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Conceptos básicos de R</span>"
    ]
  },
  {
    "objectID": "01_cbasicos.html#qué-es-rstudio",
    "href": "01_cbasicos.html#qué-es-rstudio",
    "title": "1  Conceptos básicos de R",
    "section": "1.2 ¿Qué es RStudio?",
    "text": "1.2 ¿Qué es RStudio?\nRStudio es un Entorno de Desarrollo Integrado (IDE) diseñado para optimizar el trabajo con el lenguaje de programación R. Este entorno proporciona una interfaz intuitiva y organizada, compuesta por paneles que facilitan el acceso a las principales herramientas y funciones necesarias para el análisis estadístico y la visualización de datos. La estructura de RStudio permite a los usuarios gestionar de manera eficiente sus proyectos y recursos, promoviendo buenas prácticas en la organización y documentación del trabajo analítico (Xie et al., 2018).\n\n\n\n\n\n\n1.2.1 Características principales de RStudio\nEntre las características más destacadas de RStudio se encuentra su sistema de gestión de proyectos, el cual permite organizar archivos, scripts y conjuntos de datos en directorios independientes, favoreciendo la estructura y la reproducibilidad de los análisis. Esta funcionalidad resulta fundamental para mantener el orden y facilitar la colaboración en equipos de trabajo, ya que cada proyecto puede configurarse con su propio entorno y dependencias, lo que minimiza errores y mejora la trazabilidad de los procesos analíticos (Xie et al., 2018).\nRStudio admite una amplia gama de formatos de datos, incluyendo CSV, Excel, HTML y bases de datos SQL, lo que facilita la importación y exportación de información desde diversas fuentes. Además, el entorno soporta la creación de gráficos interactivos y aplicaciones web mediante paquetes como shiny y plotly, ampliando las posibilidades de visualización y comunicación de resultados. La integración con paquetes de R es directa y eficiente, permitiendo instalar, actualizar y gestionar extensiones como ggplot2, dplyr y tidyr, lo que incrementa significativamente las capacidades analíticas del entorno (Xie et al., 2018; Wickham, 2016).\nEste IDE es compatible con los principales sistemas operativos, como Windows, macOS y Linux, lo que garantiza su accesibilidad para una amplia variedad de usuarios. Asimismo, RStudio ofrece opciones de personalización, permitiendo modificar la apariencia, los atajos de teclado y la disposición de los paneles, así como integrar herramientas externas como Git para el control de versiones y la gestión colaborativa de proyectos (Xie et al., 2018).\n\n\n1.2.2 Beneficios de usar RStudio\nEl uso de RStudio proporciona ventajas sustanciales en el proceso de análisis estadístico de datos. Este entorno incrementa la eficiencia al facilitar la organización y agilizar las tareas analíticas, y promueve la reproducibilidad mediante herramientas como R Markdown y el sistema de proyectos. Su interfaz gráfica resulta accesible tanto para usuarios principiantes como avanzados, permitiendo trabajar con datos, gráficos, modelos estadísticos y aplicaciones interactivas en un solo entorno. La flexibilidad de RStudio lo convierte en una herramienta adaptable a diversas necesidades analíticas, consolidándose como la opción preferente para quienes buscan un entorno de trabajo integral y eficiente (Xie et al., 2018).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Conceptos básicos de R</span>"
    ]
  },
  {
    "objectID": "01_cbasicos.html#reproducibilidad-y-replicabilidad-en-la-investigación-científica",
    "href": "01_cbasicos.html#reproducibilidad-y-replicabilidad-en-la-investigación-científica",
    "title": "1  Conceptos básicos de R",
    "section": "1.3 Reproducibilidad y replicabilidad en la investigación científica",
    "text": "1.3 Reproducibilidad y replicabilidad en la investigación científica\nLa reproducibilidad y la replicabilidad son pilares fundamentales en la investigación científica contemporánea, ya que garantizan la validez y la confiabilidad de los hallazgos. Numerosos estudios han evidenciado que una proporción considerable de investigadores experimenta dificultades para replicar experimentos previos, principalmente debido a la insuficiente documentación de los procedimientos y análisis originales (Baker, 2016). El uso de herramientas tradicionales como Excel o Infostat puede limitar la transparencia, ya que parte de los cálculos se realiza de manera interna y los gráficos suelen requerir modificaciones manuales, lo que dificulta la reproducción exacta de los resultados y la verificación independiente de los análisis.\n\n1.3.1 El papel de R en la reproducibilidad\nR se caracteriza por su capacidad para documentar de manera precisa y estructurada cada etapa del análisis a través de scripts, lo que permite que los procedimientos sean replicados y reinterpretados en el futuro, incrementando la transparencia y la credibilidad científica (Gentleman & Temple Lang, 2007). Esta documentación detallada facilita la reutilización de métodos en nuevos estudios, optimizando tanto el tiempo como los recursos disponibles. Un script en R puede ser comparado con una receta detallada, en la que cada paso está claramente especificado y puede adaptarse a diferentes conjuntos de datos según las necesidades del análisis, permitiendo así la replicación exacta o la adaptación a nuevos contextos (Xie et al., 2018).\n\n\n\n\n\n\n\n1.3.2 Definición y características de la reproducibilidad\nLa reproducibilidad se define como la capacidad de obtener los mismos resultados utilizando los mismos datos y métodos empleados en el análisis original. Este principio es esencial para la verificación y validación de los hallazgos científicos, ya que permite que otros investigadores, o el propio autor, puedan replicar los resultados siempre que dispongan de los datos y procedimientos originales (National Academies of Sciences, Engineering, and Medicine, 2019). Para alcanzar la reproducibilidad, es imprescindible contar con acceso a los datos originales y una documentación exhaustiva de los métodos utilizados, asegurando que los resultados sean consistentes al repetir el análisis. La reproducibilidad fomenta la transparencia, facilita la verificación de los resultados y promueve la colaboración científica, ya que otros investigadores pueden comprender y construir sobre el trabajo existente (Wilkinson et al., 2016).\n\n\n1.3.3 Definición y características de la replicabilidad\nPor otro lado, la replicabilidad se refiere a la obtención de resultados consistentes al realizar un estudio similar en un contexto diferente, utilizando nuevos datos o métodos ajustados. Este concepto evalúa la capacidad de generalización de los hallazgos y su aplicabilidad en distintos escenarios, lo que resulta fundamental para validar la robustez de los resultados científicos (National Academies of Sciences, Engineering, and Medicine, 2019). La replicabilidad implica el uso de datos diferentes, la adaptación de los métodos y la obtención de resultados coherentes con los del estudio original, aunque no necesariamente idénticos. Este proceso permite evaluar la generalización de los resultados, refuerza la credibilidad científica y facilita la extensión del conocimiento a nuevas aplicaciones o contextos (The Turing Way Community, 2023).\n\n\n1.3.4 Beneficios de utilizar R para la ciencia reproducible\nEl uso de R en la investigación científica ofrece ventajas significativas para la reproducibilidad y la replicabilidad. El código generado en R es accesible para la revisión por pares, lo que incrementa la transparencia de los análisis y permite la identificación de posibles errores o mejoras (The Turing Way Community, 2023). Además, los métodos desarrollados pueden ser reutilizados y adaptados en nuevos estudios, optimizando recursos y tiempo (Gentleman & Temple Lang, 2007). R también facilita el cumplimiento de los principios FAIR (Findable, Accessible, Interoperable, Reusable), promoviendo una gestión adecuada y responsable de los datos científicos, lo que contribuye a la apertura y reutilización de los resultados de investigación (Wilkinson et al., 2016).",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Conceptos básicos de R</span>"
    ]
  },
  {
    "objectID": "02_instalacion_confi.html",
    "href": "02_instalacion_confi.html",
    "title": "2  Instalación y configuración",
    "section": "",
    "text": "2.1 Descarga de R y RStudio\nAntes de comenzar a trabajar con R y RStudio, es fundamental realizar la instalación y configuración de ambos programas. R es un lenguaje de programación y entorno computacional ampliamente utilizado en el análisis estadístico, la visualización de datos y la investigación reproducible (Ihaka & Gentleman, 1996). Por su parte, RStudio es un Entorno de Desarrollo Integrado (IDE, por sus siglas en inglés) diseñado específicamente para trabajar con R, proporcionando una interfaz amigable y herramientas avanzadas que optimizan el flujo de trabajo (Xie et al., 2018). Este capítulo detalla los pasos necesarios para descargar, instalar y configurar ambos programas, asegurando un entorno de trabajo funcional y eficiente.\nPara utilizar R y RStudio, es necesario descargar ambos programas de sus sitios oficiales. R proporciona el núcleo del lenguaje y las herramientas computacionales fundamentales, mientras que RStudio actúa como una interfaz que simplifica el uso de R y mejora la experiencia del usuario, integrando funciones para la gestión de proyectos, edición de scripts y visualización de resultados (Xie et al., 2018).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Instalación y configuración</span>"
    ]
  },
  {
    "objectID": "02_instalacion_confi.html#descarga-de-r-y-rstudio",
    "href": "02_instalacion_confi.html#descarga-de-r-y-rstudio",
    "title": "2  Instalación y configuración",
    "section": "",
    "text": "2.1.1 Descarga de R\nSe recomienda descargar una versión estable de R para evitar posibles incompatibilidades con paquetes que aún no han sido actualizados para las versiones más recientes. Por ejemplo, la versión R 4.4.2 es reconocida por su estabilidad y amplio soporte dentro de la comunidad de usuarios (R Core Team, 2023). El repositorio oficial de R se encuentra disponible en https://cran.r-project.org/bin/windows/base/old/, donde es posible acceder a todas las versiones publicadas. Para descargar una versión específica, se debe seleccionar el nombre de la versión deseada y hacer clic en el archivo con terminación -win.exe, lo que iniciará la descarga del instalador correspondiente (R Core Team, 2023).\n\n\n2.1.2 Descarga de RStudio\nLa descarga de RStudio se realiza desde su página oficial, donde se encuentra disponible la versión más reciente para los principales sistemas operativos. Para usuarios de Windows, se debe seleccionar la opción “Download RStudio Desktop for Windows”, mientras que para quienes utilizan macOS o Linux, la misma página ofrece las versiones correspondientes para estos sistemas (Xie et al., 2018). Es importante asegurarse de descargar la versión adecuada según el sistema operativo del equipo para garantizar la compatibilidad y el correcto funcionamiento del entorno.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Instalación y configuración</span>"
    ]
  },
  {
    "objectID": "02_instalacion_confi.html#instalación-de-r-y-rstudio",
    "href": "02_instalacion_confi.html#instalación-de-r-y-rstudio",
    "title": "2  Instalación y configuración",
    "section": "2.2 Instalación de R y RStudio",
    "text": "2.2 Instalación de R y RStudio\nLa instalación de R y RStudio debe realizarse siguiendo un orden específico para evitar conflictos y asegurar que ambos programas funcionen correctamente. A continuación, se describen los pasos detallados para cada uno:\n\n2.2.1 Instalación de R\nUna vez descargado el instalador de R, se debe ejecutar el archivo .exe y seguir las instrucciones proporcionadas por el asistente de instalación. En la mayoría de los casos, es suficiente con aceptar las configuraciones predeterminadas, a menos que se requiera una configuración personalizada para necesidades específicas del usuario o del proyecto (R Core Team, 2023).\n\n\n2.2.2 Instalación de RStudio\nDespués de instalar R, se procede a ejecutar el instalador de RStudio previamente descargado. Al igual que en el caso de R, se pueden aceptar las opciones predeterminadas durante la instalación. Es relevante destacar que RStudio permite gestionar múltiples versiones de R en un mismo dispositivo, lo que resulta especialmente útil para trabajar en proyectos que requieren versiones específicas del lenguaje. Esta selección puede realizarse desde la configuración de RStudio, facilitando así la administración de entornos de trabajo diferenciados (Xie et al., 2018; R Core Team, 2023).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Instalación y configuración</span>"
    ]
  },
  {
    "objectID": "02_instalacion_confi.html#configuración-inicial",
    "href": "02_instalacion_confi.html#configuración-inicial",
    "title": "2  Instalación y configuración",
    "section": "2.3 Configuración inicial",
    "text": "2.3 Configuración inicial\nTras completar la instalación de R y RStudio, es recomendable realizar una configuración inicial que permita personalizar el entorno de trabajo, mejorar la organización y facilitar el desarrollo de análisis estadísticos. Estas configuraciones contribuyen a optimizar la experiencia del usuario y a establecer un flujo de trabajo más eficiente y productivo (Xie et al., 2018). A continuación, se describen los pasos esenciales para configurar RStudio de manera adecuada.\n\n2.3.1 Seleccionar la versión de R\nRStudio permite elegir la versión de R que se utilizará, lo cual es especialmente útil si se tienen múltiples versiones instaladas en el mismo dispositivo. Esta funcionalidad garantiza la compatibilidad con proyectos que requieren versiones específicas del lenguaje (R Core Team, 2023). Para seleccionar la versión de R en RStudio, se deben seguir estos pasos:\n\nIr al menú Tools y seleccionar Global Options.\nEn la ventana emergente, dirigirse a la pestaña General.\nEn el apartado R version, elegir la versión deseada de R.\n\n\n\n2.3.2 Configurar la apariencia de RStudio\nRStudio ofrece opciones de personalización para adaptar su apariencia a las preferencias del usuario, lo que puede mejorar la experiencia de trabajo y reducir la fatiga visual durante sesiones prolongadas (Xie et al., 2018). Para cambiar el tema de la interfaz y ajustar la fuente, se deben seguir los siguientes pasos:\n\nAcceder al menú Tools y seleccionar Global Options.\nEn la ventana emergente, ir a la pestaña Appearance.\nElegir el tema preferido, ya sea claro u oscuro (por ejemplo, el tema Cobalt para reducir la fatiga visual).\nAjustar el tamaño y el tipo de fuente según las preferencias personales.\n\n\n\n2.3.3 Configurar el panel de trabajo\nLa interfaz de RStudio está organizada en cuatro paneles principales: editor de scripts, consola, entorno/archivos y gráficos/ayuda. Estos paneles pueden reorganizarse para optimizar el flujo de trabajo. Para modificar la disposición de los paneles, se deben seguir estos pasos:\n\nIr al menú Tools y seleccionar Global Options.\nAcceder a la sección Pane Layout.\nAjustar la ubicación de los paneles según las necesidades, por ejemplo, colocando el editor de scripts en la parte superior izquierda y la consola en la parte inferior.\nGuardar los cambios para aplicar la nueva disposición.\n\n\n\n2.3.4 Habilitar el número de líneas en el editor de scripts\nLa numeración de líneas en el editor de scripts facilita la navegación y depuración del código. Para habilitar esta opción, se deben seguir los siguientes pasos:\n\nAcceder al menú Tools y seleccionar Global Options.\nIr a la pestaña Code y luego a Display.\nMarcar la casilla Show line numbers para activar la numeración de líneas.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Instalación y configuración</span>"
    ]
  },
  {
    "objectID": "02_instalacion_confi.html#organización-de-proyectos",
    "href": "02_instalacion_confi.html#organización-de-proyectos",
    "title": "2  Instalación y configuración",
    "section": "2.4 Organización de proyectos",
    "text": "2.4 Organización de proyectos\nLa organización adecuada de proyectos en RStudio es esencial para establecer un flujo de trabajo eficiente, reproducible y estructurado. Una gestión ordenada de archivos y scripts no solo facilita el desarrollo de los análisis, sino que también mejora la colaboración y la reproducibilidad de los resultados (Xie et al., 2018).\n\n2.4.1 Crear un proyecto en RStudio\nPara organizar los archivos, datos y scripts de un análisis específico, RStudio permite crear proyectos siguiendo estos pasos:\n\nEn la barra de menú, seleccionar File &gt; New Project.\nElegir una de las siguientes opciones:\n\nNew Directory: para crear un proyecto desde cero en una nueva carpeta.\nExisting Directory: para convertir una carpeta existente en un proyecto de RStudio.\nVersion Control: para clonar un repositorio de Git y trabajar en un proyecto con control de versiones.\n\nConfigurar el nombre y la ubicación del proyecto según las necesidades del análisis.\nHacer clic en Create Project para finalizar la configuración.\n\nEl uso de proyectos en RStudio permite mantener una estructura clara y organizada, facilitando la gestión de los recursos necesarios para el análisis y promoviendo la reproducibilidad (Xie et al., 2018).\n\n\n2.4.2 Establecer un directorio de trabajo\nEl directorio de trabajo es la carpeta donde R buscará los archivos y guardará los resultados generados durante el análisis. Para establecerlo manualmente, se puede utilizar la función setwd(), como se muestra a continuación:\n\n# Establecer directorio de trabajo\nsetwd(\"ruta/del/directorio\")\n\nSin embargo, al trabajar con proyectos en RStudio, el directorio de trabajo se configura automáticamente al abrir el archivo del proyecto, lo que elimina la necesidad de establecerlo manualmente y reduce errores relacionados con rutas incorrectas (R Core Team, 2023).\n\n\n2.4.3 Uso de archivos .Rproj\nEl archivo .Rproj es el elemento central de cada proyecto en RStudio. Este archivo almacena las configuraciones específicas del proyecto, como el directorio de trabajo, las opciones de visualización y otros ajustes personalizados. Al abrir un archivo .Rproj, se carga automáticamente el entorno de trabajo asociado, lo que facilita la continuidad y la gestión del análisis (Xie et al., 2018).\n\n\n2.4.4 Beneficios de la organización de proyectos\nLa correcta organización de proyectos en RStudio ofrece varios beneficios clave:\n\nReproducibilidad: Facilita que otros usuarios (o el propio usuario en el futuro) comprendan y reproduzcan el análisis, asegurando que los resultados sean consistentes.\nEficiencia: Reduce el tiempo perdido buscando archivos o configurando rutas manualmente, permitiendo un enfoque más directo en el análisis.\nColaboración: Mejora la comunicación y el trabajo en equipo al mantener una estructura clara y consistente, especialmente en proyectos compartidos.\nOptimización del flujo de trabajo: La combinación de una apariencia personalizada y una estructura organizada permite al usuario enfocarse en el análisis de datos de manera más eficiente y profesional (Xie et al., 2018).",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Instalación y configuración</span>"
    ]
  },
  {
    "objectID": "03_inicio.html",
    "href": "03_inicio.html",
    "title": "3  Primeros pasos en R",
    "section": "",
    "text": "3.1 Creación de scripts en RStudio\nIniciar el trabajo en R y RStudio puede resultar desafiante para quienes no están familiarizados con estos entornos, pero una orientación adecuada facilita considerablemente el proceso. Esta sección guía al usuario en los aspectos fundamentales para comenzar a programar en R, desde la creación de scripts hasta la comprensión de los objetos básicos del lenguaje. Estos conocimientos iniciales son esenciales para establecer un flujo de trabajo eficiente y reproducible (Xie et al., 2018).\nEl script es el archivo principal donde se escribe, guarda y ejecuta el código en R. Utilizar scripts no solo permite desarrollar análisis de datos, sino también documentar cada paso del proceso, lo que contribuye a la reproducibilidad y la organización del trabajo (Xie et al., 2018). Para crear un script en RStudio, se pueden seguir los siguientes métodos:\nUna vez creado, el script se convierte en el espacio central para el desarrollo de los análisis. Es recomendable guardar el archivo desde el inicio, utilizando File &gt; Save o el atajo Ctrl + S, para evitar la pérdida de información y facilitar la gestión de versiones (Xie et al., 2018).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Primeros pasos en R</span>"
    ]
  },
  {
    "objectID": "03_inicio.html#creación-de-scripts-en-rstudio",
    "href": "03_inicio.html#creación-de-scripts-en-rstudio",
    "title": "3  Primeros pasos en R",
    "section": "",
    "text": "Desde la barra de menú, seleccionar File &gt; New File &gt; R Script.\nUtilizar el atajo de teclado Ctrl + Shift + N para abrir un nuevo script de manera rápida.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Primeros pasos en R</span>"
    ]
  },
  {
    "objectID": "03_inicio.html#guardado-y-organización-de-archivos",
    "href": "03_inicio.html#guardado-y-organización-de-archivos",
    "title": "3  Primeros pasos en R",
    "section": "3.2 Guardado y organización de archivos",
    "text": "3.2 Guardado y organización de archivos\nUna gestión adecuada de los archivos es esencial para mantener la eficiencia y la reproducibilidad en el trabajo con R y RStudio. El uso de nombres descriptivos, la organización en carpetas y la documentación clara de los scripts contribuyen a un entorno de trabajo ordenado y profesional (Xie et al., 2018).\n\n3.2.1 Guardado de scripts y archivos\nPara guardar un script en RStudio, se deben seguir estos pasos:\n\nSeleccionar la opción File &gt; Save As… en la barra de menú.\nDefinir la ubicación y el nombre del archivo en la ventana emergente.\nUtilizar nombres que reflejen el contenido y propósito del archivo, por ejemplo: analisis_rendimiento.R para scripts o datos_suelo_2023.csv para archivos de datos.\nEvitar espacios y caracteres especiales en los nombres, empleando guiones bajos (_) o guiones medios (-) para separar palabras.\nIncluir fechas en formato estándar (YYYY-MM-DD) para facilitar la identificación de versiones, como en 2023-10-15_importacion_datos.R.\n\nEstas prácticas previenen errores en la ejecución del código y facilitan la gestión de versiones y actualizaciones (Xie et al., 2018).\n\n\n3.2.2 Organización de directorios y proyectos\nLa estructura de carpetas es clave para mantener el orden en los proyectos. Para organizar los archivos de manera eficiente, se recomienda:\n\nCrear una carpeta específica para cada proyecto, agrupando en ella todos los scripts, datos y resultados relacionados.\nUtilizar archivos de proyecto .Rproj, ya que RStudio configura automáticamente el directorio de trabajo al abrir el proyecto, simplificando la gestión de archivos y reduciendo errores asociados a rutas incorrectas (R Core Team, 2023).\n\n\n\n3.2.3 Buenas prácticas para la organización de archivos\nPara optimizar la organización y facilitar la colaboración, se aconseja:\n\nEstandarizar los nombres de archivos, siguiendo un formato uniforme y descriptivo.\nDocumentar los pasos del análisis mediante comentarios claros en los scripts, lo que ayuda a comprender y reproducir el trabajo en el futuro.\nUtilizar proyectos de RStudio (.Rproj) para asegurar que el entorno de trabajo esté correctamente configurado y todos los archivos relevantes se encuentren en la misma ubicación.\nRealizar copias de seguridad periódicas, ya sea mediante sistemas de control de versiones como Git o almacenando archivos importantes en ubicaciones seguras.\n\nLa aplicación de estas prácticas contribuye a un flujo de trabajo más eficiente, facilita la colaboración y asegura la reproducibilidad de los análisis realizados en RStudio (Xie et al., 2018; R Core Team, 2023).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Primeros pasos en R</span>"
    ]
  },
  {
    "objectID": "03_inicio.html#introducción-a-los-objetos-en-r",
    "href": "03_inicio.html#introducción-a-los-objetos-en-r",
    "title": "3  Primeros pasos en R",
    "section": "3.3 Introducción a los objetos en R",
    "text": "3.3 Introducción a los objetos en R\nEn R, la manipulación y el análisis de datos se basan en el uso de objetos, que son entidades capaces de almacenar información de diversos tipos. Cada objeto tiene un nombre, un tipo de dato y, en algunos casos, dimensiones o atributos adicionales. Esta estructura flexible permite organizar y gestionar datos de manera eficiente, lo que convierte a los objetos en el elemento central del trabajo en R (R Core Team, 2023).\n\n3.3.1 Creación de objetos en R\nPara crear un objeto en R, se utiliza un operador de asignación. Aunque existen dos opciones (= y &lt;-), la convención más extendida y recomendada es el uso de &lt;-, ya que mejora la legibilidad y sigue las normas del lenguaje (Ihaka & Gentleman, 1996). El valor o expresión a la derecha del operador se asigna al nombre del objeto a la izquierda.\n\n# Asignación de un valor numérico a un objeto\nx &lt;- 10  # El objeto x almacena el valor 10\n\n# Asignación de un texto a un objeto\nnombre &lt;- \"Ana\"  # El objeto nombre almacena la cadena de texto \"Ana\"\n\nEste método facilita la organización y claridad del código, permitiendo reutilizar y modificar los valores almacenados en los objetos según las necesidades del análisis.\n\n\n3.3.2 Buenas prácticas y documentación\nLa claridad en el código es fundamental para la reproducibilidad y el mantenimiento de los análisis. En R, los comentarios se introducen utilizando el símbolo numeral #. Los comentarios no son ejecutados por el intérprete y sirven para explicar el propósito de cada línea o bloque de código, facilitando la comprensión tanto para el autor como para otros usuarios. Por ejemplo:\n\n# Este es un comentario que explica la siguiente línea\ny &lt;- 5  # Asigna el valor 5 al objeto y\n\nSe recomienda documentar los scripts de manera clara y consistente, describiendo los pasos principales y las decisiones tomadas durante el análisis. Esta práctica es esencial para la colaboración y para la revisión futura del trabajo (Ihaka & Gentleman, 1996).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Primeros pasos en R</span>"
    ]
  },
  {
    "objectID": "03_inicio.html#tipos-principales-de-objetos-en-r",
    "href": "03_inicio.html#tipos-principales-de-objetos-en-r",
    "title": "3  Primeros pasos en R",
    "section": "3.4 Tipos principales de objetos en R",
    "text": "3.4 Tipos principales de objetos en R\nR permite trabajar con diferentes tipos de objetos, cada uno diseñado para almacenar y manipular distintos tipos de datos. Los más comunes son los siguientes (R Core Team, 2023):\n\n3.4.1 Objetos Numéricos\nLos objetos numéricos son fundamentales en R y pueden almacenar tanto números enteros como decimales (también llamados de punto flotante). Existen dos subtipos principales:\n\nNúmeros enteros (integer): Almacenan valores sin decimales\nNúmeros de punto flotante (double): Almacenan valores con decimales\n\nEjemplo de creación de objetos numéricos:\n\n# Creación de objetos numéricos\nedad &lt;- 21        # Número entero\naltura_m &lt;- 1.70  # Número decimal (punto flotante)\npeso_lb &lt;- 145    # Número entero\n\n# Exploración de objetos numéricos\nclass(edad)      # Muestra \"numeric\"\n\n[1] \"numeric\"\n\ntypeof(altura_m)  # Muestra \"double\"\n\n[1] \"double\"\n\nstr(peso_lb)     # Muestra la estructura completa\n\n num 145\n\n# Operaciones matemáticas básicas\nimc &lt;- peso_lb/2.2046/(altura_m^2)  # Cálculo del IMC\nprint(imc)  # Muestra el resultado del cálculo\n\n[1] 22.75833\n\n\nLos objetos numéricos permiten realizar operaciones matemáticas y son esenciales en análisis estadísticos, cálculos y modelado de datos.\n\n\n3.4.2 Objetos de Texto\nLos objetos de texto, también conocidos como objetos de tipo carácter, almacenan cadenas de texto. Estos se escriben entre comillas dobles (\") o simples ('). Son útiles para representar información cualitativa, como nombres, descripciones o etiquetas.\nEjemplo de creación de objetos de texto:\n\n# Creación de objetos de texto\nnombre &lt;- \"Juan\"           # Usando comillas dobles\napellido &lt;- 'García'       # Usando comillas simples\nnombre_completo &lt;- paste(nombre, apellido)  # Concatenación de textos\n\n# Exploración de objetos de texto\nclass(nombre)          # Muestra \"character\"\n\n[1] \"character\"\n\nstr(nombre_completo)   # Muestra la estructura\n\n chr \"Juan García\"\n\nnchar(nombre)          # Muestra el número de caracteres\n\n[1] 4\n\n# Manipulación de texto\nnombre_mayusculas &lt;- toupper(nombre)  # Convierte a mayúsculas\nprint(nombre_mayusculas)\n\n[1] \"JUAN\"\n\n\n\n\n3.4.3 Objetos de Tipo Factor\nLos objetos de tipo factor se utilizan para almacenar variables categóricas con niveles definidos. Estos niveles representan categorías discretas, como escalas, estados o clasificaciones. Los factores son especialmente útiles en análisis estadísticos, ya que permiten manejar variables categóricas de manera eficiente.\nEjemplo de creación de objetos tipo factor:\n\n# Creación de factores simples\nestado_civil &lt;- factor(\"soltero\")\nsexo &lt;- factor(\"masculino\")\n\n# Creación de factores con múltiples niveles\nestado_civil &lt;- factor(\"soltero\", \n                      levels = c(\"soltero\", \"casado\", \"divorciado\"))\nnivel_educativo &lt;- factor(\"licenciatura\",\n                         levels = c(\"bachillerato\", \"licenciatura\", \"posgrado\"),\n                         ordered = TRUE)  # Factor ordenado\n\n# Exploración de factores\nclass(estado_civil)     # Muestra \"factor\"\n\n[1] \"factor\"\n\nlevels(estado_civil)    # Muestra los niveles definidos\n\n[1] \"soltero\"    \"casado\"     \"divorciado\"\n\nstr(nivel_educativo)    # Muestra la estructura completa\n\n Ord.factor w/ 3 levels \"bachillerato\"&lt;..: 2\n\nis.ordered(nivel_educativo)  # Verifica si el factor es ordenado\n\n[1] TRUE\n\n\nLos factores son especialmente útiles en análisis estadísticos porque:\n\nPermiten especificar un orden en las categorías.\nFacilitan la creación de gráficos categóricos.\nSon esenciales en modelos estadísticos.\n\n\n\n3.4.4 Objetos Lógicos\nLos objetos lógicos almacenan valores TRUE o FALSE, que resultan de comparaciones lógicas. Estos objetos son esenciales para realizar análisis condicionales, aplicar filtros y evaluaciones condicionales.\nEjemplo de creación de objetos lógicos:\n\n# Creación de objetos lógicos mediante comparaciones\nmayoria_de_edad &lt;- edad &gt;= 18\n\n\n# Exploración de objetos lógicos\nclass(mayoria_de_edad)   # Muestra \"logical\"\n\n[1] \"logical\"\n\nstr(mayoria_de_edad)      # Muestra la estructura\n\n logi TRUE\n\n\nLos objetos lógicos son esenciales para:\n\nFiltrar datos basados en condiciones.\nRealizar evaluaciones condicionales.\nCrear subconjuntos de datos.\nProgramación de control de flujo (if/else).",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Primeros pasos en R</span>"
    ]
  },
  {
    "objectID": "04_datos.html",
    "href": "04_datos.html",
    "title": "4  Estructuras de datos en R",
    "section": "",
    "text": "4.1 Vectores\nLas estructuras de datos representan el pilar esencial para el análisis estadístico y científico en el entorno R, ya que posibilitan la organización, almacenamiento y manipulación de información de manera sistemática y eficiente. Estas estructuras permiten gestionar desde datos simples, como valores individuales, hasta conjuntos complejos y heterogéneos, adaptándose a los requerimientos de diversos tipos de análisis y facilitando la reproducibilidad de los resultados (Ihaka & Gentleman, 1996; R Core Team, 2023).\nEn R, las principales estructuras de datos incluyen los vectores, matrices, data frames y listas. Cada una de estas estructuras ha sido diseñada para resolver problemas específicos y se adapta a diferentes escenarios analíticos. Los vectores constituyen la unidad básica y homogénea de almacenamiento, mientras que las matrices permiten organizar datos en dos dimensiones bajo la restricción de homogeneidad de tipo. Los data frames, por su parte, ofrecen una estructura tabular flexible, capaz de contener columnas de distintos tipos de datos, lo que resulta especialmente útil en el análisis de datos reales y heterogéneos. Finalmente, las listas proporcionan una solución versátil para almacenar colecciones de objetos de diferentes tipos y longitudes, facilitando la gestión de resultados complejos y la integración de diversas fuentes de información (R Core Team, 2023; Wickham & Grolemund, 2017).\nEn el lenguaje R, los vectores constituyen la estructura de datos más elemental y versátil, sirviendo como base para la construcción de estructuras más complejas, tales como matrices y data frames. Un vector se define como una secuencia ordenada y unidimensional de elementos que comparten el mismo tipo de dato, ya sea numérico, de texto (caracter), o lógico. Esta homogeneidad en el tipo de datos asegura tanto la eficiencia computacional como la coherencia en las operaciones analíticas, facilitando la manipulación y el análisis de grandes volúmenes de información (Ihaka & Gentleman, 1996; R Core Team, 2023).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Estructuras de datos en R</span>"
    ]
  },
  {
    "objectID": "04_datos.html#vectores",
    "href": "04_datos.html#vectores",
    "title": "4  Estructuras de datos en R",
    "section": "",
    "text": "4.1.1 Tipos de Vectores y su creación\nLa función principal para la creación de vectores en R es c(), abreviatura de “concatenar”. Esta función permite agrupar elementos individuales o incluso otros vectores en una sola estructura (Wickham & Grolemund, 2017). Los tipos de vectores más comunes incluyen los numéricos, de texto y lógicos, como se ilustra a continuación:\n\n# Vectores numéricos\nedades &lt;- c(17, 20, 18, 25)          # Enteros\nalturas &lt;- c(1.75, 1.68, 1.82, 1.65) # Decimales\n\n# Vectores de texto (character)\nnombres &lt;- c(\"Juan\", \"Ana\", \"Luis\", \"María\")\n\n# Vectores lógicos\n# Creados usando la función c()\nmayores_de_edad &lt;- c(FALSE, TRUE, TRUE, TRUE)\n# O mediante una comparación empleando operadores lógicos  \nmayores_de_edad &lt;- edades &gt;= 18\n\nEn estos ejemplos, el vector edades almacena valores numéricos, nombres contiene cadenas de texto, y mayores_de_edad almacena valores lógicos (TRUE o FALSE) derivados de una comparación. Esta flexibilidad permite adaptar los vectores a diversas necesidades analíticas (Grolemund & Wickham, 2017).\n\n\n4.1.2 Coerción de Tipos de Datos en Vectores\nUna característica fundamental de los vectores en R es la coerción automática de tipos de datos. Cuando se intenta combinar elementos de diferentes tipos en un mismo vector, R convierte todos los elementos al tipo más general que pueda contener a todos ellos, siguiendo una jerarquía: carácter &gt; numérico &gt; lógico. Por ejemplo:\n\n# Mezcla de números y texto\nvector_mixto &lt;- c(1, 2, \"tres\")\n# Resultado: \"1\" \"2\" \"tres\"\n\nEn este caso, todos los elementos se convierten a texto (character), ya que es el tipo más general capaz de representar cualquier valor. Este comportamiento, conocido como coerción implícita, es esencial para evitar errores en la manipulación de datos, pero requiere atención para no perder información relevante o introducir inconsistencias (R Core Team, 2023; Grolemund & Wickham, 2017).\n\n\n4.1.3 Operaciones con Vectores\nLos vectores en R permiten realizar una amplia gama de operaciones matemáticas, lógicas y de manipulación de datos, fundamentales para el análisis estadístico y la transformación de información. A continuación, se describen las operaciones más comunes:\n\n4.1.3.1 Acceso a elementos específicos\nEl acceso a elementos individuales o múltiples de un vector se realiza mediante índices entre corchetes, comenzando en 1:\n\n# Acceder a elementos individuales\nprimer_nombre &lt;- nombres[1]    # \"Juan\"\nultima_edad &lt;- edades[4]       # 25\n\n# Acceder a múltiples elementos\nnombres_seleccionados &lt;- nombres[c(1, 3)]  # \"Juan\" \"Luis\"\n\n\n\n4.1.3.2 Filtrado de elementos\nEl filtrado de elementos se logra aplicando condiciones lógicas, lo que permite seleccionar subconjuntos de datos de manera eficiente:\n\n# Filtrar personas mayores de 20 años\nmayores_20 &lt;- edades[edades &gt; 20]\n\n# Obtener nombres de personas mayores de 20\nnombres_mayores_20 &lt;- nombres[edades &gt; 20]\n\nAquí, la condición edades &gt; 20 genera un vector lógico que selecciona únicamente los valores que cumplen el criterio especificado (Field, 2013).\n\n\n4.1.3.3 Combinación de vectores\nLa función c() también permite combinar varios vectores en uno solo:\n\n# Combinar dos vectores\nnuevo_vector &lt;- c(edades, c(22, 21))\nnuevo_vector\n\n[1] 17 20 18 25 22 21\n\n\nAquí, el vector nuevo_vector combina los elementos del vector edades con los valores 22 y 21, generando un nuevo vector.\n\n\n4.1.3.4 Funciones Útiles para Vectores\nR ofrece una variedad de funciones para analizar y manipular vectores, tales como:\n\n# Estadísticas básicas\npromedio_edades &lt;- mean(edades)       # Media\nedad_maxima &lt;- max(edades)            # Valor máximo\nedad_minima &lt;- min(edades)            # Valor mínimo\ntotal_elementos &lt;- length(edades)      # Número de elementos\n\n# Ordenamiento\nedades_ordenadas &lt;- sort(edades)      # Orden ascendente\nedades_descendente &lt;- sort(edades, decreasing = TRUE)  # Orden descendente\n\n\n\n4.1.3.5 Aplicaciones Prácticas\nLos vectores son esenciales en análisis estadísticos básicos y exploratorios:\n\n# Análisis descriptivo\nsummary(edades)  # Resumen estadístico\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  17.00   17.75   19.00   20.00   21.25   25.00 \n\ntable(mayores_de_edad)  # Tabla de frecuencias\n\nmayores_de_edad\nFALSE  TRUE \n    1     3 \n\nhist(edades)  # Histograma de edades\n\n\n\n\n\n\n\n\nLa utilización adecuada de los vectores en R resulta indispensable para la gestión eficiente de datos y la ejecución de análisis estadísticos rigurosos. El dominio de esta estructura permite no solo realizar procedimientos exploratorios y descriptivos, sino también sienta las bases metodológicas para la implementación de técnicas analíticas avanzadas y el desarrollo de modelos estadísticos complejos en entornos de investigación y aplicación profesional (Grolemund & Wickham, 2017; Tukey, 1977).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Estructuras de datos en R</span>"
    ]
  },
  {
    "objectID": "04_datos.html#matrices",
    "href": "04_datos.html#matrices",
    "title": "4  Estructuras de datos en R",
    "section": "4.2 Matrices",
    "text": "4.2 Matrices\nLas matrices en R representan estructuras de datos bidimensionales que organizan información en filas y columnas, manteniendo la homogeneidad en el tipo de datos (numérico, lógico o de texto). Esta característica fundamental garantiza la integridad y eficiencia en las operaciones matemáticas y estadísticas, siendo particularmente relevantes en el análisis multivariado y la computación científica (Ihaka & Gentleman, 1996; Venables & Ripley, 2002).\nA diferencia de los vectores unidimensionales, las matrices proporcionan un marco más sofisticado para la representación y manipulación de datos estructurados, facilitando la implementación de algoritmos estadísticos complejos y el análisis de datos experimentales (Montgomery et al., 2012).\n\n4.2.1 Creación de Matrices y Argumentos de la Función matrix()\nLa función principal para la creación de matrices en R es matrix(). Sus argumentos más relevantes son:\n\ndata: Vector de datos a organizar en la matriz. Es el único argumento obligatorio.\nnrow: Número de filas de la matriz. Opcional; si se omite, R lo infiere a partir de la longitud de data y el valor de ncol.\nncol: Número de columnas de la matriz. Opcional; si se omite, R lo infiere a partir de la longitud de data y el valor de nrow.\nbyrow: Lógico. Indica si los datos se llenan por filas (TRUE) o por columnas (FALSE, valor por defecto).\ndimnames: Lista de dos vectores de caracteres para asignar nombres a filas y columnas.\n\nEl comportamiento de estos argumentos se ilustra en los siguientes ejemplos:\n\n# Ejemplo 1: Solo se especifica data y nrow\n# R calcula automáticamente el número de columnas\nmatriz1 &lt;- matrix(1:6, nrow = 2)\nprint(matriz1)\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\n# Ejemplo 2: Solo se especifica data y ncol\n# R calcula automáticamente el número de filas\nmatriz2 &lt;- matrix(1:6, ncol = 2)\nprint(matriz2)\n\n     [,1] [,2]\n[1,]    1    4\n[2,]    2    5\n[3,]    3    6\n\n# Ejemplo 3: Se especifican nrow y ncol\nmatriz3 &lt;- matrix(1:6, nrow = 2, ncol = 3)\nprint(matriz3)\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\n# Ejemplo 4: Uso del argumento byrow\nmatriz4 &lt;- matrix(1:6, nrow = 2, ncol = 3, byrow = TRUE)\nprint(matriz4)\n\n     [,1] [,2] [,3]\n[1,]    1    2    3\n[2,]    4    5    6\n\n# Ejemplo 5: Asignación de nombres a filas y columnas con dimnames\nmatriz5 &lt;- matrix(1:4, nrow = 2, dimnames = list(c(\"Fila1\", \"Fila2\"), c(\"Col1\", \"Col2\")))\nprint(matriz5)\n\n      Col1 Col2\nFila1    1    3\nFila2    2    4\n\n\nSi la longitud del vector data no coincide exactamente con el producto de nrow y ncol, R reciclará los valores del vector para completar la matriz. Este comportamiento, conocido como reciclaje de datos, puede ser útil en ciertos contextos, pero también puede generar resultados inesperados si no se verifica la consistencia de los datos (R Core Team, 2023).\n\n# Ejemplo de reciclaje de datos\nmatriz6 &lt;- matrix(1:3, nrow = 2, ncol = 4)\nprint(matriz6) # R repite los valores de 1:3 hasta llenar la matriz\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    3    2    1\n[2,]    2    1    3    2\n\n\n\n\n4.2.2 Propiedades y Atributos de las Matrices\nLas matrices en R poseen atributos específicos que pueden ser consultados y modificados mediante funciones especializadas. Es posible obtener y modificar las dimensiones, así como asignar nombres a filas y columnas para mejorar la legibilidad y trazabilidad de los datos (Venables & Ripley, 2002; Grolemund & Wickham, 2017).\n\n# Dimensiones de la matriz\ndim(matriz1)      # Devuelve (filas, columnas)\n\n[1] 2 3\n\nnrow(matriz1)     # Número de filas\n\n[1] 2\n\nncol(matriz1)     # Número de columnas\n\n[1] 3\n\n# Asignación de nombres a filas y columnas\nrownames(matriz1) &lt;- c(\"Fila1\", \"Fila2\")\ncolnames(matriz1) &lt;- c(\"Col1\", \"Col2\", \"Col3\")\nprint(matriz1)\n\n      Col1 Col2 Col3\nFila1    1    3    5\nFila2    2    4    6\n\n\nEl acceso a los elementos de una matriz se realiza mediante la notación [fila, columna]. Esta sintaxis permite extraer elementos individuales, filas completas, columnas completas o subconjuntos específicos de la matriz. Además, es posible aplicar condiciones lógicas para filtrar elementos, lo que resulta fundamental en la exploración y transformación de datos (Field, 2013; Grolemund & Wickham, 2017).\n\n# Acceso a un elemento específico\nelemento &lt;- matriz1[2, 1]     # Elemento en fila 2, columna 1\nprint(elemento)\n\n[1] 2\n\n# Acceso a una fila completa\nfila_completa &lt;- matriz1[1, ] # Primera fila completa\nprint(fila_completa)\n\nCol1 Col2 Col3 \n   1    3    5 \n\n# Acceso a una columna completa\ncol_completa &lt;- matriz1[, 2]  # Segunda columna completa\nprint(col_completa)\n\nFila1 Fila2 \n    3     4 \n\n# Filtrado condicional\nelementos_mayores_3 &lt;- matriz1[matriz1 &gt; 3]\nprint(elementos_mayores_3)  # Devuelve: 4 5 6\n\n[1] 4 5 6\n\n\n\n\n4.2.3 Combinación de Matrices\nLa combinación de matrices es una operación fundamental en la gestión y consolidación de datos, especialmente cuando se trabaja con información proveniente de diferentes fuentes o experimentos. R proporciona funciones específicas para unir matrices de manera eficiente y controlada: cbind() para combinar por columnas y rbind() para combinar por filas. Es indispensable que las dimensiones sean compatibles; de lo contrario, R generará un error. Además, la homogeneidad de tipo de datos se mantiene, y si se combinan tipos distintos, R aplicará coerción automática al tipo más general (R Core Team, 2023; Field, 2013).\n\n# Crear dos matrices compatibles para combinar\nmatrizA &lt;- matrix(1:6, nrow = 3)\nmatrizB &lt;- matrix(7:12, nrow = 3)\n\n# Combinación por columnas\nmatriz_columnas &lt;- cbind(matrizA, matrizB)\nprint(matriz_columnas)\n\n     [,1] [,2] [,3] [,4]\n[1,]    1    4    7   10\n[2,]    2    5    8   11\n[3,]    3    6    9   12\n\n# Combinación por filas\nmatriz_filas &lt;- rbind(matrizA, matrizB)\nprint(matriz_filas)\n\n     [,1] [,2]\n[1,]    1    4\n[2,]    2    5\n[3,]    3    6\n[4,]    7   10\n[5,]    8   11\n[6,]    9   12\n\n\nLa correcta combinación de matrices permite consolidar conjuntos de datos, preparar información para análisis multivariados y estructurar resultados experimentales de manera eficiente. Este proceso es especialmente relevante en contextos de análisis de grandes volúmenes de datos y en la integración de resultados de diferentes experimentos o fuentes (Kutner et al., 2005; Hernández, Usuga & Mazo, 2024).\n\n\n4.2.4 Operaciones y aplicaciones de las matrices\nLas matrices en R permiten realizar una amplia variedad de operaciones algebraicas y estadísticas, fundamentales para el análisis de datos y la modelización matemática. Entre las operaciones más relevantes se encuentran la suma y multiplicación elemento a elemento, la multiplicación matricial, la transposición, el cálculo de matrices de correlación y la descomposición en valores singulares. Estas operaciones son esenciales en el desarrollo de modelos estadísticos avanzados, análisis multivariados y procedimientos de álgebra lineal (Montgomery et al., 2012; Venables & Ripley, 2002).\n\n# Operaciones aritméticas elemento a elemento\nmatriz_C &lt;- matrix(1:4, nrow = 2)\nmatriz_D &lt;- matrix(5:8, nrow = 2)\n\nsuma &lt;- matriz_C + matriz_D\nproducto &lt;- matriz_C * matriz_D\n\n# Multiplicación matricial\nproducto_matricial &lt;- matriz_C %*% matriz_D\n\n# Transposición de matrices\nmatriz_transpuesta &lt;- t(matriz_C)\n\nPara profundizar en el uso de matrices y su aplicación en el análisis estadístico con R, se recomienda consultar el capítulo 20 del libro Modelos de Regresión con R de Hernández, Usuga y Mazo (2024), donde se aborda el álgebra matricial de manera detallada y aplicada.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Estructuras de datos en R</span>"
    ]
  },
  {
    "objectID": "04_datos.html#data-frames",
    "href": "04_datos.html#data-frames",
    "title": "4  Estructuras de datos en R",
    "section": "4.3 Data frames",
    "text": "4.3 Data frames\nEl data frame constituye una de las estructuras de datos más relevantes y versátiles en el entorno de R, permitiendo la organización de información en un formato tabular bidimensional, donde las filas representan observaciones individuales y las columnas corresponden a variables específicas. Esta estructura es análoga a una hoja de cálculo en Excel o a una tabla en una base de datos relacional, lo que facilita la transición de datos entre diferentes plataformas y sistemas de análisis (R Core Team, 2023).\nUna característica distintiva de los data frames es la posibilidad de que cada columna almacene un tipo de dato diferente, como valores numéricos, cadenas de texto, valores lógicos o factores. Esta flexibilidad resulta fundamental para el manejo de datos heterogéneos, permitiendo la integración y el análisis eficiente de información proveniente de encuestas, experimentos científicos, registros administrativos y otros contextos donde la diversidad de variables es común. Además, los data frames son ampliamente compatibles con funciones y paquetes especializados en R, como ggplot2 y dplyr, lo que los convierte en la estructura de datos más utilizada en el análisis estadístico y científico con este lenguaje (Wickham & Grolemund, 2017; R Core Team, 2023).\n\n4.3.1 Creación de data frames\nLa creación de un data frame en R se realiza mediante la función data.frame(). Esta función combina varios vectores de igual longitud en una estructura tabular. Es imprescindible que todos los vectores tengan la misma cantidad de elementos, ya que cada fila representa una observación completa. El proceso de creación puede organizarse en los siguientes pasos:\n\nDefinir los vectores que se desean combinar, asegurando que todos tengan la misma longitud.\nUtilizar la función data.frame() para unir los vectores en una estructura tabular.\nAsignar el resultado a un objeto para su posterior manipulación y análisis.\n\nA continuación se muestra un ejemplo utilizando los vectores definidos previamente:\n\n# 1. Definir los vectores\nnombres &lt;- c(\"Juan\", \"Ana\", \"Luis\", \"María\")\nedades &lt;- c(17, 20, 18, 25)\nmayores_de_edad &lt;- edades &gt;= 18\n\n# 2. Crear el data frame\ndatos &lt;- data.frame(nombres, edades, mayores_de_edad)\n\n# 3. Visualizar el data frame\ndatos\n\n  nombres edades mayores_de_edad\n1    Juan     17           FALSE\n2     Ana     20            TRUE\n3    Luis     18            TRUE\n4   María     25            TRUE\n\n\nEn este ejemplo, el objeto datos corresponde a un data frame compuesto por tres columnas: nombres (caracteres), edades (numéricos) y mayores_de_edad (lógicos). Esta estructura permite almacenar y analizar información de manera eficiente, facilitando la manipulación y el acceso a los datos según las necesidades del análisis (R Core Team, 2023).\n\n\n4.3.2 Ventajas de un data frame\nLos data frames presentan múltiples ventajas que los hacen indispensables en el análisis de datos con R. Entre las principales ventajas se destacan:\n\nEstructura clara: Cada fila representa una observación y cada columna una variable, lo que facilita la interpretación y el manejo de la información.\nCompatibilidad: Los data frames funcionan con funciones estadísticas, herramientas de visualización y paquetes populares como ggplot2 y dplyr, ampliando significativamente las posibilidades de análisis y presentación de resultados.\nFlexibilidad: Es posible almacenar diferentes tipos de datos en las columnas, como números, texto y factores, lo que resulta esencial para el análisis de datos heterogéneos.\nFacilidad de manipulación: Existen numerosas funciones y herramientas para filtrar, seleccionar, transformar y resumir la información contenida en un data frame, lo que contribuye a la eficiencia y robustez del proceso analítico (Wickham & Grolemund, 2017; Field, 2013).\n\n\n\n4.3.3 Manipulación de data frames\nR ofrece diversas formas de manipular data frames, tanto mediante funciones básicas como a través de herramientas avanzadas de paquetes especializados. Entre las operaciones más comunes se encuentran:\n\n4.3.3.1 Acceso a columnas\nPara acceder a una columna específica de un data frame, se utiliza el operador $ seguido del nombre de la columna. Esta operación devuelve el vector correspondiente a la variable seleccionada.\n\n# Acceso a la columna 'nombres'\ndatos$nombres\n\n[1] \"Juan\"  \"Ana\"   \"Luis\"  \"María\"\n\n\n\n\n4.3.3.2 Filtrado de filas\nEs posible seleccionar filas que cumplan ciertas condiciones lógicas, lo que resulta fundamental para el análisis exploratorio y la segmentación de datos. Por ejemplo, para obtener únicamente las observaciones donde la edad es mayor a 20 años:\n\n# Filtrar filas donde la edad sea mayor a 20\ndatos_filtrados &lt;- datos[datos$edades &gt; 20, ]\ndatos_filtrados\n\n  nombres edades mayores_de_edad\n4   María     25            TRUE\n\n\n\n\n4.3.3.3 Agregar nuevas columnas\nSe pueden agregar nuevas variables a un data frame asignando un vector a un nuevo nombre de columna. Por ejemplo, para añadir la altura de cada persona:\n\n# Agregar una columna llamada 'altura' al data frame\ndatos$altura &lt;- c(1.75, 1.60, 1.80, 1.65)\n\nDespués de esta operación, el data frame datos tendrá una columna adicional llamada altura, donde cada valor corresponde a la altura de la persona en la misma fila.\n\n\n4.3.3.4 Seleccionar varias columnas\nPara trabajar con un subconjunto de variables, la función subset() permite crear un nuevo data frame que contiene únicamente las columnas seleccionadas:\n\n# Crear un nuevo data frame solo con las columnas 'nombres' y 'edades'\nsubgrupo &lt;- subset(datos, select = c(nombres, edades))\n\n\n\n4.3.3.5 Resumir información\nLa función summary() genera un resumen estadístico de cada columna del data frame, proporcionando información relevante como el valor mínimo, máximo, media, mediana y, en el caso de variables categóricas, la frecuencia de cada categoría. Esta función resulta esencial para la exploración inicial y la comprensión de la estructura de los datos antes de realizar análisis más detallados (Field, 2013; R Core Team, 2023).\n\n# Obtener un resumen estadístico de todas las columnas del data frame\nsummary(datos)\n\n   nombres              edades      mayores_de_edad     altura     \n Length:4           Min.   :17.00   Mode :logical   Min.   :1.600  \n Class :character   1st Qu.:17.75   FALSE:1         1st Qu.:1.637  \n Mode  :character   Median :19.00   TRUE :3         Median :1.700  \n                    Mean   :20.00                   Mean   :1.700  \n                    3rd Qu.:21.25                   3rd Qu.:1.762  \n                    Max.   :25.00                   Max.   :1.800",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Estructuras de datos en R</span>"
    ]
  },
  {
    "objectID": "04_datos.html#listas",
    "href": "04_datos.html#listas",
    "title": "4  Estructuras de datos en R",
    "section": "4.4 Listas",
    "text": "4.4 Listas\nLas listas en R son estructuras de datos sumamente flexibles y potentes, ya que permiten almacenar elementos de diferentes tipos y longitudes dentro de un mismo objeto. A diferencia de los data frames, donde todas las columnas deben tener la misma longitud y cada columna representa una variable, en una lista cada elemento puede ser un vector, un data frame, una matriz, una función, o incluso otra lista. Esta característica hace que las listas sean ideales para guardar resultados complejos, como salidas de modelos estadísticos, colecciones de datos heterogéneos o cualquier conjunto de información que no encaje en una estructura tabular tradicional (R Core Team, 2023).\n\n4.4.1 Creación de listas\nLa creación de una lista en R se realiza mediante la función list(). Cada elemento puede tener un nombre y puede ser de cualquier tipo de objeto. El proceso de creación puede organizarse en los siguientes pasos:\n\nDefinir los elementos que se desean incluir en la lista, pudiendo ser de cualquier tipo y longitud.\nAsignar nombres a los elementos para facilitar su identificación y acceso posterior.\nUtilizar la función list() para agrupar los elementos en un solo objeto.\n\nPor ejemplo:\n\n# 1. Definir los elementos\nnombres &lt;- c(\"Juan\", \"Ana\")      # Vector de texto\nedades &lt;- c(18, 20)              # Vector numérico\ndatos_completos &lt;- datos         # Data frame\n\n# 2. Crear la lista con nombres para cada elemento\nmi_lista &lt;- list(\n  nombres = nombres,\n  edades = edades,\n  datos_completos = datos_completos\n)\n\nEn este ejemplo, la lista mi_lista contiene tres elementos:\n\nEl elemento nombres es un vector de texto.\nEl elemento edades es un vector numérico.\nEl elemento datos_completos corresponde a un data frame.\n\nEsta estructura permite almacenar y organizar información heterogénea de manera eficiente (R Core Team, 2023).\n\n\n4.4.2 Acceso a elementos de una lista\nEl acceso a los elementos de una lista puede realizarse de varias formas, según la necesidad del análisis:\n\nPor nombre: Utilizando el operador $ o corchetes dobles [[ ]]\n\n\n# Acceder al elemento 'nombres' usando $\nmi_lista$nombres\n\n[1] \"Juan\" \"Ana\" \n\n# Acceder al elemento 'nombres' usando corchetes dobles\nmi_lista[[\"nombres\"]]\n\n[1] \"Juan\" \"Ana\" \n\n\nAmbas formas devuelven el vector de nombres almacenado en la lista.\n\nPor índice: Utilizando corchetes dobles [[ ]]:\n\n\n# Acceder al primer elemento de la lista (en este caso, el vector de nombres)\nmi_lista[[1]]\n\n[1] \"Juan\" \"Ana\" \n\n\nEsto es útil cuando se desconoce el nombre del elemento, pero se conoce su posición dentro de la lista.\n\nDiferencia entre corchetes simples y dobles: Si se utilizan corchetes simples [ ] para acceder a un elemento de la lista, el resultado será una sublista (es decir, una lista que contiene el elemento seleccionado), no el elemento en sí. Para obtener directamente el contenido, siempre utilice corchetes dobles [[ ]] o el operador $ si el elemento tiene nombre.\n\n\n# Devuelve una sublista\nmi_lista[1]\n\n$nombres\n[1] \"Juan\" \"Ana\" \n\n# Devuelve el elemento directamente\nmi_lista[[1]]\n\n[1] \"Juan\" \"Ana\" \n\n\nEsta distinción resulta fundamental para evitar errores en la manipulación de listas y para acceder correctamente a los datos almacenados (Wickham & Grolemund, 2017).\n\n\n4.4.3 Aplicaciones prácticas\nLas listas en R resultan especialmente valiosas cuando se requiere almacenar y organizar resultados complejos derivados de análisis estadísticos. Por ejemplo, al ajustar un modelo de regresión, la función lm() genera una lista que contiene los coeficientes estimados, los residuos, los valores ajustados y otros diagnósticos relevantes. Esta estructura permite acceder fácilmente a cada componente del análisis para su interpretación o procesamiento posterior (R Core Team, 2023).\nAdemás, las listas son ideales para agrupar diferentes tipos de datos relacionados en un solo objeto, como vectores, data frames, matrices o incluso otras listas. Esta capacidad de contener elementos heterogéneos facilita la gestión de información en proyectos de análisis de datos, donde es común trabajar con resultados de distintas etapas o fuentes (Wickham & Grolemund, 2017).",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Estructuras de datos en R</span>"
    ]
  },
  {
    "objectID": "04_datos.html#comparación-entre-data-frames-y-listas",
    "href": "04_datos.html#comparación-entre-data-frames-y-listas",
    "title": "4  Estructuras de datos en R",
    "section": "4.5 Comparación entre Data Frames y Listas",
    "text": "4.5 Comparación entre Data Frames y Listas\nEn R, los data frames y las listas constituyen estructuras de datos esenciales, pero difieren significativamente en su organización interna y en los contextos para los que resultan más apropiadas. La siguiente tabla resume las diferencias clave entre ambas estructuras (R Core Team, 2023; Wickham & Grolemund, 2017):\n\n\n\n\n\n\n\n\nCaracterística\nData Frame\nLista\n\n\n\n\nEstructura\nTabular: filas y columnas\nColección de objetos heterogéneos\n\n\nTipos de datos\nCada columna puede tener un tipo distinto, pero todos los elementos de una columna deben ser del mismo tipo\nCada elemento puede ser de cualquier tipo y longitud\n\n\nUso principal\nAnálisis estadístico y visualización de datos estructurados\nAlmacenamiento y gestión de resultados complejos o heterogéneos\n\n\nAcceso a elementos\nPor columnas (usando $ o corchetes) y por índices de filas y columnas\nPor nombre o por índice, utilizando corchetes dobles [[ ]] o el operador $\n\n\n\nLa elección entre data frames y listas depende del tipo de información y del objetivo del análisis. Para datos tabulares, como encuestas o resultados experimentales, se recomienda emplear data frames. Cuando se requiere almacenar y manipular resultados complejos o combinaciones de diferentes tipos de datos, las listas resultan más adecuadas.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Estructuras de datos en R</span>"
    ]
  },
  {
    "objectID": "05_importacion.html",
    "href": "05_importacion.html",
    "title": "5  Importación de datos",
    "section": "",
    "text": "5.1 Configuración previa: el directorio de trabajo\nLa importación de datos constituye una etapa esencial en el proceso de análisis estadístico, ya que posibilita el acceso y la manipulación de información proveniente de diversas fuentes, tales como archivos en formato CSV, hojas de cálculo de Excel o datos disponibles en páginas web. El entorno R ofrece un conjunto robusto de funciones y paquetes especializados que permiten realizar la importación de datos de manera eficiente, reproducible y escalable, facilitando así el manejo de grandes volúmenes de información y promoviendo la integridad y trazabilidad de los análisis (R Core Team, 2023; Grolemund & Wickham, 2017).\nPrevio a la importación de datos, resulta imprescindible verificar y establecer correctamente el directorio de trabajo. El directorio de trabajo, conocido en inglés como working directory, corresponde a la carpeta desde la cual R accede a los archivos de entrada y en la que almacena los resultados generados. Una adecuada configuración de este directorio contribuye significativamente a la organización y eficiencia del flujo de trabajo (Bryan, 2018; Gentleman & Temple Lang, 2007).\nEn el caso de utilizar proyectos de RStudio (.Rproj), el directorio de trabajo se define automáticamente al abrir el proyecto. Esta acción simplifica la gestión de archivos y minimiza la probabilidad de errores relacionados con rutas de acceso. No obstante, cuando se trabaja con scripts independientes, es necesario establecer manualmente el directorio de trabajo mediante la función setwd(). Por ejemplo:\n# Establecer directorio de trabajo\nsetwd(\"ruta/del/directorio\")\nLa correcta configuración del directorio de trabajo previene errores frecuentes, como el mensaje “archivo no encontrado”. Además, garantiza que el código sea portable y replicable en distintos sistemas o ubicaciones (Bryan, 2018).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Importación de datos</span>"
    ]
  },
  {
    "objectID": "05_importacion.html#configuración-previa-el-directorio-de-trabajo",
    "href": "05_importacion.html#configuración-previa-el-directorio-de-trabajo",
    "title": "5  Importación de datos",
    "section": "",
    "text": "5.1.1 Automatización del directorio de trabajo en scripts independientes\nPara scripts que no están asociados a un proyecto de RStudio, es posible automatizar la configuración del directorio de trabajo utilizando el paquete rstudioapi. Este enfoque permite establecer como directorio de trabajo la carpeta en la que se encuentra guardado el script. Esto facilita la portabilidad y la colaboración entre diferentes usuarios y equipos (Bryan, 2018).\nEl siguiente fragmento de código ilustra este procedimiento:\n\n# Instalación y carga del paquete rstudioapi\nif (!require(\"rstudioapi\")) install.packages(\"rstudioapi\")\n\n# Linea empleada para establecer  el directorio de trabajo\nsetwd(dirname(rstudioapi::getActiveDocumentContext()$path))\n\nEste código verifica si el paquete rstudioapi está instalado y, en caso contrario, lo instala automáticamente. Posteriormente, obtiene la ruta del script en ejecución y la utiliza para definir el directorio de trabajo. Esto permite el acceso a los archivos de la carpeta sin necesidad de especificar rutas absolutas.\n\n\n5.1.2 Verificación y buenas prácticas\nAntes de proceder con la importación de datos o el almacenamiento de resultados, se recomienda verificar el directorio de trabajo actual mediante la función getwd():\n\n# Verificación del directorio de trabajo actual\ngetwd()\n\nAdicionalmente, es aconsejable guardar el script antes de ejecutar la configuración automática del directorio. R requiere conocer la ubicación del archivo para establecer correctamente el entorno de trabajo.\nSiempre que sea posible, se sugiere trabajar dentro de un proyecto de RStudio. Esta práctica automatiza la gestión del directorio de trabajo y favorece la organización de los archivos y recursos asociados al análisis. Esto promueve la reproducibilidad y la eficiencia en el desarrollo de proyectos de análisis de datos (Bryan, 2018; Gentleman & Temple Lang, 2007).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Importación de datos</span>"
    ]
  },
  {
    "objectID": "05_importacion.html#importación-de-archivos-csv-y-excel-en-r",
    "href": "05_importacion.html#importación-de-archivos-csv-y-excel-en-r",
    "title": "5  Importación de datos",
    "section": "5.2 Importación de archivos CSV y Excel en R",
    "text": "5.2 Importación de archivos CSV y Excel en R\nLa importación de datos tabulares es una tarea fundamental en el análisis estadístico. R facilita este proceso mediante funciones y paquetes. Estos permiten trabajar con archivos en formatos ampliamente utilizados, como CSV y Excel. Comprender cómo importar estos archivos correctamente es esencial para garantizar la integridad y reproducibilidad del análisis (Wickham, 2016; Wickham & Bryan, 2023).\n\n5.2.1 Importación de archivos CSV\nEl formato CSV (Comma-Separated Values) se ha consolidado como uno de los estándares más utilizados para el almacenamiento y el intercambio de datos tabulares, debido a su simplicidad, legibilidad y compatibilidad con múltiples plataformas y aplicaciones. En R, la función read.csv() permite importar archivos CSV de manera eficiente, posibilitando la lectura de grandes volúmenes de datos sin requerir transformaciones previas (Grolemund & Wickham, 2017).\nA continuación, se presenta un ejemplo básico de importación de un archivo CSV:\n\n# Importar un archivo CSV\ndatos &lt;- read.csv(\"ruta/del/archivo/datos.csv\", \n                  header = TRUE, \n                  sep = \",\")\n\nLos parámetros principales de esta función se describen a continuación:\n\nheader: Este argumento lógico indica si la primera fila del archivo CSV contiene los nombres de las columnas. Si se establece en TRUE, la primera fila se interpretará como los nombres de las variables; si se establece en FALSE, R asignará nombres genéricos a las columnas (R Core Team, 2023).\nsep: Este argumento especifica el carácter que se utiliza para separar los valores en cada fila del archivo CSV. El valor predeterminado es la coma (,), pero puede ajustarse a otros caracteres, como el punto y coma (;) o la tabulación (\\t), en función del formato del archivo (Grolemund & Wickham, 2017).\n\n\n\n5.2.2 Importación de archivos Excel\nEl uso de hojas de cálculo en formato Excel (.xlsx) es frecuente en contextos profesionales y académicos, debido a la flexibilidad y las capacidades de organización que ofrece este software. Para la importación de archivos Excel en R, el paquete readxl proporciona funciones optimizadas que permiten acceder a los datos directamente desde las hojas de cálculo, sin necesidad de convertir los archivos a otros formatos intermedios (Wickham & Bryan, 2023).\nEl procedimiento recomendado para la importación de archivos Excel incluye la instalación y carga del paquete readxl, seguido del uso de la función read_excel(), como se muestra a continuación:\n\n# Instalar y cargar el paquete readxl\nif (!require(\"readxl\")) install.packages(\"readxl\")\n\n\n# Importar un archivo Excel\ndatos_excel &lt;- read_excel(\"ruta/del/archivo/datos.xlsx\",\n                          sheet = \"Hoja1\",  \n                          col_names = TRUE/FALSE) \n\nLos argumentos más relevantes de la función read_excel() se detallan a continuación:\n\nsheet: Este argumento permite seleccionar la hoja específica que se desea importar desde el archivo Excel. Se puede especificar el nombre de la hoja como una cadena de caracteres (por ejemplo, \"Hoja1\") o el número de la hoja (por ejemplo, 1 para la primera hoja) (Wickham & Bryan, 2023).\ncol_names: Este argumento lógico determina si la primera fila de la hoja de cálculo debe ser utilizada como los nombres de las columnas. Si se establece en TRUE, la primera fila se interpretará como los nombres de las variables; si se establece en FALSE, R asignará nombres genéricos a las columnas (Grolemund & Wickham, 2017).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Importación de datos</span>"
    ]
  },
  {
    "objectID": "05_importacion.html#ejemplo-práctico-base-de-datos-de-estudiantes-usac",
    "href": "05_importacion.html#ejemplo-práctico-base-de-datos-de-estudiantes-usac",
    "title": "5  Importación de datos",
    "section": "5.3 Ejemplo práctico: Base de datos de estudiantes USAC",
    "text": "5.3 Ejemplo práctico: Base de datos de estudiantes USAC\nPara consolidar los conceptos de importación de datos, se utilizará como ejemplo una base de datos recopilada en 2002 en la Universidad de San Carlos de Guatemala. Esta base contiene información de 460 estudiantes de distintas facultades y está disponible tanto en formato CSV como Excel. A lo largo del manual, esta base de datos servirá como referencia para los ejercicios y ejemplos prácticos.\n\n5.3.1 Importación de la base de datos\nA continuación, se muestra cómo importar la base de datos en ambos formatos:\n\nImportar archivo CSV:\n\n\n# Importar datos en formato CSV\ndatos_csv &lt;- read.csv(\"datos_estudiantes.csv\", \n                      header = TRUE, \n                      sep = \",\")\n\n\nImportar archivo Excel: Primero, el usuario se debe asegurar de tener instalado y cargado el paquete readxl.\n\n\n# Instalar y cargar el paquete readxl\nif (!require(\"readxl\")) install.packages(\"readxl\")\n\n\n# Importar datos en formato Excel\ndatos_excel &lt;- read_excel(\"datos_estudiantes_2002.xlsx\",\n                         sheet = \"datos\",\n                         col_names = TRUE)\n\n\n\n5.3.2 Verificación de la importación de datos\nUna vez importados los datos, es fundamental comprobar que la importación se realizó correctamente. R ofrece varias funciones útiles para este propósito:\n\nVisualizar las primeras filas del conjunto de datos:\n\n\n# Visualizar las primeras filas de la base de datos\nhead(datos_csv)\n\n\nExaminar la estructura del data frame:\n\n\n# Examinar la estructura de la base de datos\nstr(datos_csv)\n\n\nObtener un resumen estadístico básico:\n\n\n# Resumen estadístico básico de la base de datos\nsummary(datos_csv)\n\n\nConocer las dimensiones del data frame (número de filas y columnas):\n\n\n# Conocer las dimensiones del data frame\ndim(datos_csv)\n\n\n\n5.3.3 Consideraciones adicionales\n\nUbicación de archivos: Cuando se trabaja con proyectos de RStudio, los archivos deben estar en el directorio del proyecto para facilitar su acceso.\nCodificación de caracteres: En caso de problemas con caracteres especiales (ñ, tildes), se puede especificar la codificación:\n\ndatos &lt;- read.csv(\"archivo.csv\", encoding = \"UTF-8\")",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Importación de datos</span>"
    ]
  },
  {
    "objectID": "06_operadores.html",
    "href": "06_operadores.html",
    "title": "6  Operadores en R",
    "section": "",
    "text": "6.1 Operadores de Asignación\nEn el lenguaje de programación R, los operadores constituyen herramientas esenciales que permiten ejecutar cálculos matemáticos, realizar comparaciones lógicas, efectuar asignaciones de valores y manipular estructuras de datos. Su dominio es fundamental para desarrollar análisis estadísticos robustos y para implementar flujos de trabajo reproducibles y eficientes en programación estadística (R Core Team, 2023). Los operadores pueden considerarse los instrumentos básicos de un entorno de trabajo analítico, ya que su correcta aplicación posibilita la construcción de soluciones complejas a partir de operaciones elementales.\nLa clasificación de los operadores en R se realiza en función de la naturaleza de las operaciones que permiten ejecutar. A continuación, se describen las principales categorías de operadores, acompañadas de ejemplos prácticos que ilustran su uso y aplicación en el contexto del análisis de datos.\nEn R, los operadores de asignación cumplen la función de crear objetos y almacenar valores o resultados en ellos, lo que representa uno de los pilares de la programación y la manipulación de datos en este lenguaje. Los dos operadores de asignación más empleados son &lt;- y =, ambos válidos para asignar valores a objetos. Sin embargo, la convención ampliamente aceptada en la comunidad de R es utilizar el operador &lt;-, ya que este evita ambigüedades con el operador lógico de igualdad (==) y contribuye a mantener la claridad y coherencia en el código (Ihaka & Gentleman, 1996).\nPor ejemplo, la instrucción:\nx &lt;- 10\nasigna el valor numérico 10 al objeto denominado x. De manera similar, la instrucción:\ny = 20\nasigna el valor 20 al objeto y, aunque esta forma es menos recomendada en contextos profesionales y académicos. Una vez creados, estos objetos pueden ser utilizados en operaciones posteriores, como se muestra a continuación:\n# Asignación de valores a objetos\nx &lt;- 10          \ny = 20           \n\n# Uso de objetos\nx + y    # Resultado: 30\nLa salida generada por R será:\n[1] 30\nEs relevante señalar que, aunque el operador = puede emplearse para asignar valores, su uso puede inducir a confusiones, especialmente en contextos donde se emplean expresiones lógicas o en la definición de argumentos dentro de funciones, ya que = también se utiliza para asociar valores a parámetros. Por esta razón, se recomienda preferir el uso de &lt;- para la asignación de valores en la mayoría de los casos, siguiendo las mejores prácticas de programación en R (Ihaka & Gentleman, 1996).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Operadores en R</span>"
    ]
  },
  {
    "objectID": "06_operadores.html#operadores-aritméticos",
    "href": "06_operadores.html#operadores-aritméticos",
    "title": "6  Operadores en R",
    "section": "6.2 Operadores aritméticos",
    "text": "6.2 Operadores aritméticos\nLos operadores aritméticos son elementos fundamentales en R, ya que posibilitan la ejecución de operaciones matemáticas tanto básicas como avanzadas. Estos operadores son esenciales para la manipulación de datos numéricos y la realización de cálculos en el ámbito del análisis estadístico. Actúan sobre valores numéricos y producen resultados numéricos, permitiendo así la transformación y el análisis cuantitativo de la información (R Core Team, 2023).\nLa siguiente tabla resume los principales operadores aritméticos disponibles en R, junto con su función, un ejemplo de uso y el resultado esperado:\n\n\n\nOperador\nAcción\nEjemplo\nResultado\n\n\n\n\n+\nSuma\n5 + 3\n8\n\n\n-\nResta\n10 - 4\n6\n\n\n*\nMultiplicación\n6 * 2\n12\n\n\n/\nDivisión\n15 / 3\n5\n\n\n^\nPotencia\n2 ^ 3\n8\n\n\n%/%\nDivisión entera\n17 %/% 5\n3\n\n\n%%\nMódulo o residuo\n17 %% 5\n2\n\n\n\n\n6.2.1 Ejemplo práctico\nEn este ejemplo se utilizan operadores aritméticos para realizar cálculos básicos con dos variables numéricas:\n\n# Definición de variables\na &lt;- 15\nb &lt;- 4\n\n# Suma\nsuma &lt;- a + b           # 19\n\n# Resta\nresta &lt;- a - b          # 11\n\n# Multiplicación\nmultiplicacion &lt;- a * b # 60\n\n# División\ndivision &lt;- a / b       # 3.75\n\n# Potencia\npotencia &lt;- a ^ b       # 50625\n\n# División entera\ndivision_entera &lt;- a %/% b  # 3\n\n# Módulo o residuo\nresiduo &lt;- a %% b            # 3\n\nEn este ejemplo, se observa cómo los operadores aritméticos permiten realizar operaciones matemáticas básicas y obtener resultados numéricos de manera directa y sencilla en R (R Core Team, 2023).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Operadores en R</span>"
    ]
  },
  {
    "objectID": "06_operadores.html#operadores-lógicos",
    "href": "06_operadores.html#operadores-lógicos",
    "title": "6  Operadores en R",
    "section": "6.3 Operadores lógicos",
    "text": "6.3 Operadores lógicos\nLos operadores lógicos desempeñan un papel crucial en la evaluación de condiciones y la toma de decisiones dentro del código en R. Estos operadores permiten comparar valores y establecer reglas condicionales, lo cual es esencial para tareas como el filtrado de datos, la selección de subconjuntos y la implementación de estructuras de control. Los operadores lógicos trabajan con valores booleanos (TRUE o FALSE), y su correcta utilización facilita la construcción de análisis estadísticos robustos y flexibles (R Core Team, 2023).\nA continuación, se presenta una tabla que resume los principales operadores lógicos en R, junto con su función, un ejemplo de uso y el resultado esperado:\n\n\n\n\n\n\n\n\n\nOperador\nAcción\nEjemplo\nResultado\n\n\n\n\n&gt;\nMayor que\n5 &gt; 3\nTRUE\n\n\n&lt;\nMenor que\n5 &lt; 3\nFALSE\n\n\n&gt;=\nMayor o igual que\n5 &gt;= 5\nTRUE\n\n\n&lt;=\nMenor o igual que\n5 &lt;= 4\nFALSE\n\n\n==\nIgualdad\n5 == 5\nTRUE\n\n\n!=\nDesigualdad\n5 != 3\nTRUE\n\n\n&\nY lógico (AND)\n(5 &gt; 3) & (4 &gt; 2)\nTRUE\n\n\n|\nO lógico (OR)\n(4 &lt; 2)  | (5 &gt; 3)\nTRUE\n\n\n!\nNegación lógica\n!(5 &gt; 3)\nFALSE\n\n\n\n\n6.3.1 Ejemplo práctico\nA continuación, se presenta un ejemplo práctico donde se emplean operadores lógicos para realizar comparaciones y evaluaciones condicionales en un contexto de análisis de datos:\n\n# Comparaciones simples\nedad &lt;- 25\nes_mayor &lt;- edad &gt; 18          # TRUE, porque 25 es mayor que 18\nes_menor &lt;- edad &lt; 30          # TRUE, porque 25 es menor que 30\nes_igual &lt;- edad == 25         # TRUE, porque 25 es igual a 25\nes_diferente &lt;- edad != 20     # TRUE, porque 25 es diferente de 20\n\n# Operaciones lógicas compuestas\npeso_Kg &lt;- 70\naltura &lt;- 1.75\nimc &lt;- peso_Kg / (altura^2)    # Cálculo del índice de masa corporal\n\nsobrepeso &lt;- imc &gt;= 25 & imc &lt; 30      \nsobrepeso   # FALSE, el IMC está fuera del rango de sobrepeso\n\n[1] FALSE\n\npeso_normal &lt;- imc &gt;= 18.5 & imc &lt; 25  \npeso_normal # TRUE, el IMC está en el rango de peso normal\n\n[1] TRUE\n\n\nEn este ejemplo, se ilustra cómo los operadores lógicos permiten evaluar condiciones tanto simples como compuestas, facilitando la clasificación de datos y la toma de decisiones dentro del análisis estadístico en R (R Core Team, 2023). El cálculo del índice de masa corporal (IMC) y la posterior evaluación de si una persona se encuentra en un rango de peso normal o en sobrepeso son ejemplos claros de la aplicación práctica de estos operadores.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Operadores en R</span>"
    ]
  },
  {
    "objectID": "06_operadores.html#operadores-de-manipulación-de-datos",
    "href": "06_operadores.html#operadores-de-manipulación-de-datos",
    "title": "6  Operadores en R",
    "section": "6.4 Operadores de Manipulación de Datos",
    "text": "6.4 Operadores de Manipulación de Datos\nEn R, los operadores de manipulación de datos desempeñan una función esencial en el acceso, selección y modificación de elementos dentro de diversas estructuras de datos como vectores, matrices, listas y data frames. El dominio de estos operadores resulta indispensable para trabajar con datos organizados y ejecutar análisis estadísticos de manera eficiente, ya que permiten extraer, transformar y analizar información específica de grandes conjuntos de datos (R Core Team, 2023).\nLa siguiente tabla resume los principales operadores de manipulación de datos en R, su función, un ejemplo de uso y el resultado esperado:\n\n\n\n\n\n\n\n\n\nOperador\nAcción\nEjemplo\nResultado\n\n\n\n\n[]\nAcceso a elementos por posición\nvector[1]\nPrimer elemento del vector\n\n\n[ , ]\nAcceso a filas y columnas en un data frame\ndata[1, 2]\nElemento en la fila 1, columna 2\n\n\n$\nAcceso a una columna específica en un data frame\ndata$columna\nColumna seleccionada\n\n\n:\nCreación de secuencias\n1:10\nSecuencia del 1 al 10\n\n\n\nEstos operadores constituyen herramientas fundamentales para la manipulación de datos en R, permitiendo a los analistas y científicos de datos acceder con precisión a los elementos que necesitan procesar. Su correcta aplicación facilita la implementación de análisis estadísticos complejos y la generación de visualizaciones informativas (Wickham & Grolemund, 2017).\n\n6.4.1 Ejemplo Práctico\nA continuación, se presenta un ejemplo práctico utilizando fragmentos de código en R para ilustrar el uso de estos operadores de manipulación de datos:\n\n# Crear un vector\nvector &lt;- c(10, 20, 30, 40, 50)\n\n# Acceder al primer elemento\nvector[1]       # Resultado: 10\n\n[1] 10\n\n# Crear un data frame para el ejemplo\ndata &lt;- data.frame(\n  nombre = c(\"Juan\", \"Ana\", \"Luis\"),\n  edad = c(25, 30, 22),\n  peso = c(70, 65, 80)\n)\n\n# Acceder a una columna completa\ndata$edad      # Resultado: 25, 30, 22\n\n[1] 25 30 22\n\n# Acceder a un elemento específico\ndata[2, 3]     # Resultado: 65 (peso de Ana)\n\n[1] 65\n\n# Crear una secuencia de números del 1 al 10\nsecuencia &lt;- 1:10   # Resultado: 1, 2, 3, ..., 10               \nsecuencia\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nEste ejemplo muestra cómo los operadores de manipulación de datos permiten seleccionar elementos individuales, columnas completas o secuencias de valores dentro de las estructuras de datos más utilizadas en R. Estas operaciones son fundamentales para filtrar, transformar y analizar información de manera precisa y eficiente en el entorno estadístico (R Core Team, 2023).\n\n\n6.4.2 Aplicaciones Avanzadas\nLos operadores de manipulación de datos también pueden combinarse para realizar selecciones más complejas. Por ejemplo:\n\n# Seleccionar múltiples elementos de un vector\n## Selecciona los elementos en las posiciones 1, 3 y 5\nvector[c(1, 3, 5)]   \n\n[1] 10 30 50\n\n# Seleccionar un subconjunto de filas y columnas en un data frame\n## Selecciona nombre y peso de personas mayores de 25 años\ndata[data$edad &gt; 25, c(\"nombre\", \"peso\")]   \n\n  nombre peso\n2    Ana   65\n\n# Utilizar secuencias para seleccionar rangos de elementos\n## Selecciona los elementos desde la posición 2 hasta la 4\nvector[2:4]   \n\n[1] 20 30 40\n\n\nEstas aplicaciones avanzadas demuestran la flexibilidad y potencia de los operadores de manipulación de datos en R, permitiendo a los usuarios realizar selecciones precisas y complejas con una sintaxis relativamente sencilla (Wickham, 2016).",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Operadores en R</span>"
    ]
  },
  {
    "objectID": "06.1_funciones.html",
    "href": "06.1_funciones.html",
    "title": "7  Funciones en R",
    "section": "",
    "text": "7.1 Definición y características de las funciones en R\nLas funciones constituyen uno de los componentes esenciales y estructurales de la programación en R, desempeñando un papel central en la automatización de tareas, la reducción de la redundancia y la mejora de la legibilidad y mantenibilidad del código. Una función puede entenderse como un bloque de instrucciones encapsuladas que, al ser invocadas, ejecutan una tarea específica sobre uno o varios valores de entrada, denominados argumentos, y devuelven un resultado. El dominio en la creación y utilización de funciones es indispensable para aprovechar plenamente las capacidades de R en el análisis estadístico, la manipulación de datos y el desarrollo de soluciones eficientes (R Core Team, 2023; Wickham & Grolemund, 2017).\nEl presente capítulo explora en profundidad el concepto de función en R, su estructura fundamental, los tipos existentes y el proceso para definir funciones personalizadas que respondan a necesidades analíticas particulares.\nEn R, una función se define formalmente como un objeto que recibe uno o más argumentos, ejecuta una secuencia de operaciones sobre ellos y retorna un valor como resultado. El uso de funciones permite modularizar el código, facilitando su reutilización, mantenimiento y escalabilidad, aspectos cruciales en el desarrollo de proyectos de análisis de datos de cualquier envergadura (Chambers, 2008).\nToda función en R está compuesta por los siguientes elementos fundamentales:\nLa correcta estructuración de estos elementos permite desarrollar funciones robustas, reutilizables y fácilmente integrables en flujos de trabajo complejos.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Funciones en R</span>"
    ]
  },
  {
    "objectID": "06.1_funciones.html#definición-y-características-de-las-funciones-en-r",
    "href": "06.1_funciones.html#definición-y-características-de-las-funciones-en-r",
    "title": "7  Funciones en R",
    "section": "",
    "text": "Nombre: Identificador único que permite invocar la función dentro del entorno de trabajo.\nArgumentos: Valores de entrada que la función requiere para ejecutar sus operaciones internas. Los argumentos pueden tener valores por defecto, lo que otorga flexibilidad y adaptabilidad a la función.\nCuerpo: Conjunto de instrucciones que definen el comportamiento de la función y especifican las operaciones a realizar sobre los argumentos recibidos.\nValor de retorno: Resultado que la función entrega tras la ejecución de su cuerpo. En R, este valor se especifica mediante la instrucción return(), aunque si no se utiliza explícitamente, la función devolverá el último valor evaluado.\n\n\n\n7.1.1 Tipos de funciones\nLas funciones en R pueden clasificarse en dos grandes categorías, según su origen y propósito:\n\nFunciones predefinidas (built-in): Son aquellas incluidas en el núcleo del lenguaje R o en paquetes adicionales ampliamente utilizados. Estas funciones abarcan una extensa variedad de tareas, que van desde operaciones matemáticas y estadísticas básicas, hasta la manipulación avanzada de datos y la generación de gráficos. Su uso es fundamental para la resolución eficiente de problemas comunes y para la implementación de análisis estándar (R Core Team, 2023).\nFunciones personalizadas (user-defined): Son funciones definidas por el usuario para abordar necesidades específicas que no pueden resolverse mediante las funciones predefinidas. La creación de funciones personalizadas resulta especialmente útil para automatizar procesos repetitivos, encapsular procedimientos complejos y adaptar el análisis a contextos particulares. El desarrollo de funciones propias fomenta la modularidad y la reutilización del código, contribuyendo a la eficiencia y escalabilidad de los proyectos (Wickham & Grolemund, 2017; Chambers, 2008).\n\n\n\n7.1.2 Funciones predefinidas en R\nLas funciones predefinidas en R constituyen un conjunto de herramientas esenciales que facilitan la ejecución de operaciones comunes de manera eficiente y directa. Integradas en el núcleo del lenguaje y en diversos paquetes complementarios, estas funciones permiten realizar cálculos estadísticos, manipular datos y obtener resúmenes informativos sin necesidad de definir procedimientos adicionales. Su uso simplifica la resolución de tareas habituales y contribuye a la claridad y concisión del código (R Core Team, 2023).\nA continuación, se presentan algunos ejemplos de funciones predefinidas ampliamente utilizadas en R, junto con fragmentos de código que ilustran su aplicación:\n\n# Definición de un vector de datos\ndatos &lt;- c(1:25)\n\n# Cálculo de la media aritmética\nmean(datos)           # Resultado: 13\n\n[1] 13\n\n# Suma de los elementos del vector\nsum(datos)             # Resultado: 325\n\n[1] 325\n\n# Cálculo de la desviación estándar\nsd(datos)        # Resultado: 7.359801\n\n[1] 7.359801\n\n# Resumen estadístico del conjunto de datos\nsummary(datos)   # Resultado:\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n      1       7      13      13      19      25 \n\n\nEstas funciones están disponibles de forma predeterminada en R y permiten realizar análisis estadísticos básicos de manera inmediata, sin requerir definiciones adicionales por parte del usuario. Su dominio es esencial para el trabajo cotidiano con datos en R (R Core Team, 2023). Además de las funciones mostradas, R ofrece una amplia gama de funciones predefinidas para tareas como la manipulación de cadenas de texto (grep, gsub), la gestión de fechas y horas (Sys.Date, strptime), la generación de números aleatorios (runif, rnorm) y la creación de gráficos (plot, hist), entre muchas otras.\n\n\n7.1.3 Funciones personalizadas en R\nLas funciones personalizadas permiten al usuario definir procedimientos específicos para resolver problemas que no están cubiertos por las funciones predefinidas. Este tipo de funciones resulta especialmente útil para automatizar tareas repetitivas o implementar cálculos complejos adaptados a necesidades particulares. La creación de funciones personalizadas contribuye a la organización y reutilización del código, facilitando el desarrollo de análisis más eficientes (R Core Team, 2023).\nA continuación, se muestra un ejemplo de cómo definir y utilizar una función personalizada en R para calcular el área de un círculo a partir de su radio:\n\n# Definición de una función personalizada para calcular el área de un círculo\narea_circulo &lt;- function(radio) {\n  area &lt;- pi * radio^2\n  return(area)\n}\n\n# Uso de la función personalizada\narea &lt;- area_circulo(5)   # Resultado: 78.53982\n\nEn este ejemplo, el usuario define la lógica de la función, especifica el argumento necesario (radio) y utiliza la función para obtener el resultado deseado. La función area_circulo encapsula la fórmula matemática para el cálculo del área, lo que permite reutilizarla fácilmente con diferentes valores de radio sin necesidad de repetir el código.\n\n\n7.1.4 Diferencias entre funciones predefinidas y personalizadas\nLas principales diferencias entre funciones predefinidas y personalizadas en R se resumen en la siguiente tabla:\n\n\n\n\n\n\n\n\nCaracterística\nFunciones predefinidas\nFunciones personalizadas\n\n\n\n\nDisponibilidad\nIncluidas en R o en paquetes\nCreadas por el usuario\n\n\nFlexibilidad\nLimitada a las tareas para las que fueron diseñadas\nTotalmente adaptables a las necesidades del usuario\n\n\nEjemplos\nmean(), sum(), sd(), summary()\narea_circulo()\n\n\nReutilización\nReutilizables en cualquier script\nReutilizables si se definen en el entorno o se importan\n\n\nMantenimiento\nActualizadas por los desarrolladores de R o del paquete\nMantenidas por el usuario\n\n\nDocumentación\nGeneralmente bien documentadas en la ayuda de R\nRequieren documentación explícita por parte del usuario\n\n\n\nEsta distinción permite seleccionar el tipo de función más adecuado según el contexto y los objetivos del análisis (R Core Team, 2023). En general, se recomienda utilizar funciones predefinidas siempre que sea posible, ya que suelen estar optimizadas y bien probadas. Sin embargo, cuando se requiere una funcionalidad específica que no está disponible en las funciones predefinidas, la creación de funciones personalizadas es la solución más adecuada.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Funciones en R</span>"
    ]
  },
  {
    "objectID": "06.1_funciones.html#usos-y-beneficios-de-las-funciones-en-r",
    "href": "06.1_funciones.html#usos-y-beneficios-de-las-funciones-en-r",
    "title": "7  Funciones en R",
    "section": "7.2 Usos y beneficios de las funciones en R",
    "text": "7.2 Usos y beneficios de las funciones en R\nEl uso de funciones en R representa una estrategia fundamental para optimizar el desarrollo y la gestión de proyectos de análisis de datos. Una de las ventajas más destacadas es la reutilización del código. Cuando se define una función, esta puede emplearse en diferentes partes de un mismo proyecto o incluso en proyectos distintos, lo que reduce la duplicación de instrucciones y facilita el mantenimiento del código. Esta reutilización contribuye a minimizar errores, ya que la lógica centralizada en una función puede ser actualizada o corregida en un solo lugar, propagando automáticamente los cambios a todas las instancias donde se utiliza.\nOtro beneficio esencial es la modularidad. Las funciones permiten descomponer problemas complejos en componentes más simples y manejables. Esta descomposición facilita la organización y la estructura del código, haciendo que cada función se enfoque en una tarea específica. Como resultado, el proceso de depuración y mejora del código se vuelve más eficiente, ya que es posible identificar y corregir errores en módulos independientes sin afectar el resto del programa.\nLa legibilidad y la mantenibilidad del código también se ven favorecidas por el uso de funciones. Encapsular operaciones dentro de funciones contribuye a que el código sea más claro y comprensible, lo que facilita su revisión y la colaboración entre diferentes usuarios o equipos de trabajo. Además, la automatización de tareas repetitivas mediante funciones incrementa la eficiencia y ahorra tiempo en la ejecución de procesos rutinarios, permitiendo que el analista se concentre en aspectos más estratégicos del análisis (R Core Team, 2023; Wickham & Grolemund, 2017).\n\n7.2.1 Ejemplo de automatización con funciones\nPara ilustrar estos beneficios, se puede considerar el caso en el que se requiere calcular el área de varios círculos con diferentes radios. En lugar de repetir manualmente el cálculo para cada radio, basta con aplicar una función personalizada a un vector de radios. Por ejemplo:\n\nradios &lt;- c(1, 2, 3, 4, 5)\nareas &lt;- area_circulo(radios)\nareas  \n\nEl resultado será un vector con las áreas correspondientes a cada radio:\n\n\n[1]  3.141593 12.566371 28.274334 50.265482 78.539816\n\n\nEste ejemplo demuestra cómo el uso de funciones personalizadas permite automatizar cálculos y trabajar de manera más eficiente con conjuntos de datos, reafirmando la importancia de las funciones en la programación con R (R Core Team, 2023).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Funciones en R</span>"
    ]
  },
  {
    "objectID": "06.1_funciones.html#cómo-crear-funciones-en-r-sintaxis-y-ejemplos-básicos",
    "href": "06.1_funciones.html#cómo-crear-funciones-en-r-sintaxis-y-ejemplos-básicos",
    "title": "7  Funciones en R",
    "section": "7.3 Cómo crear funciones en R: Sintaxis y ejemplos básicos",
    "text": "7.3 Cómo crear funciones en R: Sintaxis y ejemplos básicos\nLa definición de funciones en R se realiza mediante una sintaxis clara y estructurada, lo que facilita la creación de procedimientos personalizados para resolver tareas específicas. Comprender la estructura básica de una función es fundamental para aprovechar al máximo la modularidad y reutilización del código en R (R Core Team, 2023; Wickham & Grolemund, 2017).\n\n7.3.1 Sintaxis básica\nLa sintaxis general para crear una función en R es la siguiente:\n\nnombre_funcion &lt;- function(argumento1, argumento2, ...) {\n  # Instrucciones y operaciones\n  return(resultado)\n}\n\n\n\n7.3.2 Elementos clave de una función\nCada función en R se compone de los siguientes elementos:\n\nNombre de la función: que debe ser descriptivo y reflejar claramente la tarea que realiza.\nArgumentos: representan los valores de entrada requeridos por la función. Es posible asignar valores por defecto a estos argumentos para hacer la función más flexible.\nCuerpo de la función: donde se incluyen las operaciones y cálculos que se ejecutan al llamar a la función.\nValor de retorno: que se especifica mediante la instrucción return(). Si no se utiliza return(), la función devolverá automáticamente el último valor evaluado en su cuerpo.\n\n\n\n7.3.3 Ejemplo básico\nA continuación, se muestra un ejemplo de una función personalizada que convierte temperaturas de grados Celsius a Fahrenheit:\n\n# Función para convertir grados Celsius a Fahrenheit\ncelsius_a_fahrenheit &lt;- function(celsius) {\n  fahrenheit &lt;- (celsius * 9/5) + 32\n  return(fahrenheit)\n}\n\n# Uso de la función\ncelsius_a_fahrenheit(25)   # Resultado: 77\n\n[1] 77",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Funciones en R</span>"
    ]
  },
  {
    "objectID": "07_paquetes.html",
    "href": "07_paquetes.html",
    "title": "8  Paquetes en R: Expansión y Modularidad del Entorno",
    "section": "",
    "text": "8.1 Fundamentos de los Paquetes en R: Definición y Alcance\nLos paquetes en R constituyen una de las herramientas más potentes del lenguaje, ya que permiten ampliar sus funcionalidades básicas y abordar tareas especializadas de manera eficiente. Gracias a los paquetes, es posible acceder a funciones, datos y documentación desarrollados por expertos, lo que facilita la realización de análisis estadísticos, manipulación de datos y visualización avanzada, entre otras aplicaciones. Esta arquitectura modular posibilita que R se adapte a las necesidades de usuarios de distintos campos, promoviendo la reutilización de código y la colaboración científica (R Core Team, 2023).\nUn paquete en R se define como una colección estructurada de funciones, conjuntos de datos y documentación que extiende las capacidades del entorno base. Estos paquetes, desarrollados tanto por la comunidad como por equipos especializados, están orientados a resolver problemas concretos en áreas como la estadística, la ciencia de datos, la visualización gráfica y la programación. La modularidad de los paquetes permite que los usuarios seleccionen e instalen únicamente las herramientas que requieren para sus proyectos, optimizando así el uso de recursos y facilitando la actualización de funcionalidades (Wickham, 2016; R Core Team, 2023).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Paquetes en R: Expansión y Modularidad del Entorno</span>"
    ]
  },
  {
    "objectID": "07_paquetes.html#qué-es-un-paquete-en-r",
    "href": "07_paquetes.html#qué-es-un-paquete-en-r",
    "title": "8  Paquetes en R",
    "section": "",
    "text": "8.1.1 Características principales de los paquetes\nLas características más relevantes de los paquetes en R se resumen a continuación:\n\nFunciones especializadas: Cada paquete incluye funciones diseñadas para tareas específicas, como crear gráficos, realizar análisis estadísticos o manipular datos.\nDocumentación: Los paquetes incluyen documentación detallada que explica cómo utilizarlos, con ejemplos prácticos.\nDatos de ejemplo: Muchos paquetes incluyen conjuntos de datos que permiten practicar y entender su funcionalidad.\n\n\n\n8.1.2 Importancia del uso de paquetes en R\nEl uso de paquetes resulta fundamental para aprovechar plenamente el potencial de R, ya que ofrecen extensibilidad, eficiencia y especialización. Los paquetes permiten realizar tareas que no están disponibles en el entorno base, simplifican procesos complejos y proporcionan soluciones adaptadas a áreas específicas, como la agronomía, la biología o la economía. Además, la comunidad activa de R garantiza la actualización y el soporte continuo de una amplia variedad de paquetes, lo que contribuye a mantener el lenguaje a la vanguardia en el análisis de datos (Wickham & Grolemund, 2017).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Paquetes en R</span>"
    ]
  },
  {
    "objectID": "07_paquetes.html#instalación-y-carga-de-paquetes",
    "href": "07_paquetes.html#instalación-y-carga-de-paquetes",
    "title": "8  Paquetes en R",
    "section": "8.2 Instalación y carga de paquetes",
    "text": "8.2 Instalación y carga de paquetes\nLa gestión de paquetes en R es un proceso fundamental para acceder a herramientas especializadas y ampliar las capacidades del entorno base. La mayoría de los paquetes se obtienen desde CRAN (Comprehensive R Archive Network), el repositorio oficial que alberga una amplia variedad de recursos para diferentes áreas de aplicación (R Core Team, 2023).\n\n8.2.1 Proceso de instalación\nPara instalar un paquete desde CRAN, se utiliza la función install.packages(). Por ejemplo, para instalar el paquete ggplot2, ampliamente utilizado para la visualización de datos, se emplea la siguiente instrucción:\n\n# Instalación del paquete ggplot2\ninstall.packages(\"ggplot2\")\n\nLa instalación de un paquete es un proceso que solo debe realizarse una vez en el sistema.\n\n\n8.2.2 Carga de paquetes\nDespués de instalar un paquete, es necesario cargarlo en cada nueva sesión de trabajo para poder utilizar sus funciones. Esto se realiza mediante la función library():\n\n# Cargar el paquete ggplot2\nlibrary(ggplot2)\n\nLa carga de paquetes debe repetirse cada vez que se inicia una nueva sesión en R, ya que los paquetes no se cargan automáticamente al abrir el entorno.\n\n\n8.2.3 Automatización de la instalación y carga\nPara asegurar que un paquete esté disponible y evitar errores al compartir scripts, es recomendable automatizar el proceso de verificación, instalación y carga. La siguiente estructura permite comprobar si el paquete está instalado y, en caso contrario, instalarlo y cargarlo automáticamente:\n\n# Verificar e instalar automáticamente un paquete\nif (!require(\"ggplot2\")) install.packages(\"ggplot2\")\n\nEste enfoque contribuye a la reproducibilidad del código y facilita el intercambio de scripts entre usuarios, garantizando que todas las dependencias necesarias estén disponibles en el entorno de trabajo (R Core Team, 2023).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Paquetes en R</span>"
    ]
  },
  {
    "objectID": "07_paquetes.html#paquetes-recomendados-para-tareas-específicas",
    "href": "07_paquetes.html#paquetes-recomendados-para-tareas-específicas",
    "title": "8  Paquetes en R",
    "section": "8.3 Paquetes recomendados para tareas específicas",
    "text": "8.3 Paquetes recomendados para tareas específicas\nEn el ámbito del análisis estadístico y la manipulación de datos, existen diversos paquetes que facilitan tareas específicas y permiten realizar análisis complejos de manera eficiente. A continuación, se presenta una clasificación de los paquetes más relevantes según su área de aplicación:\n\n\n\n\n\n\n\n\nÁrea\nPaquete\nDescripción\n\n\n\n\nManipulación de datos\ndplyr\nFacilita la transformación y manipulación de datos mediante funciones intuitivas\n\n\n\ntidyr\nPermite reorganizar datos entre formatos ancho y largo\n\n\n\ndata.table\nOptimizado para el manejo de grandes conjuntos de datos\n\n\nAnálisis exploratorio\nDataExplorer\nAutomatiza el análisis exploratorio de datos\n\n\n\nsummarytools\nGenera resúmenes estadísticos detallados\n\n\n\npsych\nProporciona funciones para análisis psicométricos y estadística descriptiva\n\n\nAnálisis estadístico\nstats\nIncluye funciones base para pruebas estadísticas comunes\n\n\n\nagricolae\nEspecializado en diseños experimentales y análisis agrícolas\n\n\n\nAgroR\nProporciona funciones y herramientas para análisis estadísticos en agronomía\n\n\n\ncar\nFacilita análisis de regresión avanzados\n\n\nVisualización\nggplot2\nPermite crear gráficos personalizados de alta calidad\n\n\n\nplotly\nGenera gráficos interactivos\n\n\n\n\n8.3.1 Instalación y carga de paquetes esenciales\nEl siguiente código muestra cómo instalar y cargar un conjunto básico de paquetes para análisis estadísticos:\n\n# Paquetes para manipulación y visualización de datos\nif (!require(\"tidyverse\")) install.packages(\"tidyverse\")\nif (!require(\"data.table\")) install.packages(\"data.table\")\n\n# Paquetes para análisis exploratorio\nif (!require(\"DataExplorer\")) install.packages(\"DataExplorer\")\nif (!require(\"psych\")) install.packages(\"psych\")\n\n# Paquetes para análisis estadísticos especializados\nif (!require(\"agricolae\")) install.packages(\"agricolae\")\nif (!require(\"AgroR\")) install.packages(\"AgroR\")\nif (!require(\"car\")) install.packages(\"car\")\n\n# Paquetes para manejo de archivos\nif (!require(\"readxl\")) install.packages(\"readxl\")\nif (!require(\"writexl\")) install.packages(\"writexl\")\n\nEstos paquetes proporcionan un conjunto robusto de herramientas para realizar análisis estadísticos completos, desde la exploración inicial de datos hasta análisis especializados en áreas específicas como la agronomía o la psicometría (R Core Team, 2023).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Paquetes en R</span>"
    ]
  },
  {
    "objectID": "07_paquetes.html#ejemplo-práctico-integración-de-paquetes-en-un-análisis-estadístico",
    "href": "07_paquetes.html#ejemplo-práctico-integración-de-paquetes-en-un-análisis-estadístico",
    "title": "8  Paquetes en R",
    "section": "8.4 Ejemplo práctico: Integración de paquetes en un análisis estadístico",
    "text": "8.4 Ejemplo práctico: Integración de paquetes en un análisis estadístico\nEste ejemplo ilustra la aplicación práctica de diversos paquetes de R en un análisis estadístico, basado en el estudio presentado por López y González (2016) sobre el crecimiento de plántulas de pino maximinoii. El código completo está disponible en: https://github.com/Ludwing-MJ/Paquetes_Ej.\n\n8.4.1 Contexto del estudio\nSe evaluó el efecto de cinco tratamientos de preparación del terreno sobre el crecimiento en altura de plántulas de pino maximinoii. El experimento incluyó 25 parcelas, con cinco repeticiones por tratamiento, asignadas aleatoriamente. Las mediciones de altura se realizaron después de cinco años de crecimiento.\n\n\n8.4.2 Implementación del análisis\n\n# Instalación y carga de paquetes necesarios\n## Incluye ggplot2, dplyr, tidyr\nif (!require(\"tidyverse\")) install.packages(\"tidyverse\")\n## Diseños experimentales agrícolas\nif (!require(\"agricolae\")) install.packages(\"agricolae\")\n## Importación de archivos Excel\nif (!require(\"readxl\")) install.packages(\"readxl\")    \n## Exportación a Excel\nif (!require(\"writexl\")) install.packages(\"writexl\")    \n## Establecer directorio de trabajo automaticamente\nif (!require(\"rstudioapi\")) install.packages(\"rstudioapi\")\n\n\n# Carga de datos\naltura_pino &lt;- read_excel(\"datos_arboles.xlsx\")\n\n# Análisis de varianza y comparaciones múltiples\nmodelo_anova &lt;- aov(altura_ft ~ tratamiento, data = altura_pino)\nsummary (modelo_anova)\n\n            Df Sum Sq Mean Sq F value  Pr(&gt;F)   \ntratamiento  4  34.64    8.66   5.851 0.00276 **\nResiduals   20  29.60    1.48                   \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\ncomparacion_tukey &lt;- HSD.test(modelo_anova, \"tratamiento\")\ncomparacion_tukey$groups\n\n  altura_ft groups\nB      14.4      a\nA      13.4     ab\nE      11.8      b\nC      11.6      b\nD      11.4      b\n\n# Visualización de resultados\nggplot(altura_pino, aes(x = tratamiento, y = altura_ft, fill = tratamiento)) +\n    geom_boxplot() +\n    labs(title = \"Altura por Tratamiento\",\n         x = \"Tratamiento\",\n         y = \"Altura en pies\") +\n    theme_minimal() +\n    theme(legend.position = \"none\")\n\n\n\n\n\n\n\n# Exportación de resultados\nwrite_xlsx(comparacion_tukey$groups, \"resultados_tukey.xlsx\")\nggsave(\"ggplot_pino.png\")\n\nEste ejemplo demuestra cómo los diferentes paquetes se integran en un flujo de trabajo coherente, desde la preparación de datos hasta la visualización y exportación de resultados. El uso de paquetes especializados como agricolae para el análisis estadístico, ggplot2 para la visualización y writexl para la exportación de resultados, simplifica significativamente el proceso de análisis y presentación de datos (López & González, 2016; R Core Team, 2023).\nLa estructura modular del código y el uso de paquetes especializados facilitan la reproducibilidad del análisis y permiten adaptarlo fácilmente a otros conjuntos de datos similares.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Paquetes en R</span>"
    ]
  },
  {
    "objectID": "08.1_manipulacion.html",
    "href": "08.1_manipulacion.html",
    "title": "9  Introducción a la manipulación de datos en R",
    "section": "",
    "text": "9.1 Principales tareas de manipulación de datos\nLa manipulación de datos representa una etapa crítica en el proceso de análisis estadístico, ya que los datos raramente se encuentran en condiciones óptimas para su análisis inmediato. Es común que los conjuntos de datos contengan errores, valores faltantes, duplicados o estén organizados de manera poco conveniente para los objetivos del estudio. Por ello, la manipulación de datos es esencial para transformar los datos crudos en información útil, confiable y lista para el análisis estadístico y la visualización. Sin una adecuada manipulación, los resultados pueden ser erróneos o difíciles de interpretar, lo que afecta la validez y la reproducibilidad de los análisis (Wickham & Grolemund, 2017; R Core Team, 2023).\nLa manipulación de datos en R abarca un conjunto de tareas fundamentales que permiten preparar la información para su análisis estadístico. Estas tareas son necesarias para garantizar que los datos sean consistentes, completos y estén organizados de acuerdo con los requerimientos del análisis a realizar (Wickham & Grolemund, 2017).",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introducción a la manipulación de datos en R</span>"
    ]
  },
  {
    "objectID": "08.1_manipulacion.html#principales-tareas-de-manipulación-de-datos",
    "href": "08.1_manipulacion.html#principales-tareas-de-manipulación-de-datos",
    "title": "9  Introducción a la manipulación de datos en R",
    "section": "",
    "text": "9.1.1 Filtrado de datos\nEl filtrado consiste en seleccionar subconjuntos de datos que cumplen ciertas condiciones específicas. Esta tarea es fundamental para enfocar el análisis en grupos de interés, eliminar registros no relevantes o excluir observaciones que puedan distorsionar los resultados. Por ejemplo, se puede filtrar una base de datos para analizar únicamente los registros de un grupo demográfico particular o eliminar casos con información incompleta (R Core Team, 2023).\n\n\n9.1.2 Selección de variables\nLa selección de variables implica elegir únicamente las columnas o variables relevantes para el análisis. Esta tarea simplifica el conjunto de datos, reduce la complejidad del análisis y facilita la interpretación de los resultados. Seleccionar las variables adecuadas es especialmente importante cuando se trabaja con bases de datos extensas o con información redundante (Wickham & Grolemund, 2017).\n\n\n9.1.3 Transformación de datos\nLa transformación de datos abarca la creación de nuevas variables, la modificación de valores existentes o la recodificación de categorías. Estas operaciones permiten adaptar los datos a los requerimientos del análisis estadístico, por ejemplo, convirtiendo variables categóricas en numéricas, calculando índices o agrupando categorías similares. La transformación es clave para preparar los datos antes de aplicar técnicas estadísticas específicas (R Core Team, 2023).\n\n\n9.1.4 Agregación de información\nLa agregación consiste en resumir la información contenida en los datos, calculando medidas como promedios, totales, conteos o proporciones por grupo. Esta tarea es fundamental para comparar tendencias, identificar patrones y realizar análisis descriptivos o inferenciales. La agregación permite sintetizar grandes volúmenes de datos en resúmenes comprensibles y útiles para la toma de decisiones (Wickham & Grolemund, 2017).\n\n\n9.1.5 Reestructuración de datos\nLa reestructuración de datos implica cambiar la forma en que los datos están organizados, por ejemplo, convirtiendo un conjunto de datos de formato ancho a largo o viceversa. Esta tarea es necesaria cuando la estructura original de los datos no es compatible con las técnicas estadísticas o de visualización que se desean aplicar. La reestructuración facilita la aplicación de modelos y la generación de gráficos adecuados (Wickham & Grolemund, 2017).",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introducción a la manipulación de datos en R</span>"
    ]
  },
  {
    "objectID": "08.1_manipulacion.html#enfoques-disponibles-en-r-para-la-manipulación-de-datos",
    "href": "08.1_manipulacion.html#enfoques-disponibles-en-r-para-la-manipulación-de-datos",
    "title": "9  Introducción a la manipulación de datos en R",
    "section": "9.2 Enfoques disponibles en R para la manipulación de datos",
    "text": "9.2 Enfoques disponibles en R para la manipulación de datos\nR ofrece dos enfoques principales para la manipulación de datos, cada uno con características y ventajas particulares que se adaptan a diferentes necesidades y niveles de experiencia del usuario (R Core Team, 2023).\n\n9.2.1 Herramientas base de R\nLas herramientas base de R incluyen funciones integradas como el uso de corchetes para seleccionar filas y columnas, así como funciones como subset(), aggregate() y tapply(). Estas herramientas permiten realizar operaciones fundamentales de manipulación de datos de manera flexible y eficiente. Sin embargo, la sintaxis puede resultar menos intuitiva para quienes se inician en R, y las operaciones complejas pueden requerir múltiples pasos o combinaciones de funciones (R Core Team, 2023).\n\n\n9.2.2 El enfoque tidyverse\nEl tidyverse es un conjunto de paquetes desarrollados para simplificar y estandarizar la manipulación de datos en R. Entre estos paquetes destacan dplyr y tidyr, que ofrecen funciones específicas para filtrar, seleccionar, transformar y reestructurar datos de manera clara y legible. El uso del tidyverse facilita la construcción de flujos de trabajo reproducibles y eficientes, y su sintaxis está diseñada para ser accesible tanto para principiantes como para usuarios avanzados. Además, el tidyverse promueve el principio de “datos ordenados” (tidy data), que facilita la aplicación de técnicas estadísticas y la generación de visualizaciones (Wickham & Grolemund, 2017).",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Introducción a la manipulación de datos en R</span>"
    ]
  },
  {
    "objectID": "08.2_manipulacion.html",
    "href": "08.2_manipulacion.html",
    "title": "10  Manipulación de Datos con Herramientas Base de R",
    "section": "",
    "text": "10.1 Datos de ejemplo\nLa manipulación de datos con herramientas base de R constituye una etapa esencial en el flujo de trabajo estadístico clásico. Antes de aplicar técnicas como el análisis de varianza, la regresión lineal o la comparación de medias, es necesario preparar los datos para asegurar su integridad y adecuación al análisis. Las funciones básicas de R permiten seleccionar, filtrar, transformar, agrupar y limpiar datos de manera eficiente, facilitando la obtención de resultados estadísticos válidos y reproducibles (R Core Team, 2023).\nPara ilustrar las técnicas de manipulación, se emplea un conjunto de datos simulado que representa un experimento agrícola. Este conjunto contiene variables numéricas y categóricas, así como algunos valores faltantes, lo que permite demostrar operaciones comunes en la estadística clásica. El uso de datos simulados garantiza la reproducibilidad de los ejemplos y facilita la comprensión de los procedimientos (Wickham & Grolemund, 2017).\nEl objetivo de este ejemplo es mostrar cómo crear y explorar un conjunto de datos en R, identificando la estructura de las variables y la presencia de valores faltantes, aspectos fundamentales en la preparación de datos para el análisis estadístico.\nCreación del conjunto de datos simulado\n# Establecer una semilla para que el usuario pueda replicar el ejemplo\nset.seed(123) # Garantiza reproducibilidad\n\n# Simular los resultados de un experimento con el diseño bloques completos al azar\ndatos_cultivo &lt;- data.frame(\n  parcela = 1:20,\n  tratamiento = rep(c(\"Control\", \n                      \"Fertilizante A\", \n                      \"Fertilizante B\", \n                      \"Fertilizante C\"), each = 5),\n  bloque = rep(1:5, times = 4),\n  altura_cm = round(rnorm(20, mean = 65, sd = 10), 1),\n  peso_gr = round(rnorm(20, mean = 120, sd = 25), 1),\n  daño_plaga = sample(c(\"Alto\", \"Medio\", \"Bajo\"), 20, replace = TRUE),\n  fecha_siembra = as.Date(\"2024-01-01\") + sample(1:10, 20, replace = TRUE)\n)\nEn este bloque de código se crea un data frame denominado datos_cultivo que simula los resultados de un experimento agrícola bajo un diseño de bloques completos al azar. Las variables incluyen identificadores de parcela, tipo de tratamiento aplicado, bloque experimental, altura y peso de las plantas, nivel de daño por plaga y fecha de siembra. La función set.seed(123) asegura que los resultados sean reproducibles, permitiendo que cualquier usuario obtenga el mismo conjunto de datos al ejecutar el código.\nSimulación de valores faltantes\n# Simular la presencia de datos faltantes en los resultados del experimento\ndatos_cultivo$altura_cm[c(3, 15)] &lt;- NA\ndatos_cultivo$peso_gr[c(7, 18)] &lt;- NA\nSe introducen valores faltantes (NA) en las variables altura_cm y peso_gr para reflejar situaciones reales en las que los datos pueden estar incompletos. Este paso es fundamental para demostrar técnicas de manejo de datos faltantes en secciones posteriores.\nVisualización preliminar de los datos\n# Visualizar las primeras filas del data frame con los datos simulados\nhead(datos_cultivo)\n\n  parcela    tratamiento bloque altura_cm peso_gr daño_plaga fecha_siembra\n1       1        Control      1      59.4    93.3      Medio    2024-01-10\n2       2        Control      2      62.7   114.6      Medio    2024-01-08\n3       3        Control      3        NA    94.3       Bajo    2024-01-04\n4       4        Control      4      65.7   101.8      Medio    2024-01-09\n5       5        Control      5      66.3   104.4      Medio    2024-01-10\n6       6 Fertilizante A      1      82.2    77.8       Bajo    2024-01-04\nLa función head() permite observar las primeras seis filas del data frame, facilitando la verificación de la correcta creación de las variables y la identificación de valores faltantes.\nEs importante revisar la estructura y el contenido de los datos antes de proceder con cualquier análisis, ya que la presencia de valores faltantes o inconsistencias puede afectar la validez de los resultados estadísticos. Para ello también se puede usar la función view (), que nos permite visualizar un objeto en su totalidad en formato tabular (R Core Team, 2023).",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Manipulación de Datos con Herramientas Base de R</span>"
    ]
  },
  {
    "objectID": "08.2_manipulacion.html#selección-y-filtrado-de-datos-en-data-frames",
    "href": "08.2_manipulacion.html#selección-y-filtrado-de-datos-en-data-frames",
    "title": "10  Manipulación de Datos con Herramientas Base de R",
    "section": "10.2 Selección y filtrado de datos en data frames",
    "text": "10.2 Selección y filtrado de datos en data frames\nLa selección y el filtrado de datos constituyen operaciones fundamentales en la manipulación de data frames en R, ya que permiten enfocar el análisis en subconjuntos de información relevantes para los objetivos estadísticos planteados. Estas tareas se realizan mediante la indexación y el uso de condiciones lógicas, lo que facilita la extracción precisa de variables y observaciones de interés (Wickham & Grolemund, 2017; R Core Team, 2023).\n\n10.2.1 Selección de columnas\nEn R, la selección de columnas dentro de un data frame se efectúa utilizando la notación de corchetes [, ], donde el primer argumento corresponde a las filas y el segundo a las columnas. Por ejemplo, para extraer únicamente las variables de altura y peso del conjunto de datos simulado, se emplea el siguiente código:\n\n# Seleccionar solo las columnas de altura y peso del data frame\ndatos_mediciones &lt;- datos_cultivo[, c(\"altura_cm\", \"peso_gr\")]\n\n# Visualizar las primeras filas para verificar la selección\nhead(datos_mediciones)\n\n  altura_cm peso_gr\n1      59.4    93.3\n2      62.7   114.6\n3        NA    94.3\n4      65.7   101.8\n5      66.3   104.4\n6      82.2    77.8\n\n\nEn este caso, se especifican los nombres de las columnas deseadas dentro de un vector, lo que permite obtener un nuevo data frame que contiene exclusivamente las mediciones de altura y peso. Esta práctica es recomendable, ya que el uso de nombres de columnas, en lugar de posiciones numéricas, previene errores en caso de que la estructura del data frame cambie posteriormente.\nLa exclusión de columnas también puede realizarse de manera eficiente. Por ejemplo, si se requiere eliminar la variable de fecha de siembra, se puede utilizar la función names() junto con el operador %in% para identificar y excluir la columna correspondiente:\n\n# Excluir la columna fecha_siembra del data frame\ndatos_sin_fecha &lt;- datos_cultivo[, !names(datos_cultivo) %in% \"fecha_siembra\"]\n\n# Verificar que la columna fecha_siembra ha sido eliminada\nhead(datos_sin_fecha)\n\n  parcela    tratamiento bloque altura_cm peso_gr daño_plaga\n1       1        Control      1      59.4    93.3      Medio\n2       2        Control      2      62.7   114.6      Medio\n3       3        Control      3        NA    94.3       Bajo\n4       4        Control      4      65.7   101.8      Medio\n5       5        Control      5      66.3   104.4      Medio\n6       6 Fertilizante A      1      82.2    77.8       Bajo\n\n\nEste procedimiento genera un nuevo data frame en el que la columna “fecha_siembra” ha sido removida, manteniendo el resto de las variables intactas. La visualización de las primeras filas permite comprobar que la exclusión se ha realizado correctamente.\nAsimismo, la selección de columnas puede basarse en la posición que ocupan dentro del data frame:\n\n# Seleccionar las tres primeras columnas usando índices numéricos\nprimeras_tres_columnas &lt;- datos_cultivo[, 1:3]\n\n# Mostrar el resultado para verificar la selección\nhead(primeras_tres_columnas)\n\n  parcela    tratamiento bloque\n1       1        Control      1\n2       2        Control      2\n3       3        Control      3\n4       4        Control      4\n5       5        Control      5\n6       6 Fertilizante A      1\n\n\nEste método resulta útil en situaciones donde se conoce la estructura del data frame y se requiere trabajar con un subconjunto de variables contiguas. Sin embargo, es importante considerar que la selección por posición puede ser menos robusta ante modificaciones en el orden de las columnas, por lo que se recomienda preferir la selección por nombre cuando sea posible.\n\n\n10.2.2 Filtrado de filas por condiciones lógicas\nEl filtrado de filas en un data frame se realiza especificando una condición lógica en el argumento correspondiente a las filas dentro de la notación de corchetes. Por ejemplo, para seleccionar únicamente las observaciones que corresponden al tratamiento “Control”, se utiliza la siguiente instrucción:\n\n# Filtrar las filas donde el tratamiento es \"Control\"\ndatos_control &lt;- datos_cultivo[datos_cultivo$tratamiento == \"Control\", ]\n\n# Verificar el filtrado mostrando las primeras filas\nhead(datos_control)\n\n  parcela tratamiento bloque altura_cm peso_gr daño_plaga fecha_siembra\n1       1     Control      1      59.4    93.3      Medio    2024-01-10\n2       2     Control      2      62.7   114.6      Medio    2024-01-08\n3       3     Control      3        NA    94.3       Bajo    2024-01-04\n4       4     Control      4      65.7   101.8      Medio    2024-01-09\n5       5     Control      5      66.3   104.4      Medio    2024-01-10\n\n\nEn este caso, la condición datos_cultivo$tratamiento == \"Control\" evalúa cada fila del data frame y selecciona aquellas en las que la variable “tratamiento” coincide con el valor especificado. El resultado es un nuevo data frame que contiene únicamente las observaciones del grupo de control, lo que facilita la comparación o el análisis específico de este subconjunto.\nEl filtrado puede combinar múltiples condiciones lógicas para refinar aún más la selección de datos. Por ejemplo, si se desea obtener las observaciones en las que la altura es mayor a 65 centímetros y el tratamiento es distinto de “Control”, se puede emplear el siguiente código:\n\n# Filtrar filas que cumplen dos condiciones simultáneamente\ndatos_altos &lt;- datos_cultivo[datos_cultivo$altura_cm &gt; 65 &\n                             datos_cultivo$tratamiento != \"Control\", ]\n\n# Verificar el resultado del filtrado\nhead(datos_altos)\n\n   parcela    tratamiento bloque altura_cm peso_gr daño_plaga fecha_siembra\n6        6 Fertilizante A      1      82.2    77.8       Bajo    2024-01-04\n7        7 Fertilizante A      2      69.6      NA       Bajo    2024-01-08\n11      11 Fertilizante B      1      77.2   130.7       Alto    2024-01-11\n12      12 Fertilizante B      2      68.6   112.6      Medio    2024-01-06\n13      13 Fertilizante B      3      69.0   142.4       Alto    2024-01-06\n14      14 Fertilizante B      4      66.1   142.0       Alto    2024-01-09\n\n\nAquí, la condición compuesta utiliza el operador lógico & para exigir que ambas condiciones se cumplan simultáneamente. De este modo, se obtiene un subconjunto de datos que cumple criterios específicos de interés para el análisis.\nPara mejorar la legibilidad y evitar la repetición del nombre del data frame en cada condición, R ofrece la función subset(), que permite expresar las condiciones de filtrado de manera más clara. Por ejemplo, para seleccionar las observaciones correspondientes al tratamiento “Fertilizante A” y a los bloques 2, 3 o 4, se puede utilizar la siguiente instrucción:\n\n# Usar subset() para filtrar datos de manera más legible\ndatos_fertilizante_A &lt;- subset(datos_cultivo, \n                              tratamiento == \"Fertilizante A\" &\n                              bloque %in% c(2,3,4))\n\n# Mostrar el resultado del filtrado\nhead(datos_fertilizante_A)\n\n  parcela    tratamiento bloque altura_cm peso_gr daño_plaga fecha_siembra\n7       7 Fertilizante A      2      69.6      NA       Bajo    2024-01-08\n8       8 Fertilizante A      3      52.3   123.8       Alto    2024-01-04\n9       9 Fertilizante A      4      58.1    91.5      Medio    2024-01-08\n\n\nLa función subset() interpreta las condiciones dentro del contexto del data frame especificado, lo que simplifica la sintaxis y reduce la posibilidad de errores. Esta función resulta especialmente útil cuando se trabaja con condiciones complejas o con múltiples variables.\nLa correcta aplicación de estas técnicas de selección y filtrado permite preparar conjuntos de datos ajustados a los requerimientos de los análisis estadísticos, optimizando la eficiencia y la claridad en la manipulación de la información (R Core Team, 2023; Wickham & Grolemund, 2017).",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Manipulación de Datos con Herramientas Base de R</span>"
    ]
  },
  {
    "objectID": "08.2_manipulacion.html#modificación-de-variables",
    "href": "08.2_manipulacion.html#modificación-de-variables",
    "title": "10  Manipulación de Datos con Herramientas Base de R",
    "section": "10.3 Modificación de variables",
    "text": "10.3 Modificación de variables\nLa modificación de variables constituye una etapa crucial en la preparación de datos para análisis estadísticos, ya que permite adaptar la información a los requerimientos específicos de diferentes métodos analíticos. Esta transformación de datos puede incluir la creación de nuevas variables derivadas, la recodificación de variables categóricas y la aplicación de transformaciones matemáticas para cumplir con los supuestos de las técnicas estadísticas (Wickham & Grolemund, 2017; R Core Team, 2023).\n\n10.3.1 Creación de nuevas columnas\nEn R, la creación de nuevas variables se realiza mediante operaciones aritméticas o lógicas sobre las columnas existentes del data frame. Estas transformaciones permiten generar índices, categorizar datos continuos o aplicar transformaciones matemáticas para mejorar la normalidad de las distribuciones:\n\n# Crear un índice que relaciona altura y peso\n# La división de columnas genera una nueva variable que representa la relación\ndatos_cultivo$indice_crecimiento &lt;- datos_cultivo$altura_cm/datos_cultivo$peso_gr\n\n# Categorizar el peso en dos niveles usando una condición lógica\n# ifelse() evalúa cada valor y asigna una categoría según la condición\ndatos_cultivo$categoria_peso &lt;- ifelse(datos_cultivo$peso_gr &gt; 120, \n                                      \"Alto\", \"Bajo\")\n\n# Aplicar transformación logarítmica para normalizar la distribución\n# log() es útil para variables con asimetría positiva\ndatos_cultivo$log_peso &lt;- log(datos_cultivo$peso_gr)\n\n# Verificar las nuevas variables creadas\nhead(datos_cultivo)\n\n  parcela    tratamiento bloque altura_cm peso_gr daño_plaga fecha_siembra\n1       1        Control      1      59.4    93.3      Medio    2024-01-10\n2       2        Control      2      62.7   114.6      Medio    2024-01-08\n3       3        Control      3        NA    94.3       Bajo    2024-01-04\n4       4        Control      4      65.7   101.8      Medio    2024-01-09\n5       5        Control      5      66.3   104.4      Medio    2024-01-10\n6       6 Fertilizante A      1      82.2    77.8       Bajo    2024-01-04\n  indice_crecimiento categoria_peso log_peso\n1          0.6366559           Bajo 4.535820\n2          0.5471204           Bajo 4.741448\n3                 NA           Bajo 4.546481\n4          0.6453831           Bajo 4.623010\n5          0.6350575           Bajo 4.648230\n6          1.0565553           Bajo 4.354141\n\n\nLa creación de estas nuevas variables permite enriquecer el análisis y adaptar los datos a diferentes necesidades analíticas. Por ejemplo, la transformación logarítmica es especialmente útil cuando se requiere normalizar distribuciones asimétricas para análisis paramétricos, mientras que la categorización facilita la comparación entre grupos.\n\n\n10.3.2 Recodificación de variables categóricas\nLa recodificación de variables categóricas es fundamental para establecer un orden específico en los niveles de los factores o para simplificar categorías, lo cual es particularmente relevante en análisis como ANOVA o regresión logística:\n\n# Recodificar los niveles de daño por plaga como valores numéricos ordenados\n# factor() permite especificar el orden de los niveles y asignar nuevas etiquetas\ndatos_cultivo$nivel_daño &lt;- factor(datos_cultivo$daño_plaga, \n                                  levels = c(\"Bajo\", \"Medio\", \"Alto\"), \n                                  labels = c(\"1\", \"2\", \"3\"))\n\n# Crear una variable indicadora (dummy) para el tratamiento control\n# Esta transformación es útil para análisis de regresión\ndatos_cultivo$es_control &lt;- ifelse(datos_cultivo$tratamiento == \"Control\", 1, 0)\n\n# Verificar la recodificación\nhead(datos_cultivo)\n\n  parcela    tratamiento bloque altura_cm peso_gr daño_plaga fecha_siembra\n1       1        Control      1      59.4    93.3      Medio    2024-01-10\n2       2        Control      2      62.7   114.6      Medio    2024-01-08\n3       3        Control      3        NA    94.3       Bajo    2024-01-04\n4       4        Control      4      65.7   101.8      Medio    2024-01-09\n5       5        Control      5      66.3   104.4      Medio    2024-01-10\n6       6 Fertilizante A      1      82.2    77.8       Bajo    2024-01-04\n  indice_crecimiento categoria_peso log_peso nivel_daño es_control\n1          0.6366559           Bajo 4.535820          2          1\n2          0.5471204           Bajo 4.741448          2          1\n3                 NA           Bajo 4.546481          1          1\n4          0.6453831           Bajo 4.623010          2          1\n5          0.6350575           Bajo 4.648230          2          1\n6          1.0565553           Bajo 4.354141          1          0\n\n\nLa función factor() es especialmente útil en este contexto, ya que permite no solo recodificar los valores, sino también establecer un orden específico en los niveles de la variable categórica. Esto es crucial en análisis donde el orden de los niveles afecta la interpretación de los resultados, como en el caso de variables ordinales.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Manipulación de Datos con Herramientas Base de R</span>"
    ]
  },
  {
    "objectID": "08.2_manipulacion.html#ordenamiento-y-agrupamiento-de-datos",
    "href": "08.2_manipulacion.html#ordenamiento-y-agrupamiento-de-datos",
    "title": "10  Manipulación de Datos con Herramientas Base de R",
    "section": "10.4 Ordenamiento y agrupamiento de datos",
    "text": "10.4 Ordenamiento y agrupamiento de datos\nEl ordenamiento y agrupamiento de datos constituyen operaciones fundamentales en el análisis exploratorio de datos, facilitando la identificación de patrones, la detección de valores atípicos y el cálculo de estadísticas descriptivas por grupos. Estas operaciones son especialmente relevantes en la estadística clásica, donde la estructura y organización de los datos influyen directamente en la calidad de los análisis posteriores (Wickham & Grolemund, 2017; R Core Team, 2023).\n\n10.4.1 Ordenamiento de datos\nR proporciona herramientas eficientes para ordenar datos mediante la función order(), que genera índices de ordenamiento que pueden aplicarse a las filas del data frame. Este ordenamiento puede realizarse por una o múltiples variables, en orden ascendente o descendente:\n\n# Ordenar el data frame por altura de manera ascendente\n# order() genera índices basados en los valores de altura_cm\n# Los índices se utilizan para reordenar todas las filas del data frame\ndatos_ordenados &lt;- datos_cultivo[order(datos_cultivo$altura_cm), ]\n\n# Verificar el ordenamiento mostrando las primeras filas\nhead(datos_ordenados)\n\n   parcela    tratamiento bloque altura_cm peso_gr daño_plaga fecha_siembra\n18      18 Fertilizante C      3      45.3      NA       Alto    2024-01-11\n8        8 Fertilizante A      3      52.3   123.8       Alto    2024-01-04\n9        9 Fertilizante A      4      58.1    91.5      Medio    2024-01-08\n1        1        Control      1      59.4    93.3      Medio    2024-01-10\n20      20 Fertilizante C      5      60.3   110.5       Alto    2024-01-11\n10      10 Fertilizante A      5      60.5   151.3      Medio    2024-01-07\n   indice_crecimiento categoria_peso log_peso nivel_daño es_control\n18                 NA           &lt;NA&gt;       NA          3          0\n8           0.4224556           Alto 4.818667          3          0\n9           0.6349727           Bajo 4.516339          2          0\n1           0.6366559           Bajo 4.535820          2          1\n20          0.5457014           Bajo 4.705016          3          0\n10          0.3998678           Alto 5.019265          2          0\n\n# Ordenar por múltiples criterios: por tratamiento y por peso descendente\n# El signo negativo en peso_gr invierte el orden de esta variable\ndatos_ordenados_multi &lt;- datos_cultivo[order(datos_cultivo$tratamiento,\n                                             -datos_cultivo$peso_gr), ]\n\n# Verificar el ordenamiento múltiple\nhead(datos_ordenados_multi)\n\n   parcela    tratamiento bloque altura_cm peso_gr daño_plaga fecha_siembra\n2        2        Control      2      62.7   114.6      Medio    2024-01-08\n5        5        Control      5      66.3   104.4      Medio    2024-01-10\n4        4        Control      4      65.7   101.8      Medio    2024-01-09\n3        3        Control      3        NA    94.3       Bajo    2024-01-04\n1        1        Control      1      59.4    93.3      Medio    2024-01-10\n10      10 Fertilizante A      5      60.5   151.3      Medio    2024-01-07\n   indice_crecimiento categoria_peso log_peso nivel_daño es_control\n2           0.5471204           Bajo 4.741448          2          1\n5           0.6350575           Bajo 4.648230          2          1\n4           0.6453831           Bajo 4.623010          2          1\n3                  NA           Bajo 4.546481          1          1\n1           0.6366559           Bajo 4.535820          2          1\n10          0.3998678           Alto 5.019265          2          0\n\n\nEl ordenamiento por múltiples variables es particularmente útil cuando se necesita establecer una jerarquía en la organización de los datos, como por ejemplo, ordenar primero por tratamiento y luego por una variable de respuesta.\n\n\n10.4.2 Cálculo de estadísticas por grupo\nEl análisis por grupos es esencial en la estadística experimental, permitiendo comparar tratamientos y evaluar la variabilidad dentro y entre grupos. R ofrece varias funciones para realizar estos cálculos:\n\n# Calcular medias por tratamiento usando tapply\n# tapply aplica la función mean a cada subconjunto definido por tratamiento\nmedias_altura &lt;- tapply(datos_cultivo$altura_cm, \n                       datos_cultivo$tratamiento, \n                       mean, \n                       na.rm = TRUE)\n\nLa salida muestra las medias de altura para cada tratamiento:\n\n# Mostrar las medias por tratamiento\nprint(medias_altura)\n\n       Control Fertilizante A Fertilizante B Fertilizante C \n        63.525         64.540         70.225         66.100 \n\n\nPara un análisis más completo, se pueden calcular múltiples estadísticas simultáneamente utilizando la función aggregate():\n\n# Calcular media y desviación estándar por tratamiento\n# aggregate permite trabajar con múltiples variables y funciones\nestadisticas_grupo &lt;- aggregate(cbind(altura_cm, peso_gr) ~ tratamiento, \n                               data = datos_cultivo, \n                               FUN = function(x) \n                                 c(media = mean(x, na.rm = TRUE), \n                                   sd = sd(x, na.rm = TRUE)))\n\nLos resultados muestran estadísticas descriptivas por tratamiento:\n\n# Mostrar los resultados del análisis por grupo\nprint(estadisticas_grupo)\n\n     tratamiento altura_cm.media altura_cm.sd peso_gr.media peso_gr.sd\n1        Control       63.525000     3.168990    103.525000   8.773967\n2 Fertilizante A       63.275000    13.077812    111.100000  33.017066\n3 Fertilizante B       70.225000     4.823812    131.925000  13.978406\n4 Fertilizante C       71.300000     9.268945    123.475000  13.976021\n\n\nEsta salida proporciona una visión completa de la variabilidad en las mediciones de altura y peso para cada tratamiento, revelando que:\n\nEl Fertilizante B y C muestran las mayores medias en altura\nEl Fertilizante A presenta la mayor variabilidad en altura (SD = 13.08)\nLos tratamientos con fertilizantes muestran mayores pesos medios que el control\n\nEstos resultados preliminares son fundamentales para guiar análisis estadísticos más detallados, como ANOVA o comparaciones múltiples, y para identificar posibles patrones o anomalías en los datos experimentales (R Core Team, 2023).\nLa combinación de técnicas de ordenamiento y agrupamiento proporciona una base sólida para la exploración inicial de datos experimentales, permitiendo identificar patrones, detectar valores atípicos y generar hipótesis para análisis posteriores. Esta preparación cuidadosa de los datos es esencial para garantizar la validez y robustez de las conclusiones estadísticas.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Manipulación de Datos con Herramientas Base de R</span>"
    ]
  },
  {
    "objectID": "08.2_manipulacion.html#manejo-de-valores-faltantes-y-duplicados",
    "href": "08.2_manipulacion.html#manejo-de-valores-faltantes-y-duplicados",
    "title": "10  Manipulación de Datos con Herramientas Base de R",
    "section": "10.5 Manejo de valores faltantes y duplicados",
    "text": "10.5 Manejo de valores faltantes y duplicados\nEl tratamiento adecuado de valores faltantes y registros duplicados constituye un paso fundamental en la preparación de datos para análisis estadísticos. Esta etapa es crucial para garantizar la validez de los resultados y evitar sesgos en las estimaciones estadísticas. Los valores faltantes pueden afectar significativamente los análisis si no se manejan apropiadamente, mientras que los duplicados pueden inflar artificialmente el tamaño de la muestra y distorsionar las conclusiones (Wickham & Grolemund, 2017; R Core Team, 2023).\n\n10.5.1 Identificación y manejo de valores faltantes\nEn R, existen diversas herramientas para identificar y tratar los valores faltantes (NA). La estrategia de manejo debe seleccionarse cuidadosamente según el contexto del estudio y el impacto potencial en los análisis posteriores:\n\n# Realizar un diagnóstico inicial de valores faltantes por columna\n# is.na() identifica valores NA en cada celda\n# colSums() suma los valores TRUE (NA) en cada columna\nna_por_columna &lt;- colSums(is.na(datos_cultivo))\n\n# Mostrar el resultado del diagnóstico\nprint(na_por_columna)\n\n           parcela        tratamiento             bloque          altura_cm \n                 0                  0                  0                  2 \n           peso_gr         daño_plaga      fecha_siembra indice_crecimiento \n                 2                  0                  0                  4 \n    categoria_peso           log_peso         nivel_daño         es_control \n                 2                  2                  0                  0 \n\n\nEste diagnóstico inicial permite identificar las variables más afectadas por valores faltantes y determinar la estrategia más apropiada para su tratamiento. Una vez identificados los valores faltantes, se pueden aplicar diferentes métodos de manejo:\n\n# Eliminar filas que contengan cualquier valor faltante\n# na.omit() crea un nuevo data frame excluyendo filas con NA\ndatos_completos &lt;- na.omit(datos_cultivo)\n\n# Imputar valores faltantes utilizando la media de la variable\n# Este método es común pero debe usarse con precaución\ndatos_cultivo$altura_cm[is.na(datos_cultivo$altura_cm)] &lt;- \n  mean(datos_cultivo$altura_cm, na.rm = TRUE)\n\n# Verificar el resultado de la imputación\nsummary(datos_cultivo$altura_cm)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  45.30   60.45   66.06   66.01   69.70   82.90 \n\n\nLa eliminación de filas con valores faltantes (caso completo) es una solución simple pero puede resultar en pérdida significativa de información. La imputación con la media es una alternativa común, aunque puede subestimar la variabilidad real de los datos. En estudios más rigurosos, se pueden considerar métodos de imputación más sofisticados basados en modelos estadísticos.\n\n\n10.5.2 Identificación y manejo de duplicados\nLos registros duplicados pueden surgir por diversos motivos, desde errores en la entrada de datos hasta repeticiones intencionales en el diseño experimental. Su identificación y manejo apropiado es esencial para mantener la integridad del análisis:\n\n# Identificar duplicados basados en variables específicas\n# duplicated() evalúa si cada fila es una duplicación de una fila anterior\nduplicados &lt;- duplicated(datos_cultivo[, c(\"tratamiento\", \"bloque\")])\n\n# Analizar la presencia de duplicados\nsummary(duplicados)\n\n   Mode   FALSE \nlogical      20 \n\n# Mostrar las filas duplicadas para su inspección\ndatos_cultivo[duplicados, ]\n\n [1] parcela            tratamiento        bloque             altura_cm         \n [5] peso_gr            daño_plaga         fecha_siembra      indice_crecimiento\n [9] categoria_peso     log_peso           nivel_daño         es_control        \n&lt;0 rows&gt; (o 0- extensión row.names)\n\n# Eliminar duplicados manteniendo la primera ocurrencia\n# unique() conserva solo las filas únicas del data frame\ndatos_sin_duplicados &lt;- unique(datos_cultivo)\n\n# Verificar la reducción en el número de filas\nnrow(datos_cultivo) - nrow(datos_sin_duplicados)\n\n[1] 0\n\n\nLa identificación de duplicados puede realizarse considerando todas las variables o solo un subconjunto relevante para el análisis. Es importante distinguir entre duplicados verdaderos (errores) y repeticiones válidas en el diseño experimental.\n\n\n10.5.3 Verificación de la integridad de los datos\nDespués de manejar valores faltantes y duplicados, es importante verificar la integridad general de los datos:\n\n# Realizar un resumen estadístico completo\nsummary(datos_sin_duplicados)\n\n    parcela      tratamiento            bloque    altura_cm        peso_gr     \n Min.   : 1.00   Length:20          Min.   :1   Min.   :45.30   Min.   : 77.8  \n 1st Qu.: 5.75   Class :character   1st Qu.:2   1st Qu.:60.45   1st Qu.:102.5  \n Median :10.50   Mode  :character   Median :3   Median :66.06   Median :113.6  \n Mean   :10.50                      Mean   :3   Mean   :66.01   Mean   :117.5  \n 3rd Qu.:15.25                      3rd Qu.:4   3rd Qu.:69.70   3rd Qu.:136.3  \n Max.   :20.00                      Max.   :5   Max.   :82.90   Max.   :151.3  \n                                                                NA's   :2      \n  daño_plaga        fecha_siembra        indice_crecimiento categoria_peso    \n Length:20          Min.   :2024-01-03   Min.   :0.3999     Length:20         \n Class :character   1st Qu.:2024-01-04   1st Qu.:0.5135     Class :character  \n Mode  :character   Median :2024-01-08   Median :0.5974     Mode  :character  \n                    Mean   :2024-01-07   Mean   :0.5901                       \n                    3rd Qu.:2024-01-10   3rd Qu.:0.6355                       \n                    Max.   :2024-01-11   Max.   :1.0566                       \n                                         NA's   :4                            \n    log_peso     nivel_daño   es_control  \n Min.   :4.354   1:5        Min.   :0.00  \n 1st Qu.:4.629   2:9        1st Qu.:0.00  \n Median :4.733   3:6        Median :0.00  \n Mean   :4.750              Mean   :0.25  \n 3rd Qu.:4.915              3rd Qu.:0.25  \n Max.   :5.019              Max.   :1.00  \n NA's   :2                                \n\n# Verificar la estructura del data frame resultante\nstr(datos_sin_duplicados)\n\n'data.frame':   20 obs. of  12 variables:\n $ parcela           : int  1 2 3 4 5 6 7 8 9 10 ...\n $ tratamiento       : chr  \"Control\" \"Control\" \"Control\" \"Control\" ...\n $ bloque            : int  1 2 3 4 5 1 2 3 4 5 ...\n $ altura_cm         : num  59.4 62.7 66 65.7 66.3 ...\n $ peso_gr           : num  93.3 114.6 94.3 101.8 104.4 ...\n $ daño_plaga        : chr  \"Medio\" \"Medio\" \"Bajo\" \"Medio\" ...\n $ fecha_siembra     : Date, format: \"2024-01-10\" \"2024-01-08\" ...\n $ indice_crecimiento: num  0.637 0.547 NA 0.645 0.635 ...\n $ categoria_peso    : chr  \"Bajo\" \"Bajo\" \"Bajo\" \"Bajo\" ...\n $ log_peso          : num  4.54 4.74 4.55 4.62 4.65 ...\n $ nivel_daño        : Factor w/ 3 levels \"1\",\"2\",\"3\": 2 2 1 2 2 1 1 3 2 2 ...\n $ es_control        : num  1 1 1 1 1 0 0 0 0 0 ...\n\n# Calcular el porcentaje de datos completos por variable\nporcentaje_completos &lt;- colMeans(!is.na(datos_sin_duplicados)) * 100\nprint(porcentaje_completos)\n\n           parcela        tratamiento             bloque          altura_cm \n               100                100                100                100 \n           peso_gr         daño_plaga      fecha_siembra indice_crecimiento \n                90                100                100                 80 \n    categoria_peso           log_peso         nivel_daño         es_control \n                90                 90                100                100 \n\n\nEsta verificación final permite asegurar que el tratamiento de valores faltantes y duplicados no ha introducido sesgos o inconsistencias en los datos. Es fundamental documentar todas las decisiones tomadas en este proceso, ya que pueden afectar la interpretación de los resultados posteriores.\nLa gestión adecuada de valores faltantes y duplicados es un paso crítico en la preparación de datos para análisis estadísticos. Las decisiones tomadas en esta etapa deben basarse en un entendimiento profundo del contexto del estudio y documentarse apropiadamente para garantizar la reproducibilidad del análisis (R Core Team, 2023).",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Manipulación de Datos con Herramientas Base de R</span>"
    ]
  },
  {
    "objectID": "08.3_manipulacion.html",
    "href": "08.3_manipulacion.html",
    "title": "11  Manipulación de datos con dplyr y tidyr",
    "section": "",
    "text": "11.1 Introducción a los paquetes dplyr y tidyr\nLos paquetes dplyr y tidyr constituyen componentes fundamentales del ecosistema tidyverse, diseñados específicamente para la manipulación y transformación eficiente de datos en R. Estos paquetes implementan una filosofía de programación que prioriza la claridad y consistencia en el código, facilitando el desarrollo de análisis estadísticos reproducibles (Wickham & Grolemund, 2017).\nEl paquete dplyr se especializa en la manipulación de datos tabulares, proporcionando un conjunto coherente de verbos (funciones) que corresponden a las operaciones más comunes en el análisis de datos. Estas operaciones incluyen el filtrado de observaciones, la selección de variables, la creación de nuevas variables y la agregación de datos. Por su parte, tidyr se centra en la reorganización estructural de los datos, permitiendo transformaciones entre diferentes formatos según los requerimientos específicos del análisis estadístico (Wickham et al., 2023).",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Manipulación de datos con dplyr y tidyr</span>"
    ]
  },
  {
    "objectID": "08.3_manipulacion.html#datos-ejemplo",
    "href": "08.3_manipulacion.html#datos-ejemplo",
    "title": "11  Manipulación de datos con dplyr y tidyr",
    "section": "11.2 Datos ejemplo",
    "text": "11.2 Datos ejemplo\nSe emplea el mismo conjunto de datos simulado del experimento agrícola utilizado en el capítulo anterior, lo que permite comparar directamente los enfoques de R base y tidyverse.\n\n# Cargar el paquete necesario\nlibrary(dplyr)\nlibrary(tidyr)\n\n# Crear datos de ejemplo\nset.seed(123) # Para reproducibilidad\n\ndatos_cultivo &lt;- data.frame(\n  parcela = 1:20,\n  tratamiento = rep(c(\"Control\", \"Fertilizante A\",\n                      \"Fertilizante B\", \"Fertilizante C\"), each = 5),\n  bloque = rep(1:5, times = 4),\n  altura_cm = round(rnorm(20, mean = 65, sd = 10), 1),\n  peso_gr = round(rnorm(20, mean = 120, sd = 25), 1),\n  daño_plaga = sample(c(\"Alto\", \"Medio\", \"Bajo\"), 20, replace = TRUE),\n  fecha_siembra = as.Date(\"2024-01-01\") + sample(1:10, 20, replace = TRUE)\n)\n\n# Agregar algunos valores NA para ejemplos posteriores\ndatos_cultivo$altura_cm[c(3, 15)] &lt;- NA\ndatos_cultivo$peso_gr[c(7, 18)] &lt;- NA",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Manipulación de datos con dplyr y tidyr</span>"
    ]
  },
  {
    "objectID": "08.3_manipulacion.html#operaciones-básicas-con-dplyr",
    "href": "08.3_manipulacion.html#operaciones-básicas-con-dplyr",
    "title": "11  Manipulación de datos con dplyr y tidyr",
    "section": "11.3 Operaciones básicas con dplyr",
    "text": "11.3 Operaciones básicas con dplyr\n\n11.3.1 Filtrado de datos con filter()\nLa función filter() permite seleccionar subconjuntos de filas en un data frame o tibble, basándose en una o más condiciones lógicas. Esta función es esencial para enfocar el análisis en observaciones específicas que cumplen con criterios predefinidos (Wickham & Grolemund, 2017).\n\n# Sintaxis general de la función filter()\nfilter(.data, ...)\n\nDonde:\n\n.data: Especifica el data frame o tibble sobre el cual se aplicará el filtrado.\n...: Representa una o más expresiones lógicas que deben evaluarse como TRUE para que una fila sea seleccionada.\n\nEjemplos:\n\n# Ejemplo 1: Filtrar parcelas con tratamiento \"Control\"\ndatos_control &lt;- filter(datos_cultivo, tratamiento == \"Control\")\nhead(datos_control) # Visualizar las primeras filas del resultado\n\n  parcela tratamiento bloque altura_cm peso_gr daño_plaga fecha_siembra\n1       1     Control      1      59.4    93.3      Medio    2024-01-10\n2       2     Control      2      62.7   114.6      Medio    2024-01-08\n3       3     Control      3        NA    94.3       Bajo    2024-01-04\n4       4     Control      4      65.7   101.8      Medio    2024-01-09\n5       5     Control      5      66.3   104.4      Medio    2024-01-10\n\n\nEn este ejemplo, se crea un nuevo data frame llamado datos_control que contiene únicamente las filas donde la columna tratamiento es igual a “Control”. La función head() se utiliza para mostrar las primeras filas del resultado, permitiendo una rápida verificación del filtrado.\n\n# Ejemplo 2: Filtrar parcelas con altura mayor a 65 cm y \n           # tratamiento distinto de \"Control\"\ndatos_altos &lt;- filter(datos_cultivo, altura_cm &gt; 65, tratamiento != \"Control\")\nhead(datos_altos) # Visualizar las primeras filas del resultado\n\n  parcela    tratamiento bloque altura_cm peso_gr daño_plaga fecha_siembra\n1       6 Fertilizante A      1      82.2    77.8       Bajo    2024-01-04\n2       7 Fertilizante A      2      69.6      NA       Bajo    2024-01-08\n3      11 Fertilizante B      1      77.2   130.7       Alto    2024-01-11\n4      12 Fertilizante B      2      68.6   112.6      Medio    2024-01-06\n5      13 Fertilizante B      3      69.0   142.4       Alto    2024-01-06\n6      14 Fertilizante B      4      66.1   142.0       Alto    2024-01-09\n\n\nEn este caso, se combinan dos condiciones lógicas: la altura debe ser mayor a 65 cm y el tratamiento no debe ser “Control”. El resultado es un data frame que contiene solo las parcelas que cumplen ambos criterios. Es importante notar que, a diferencia de R base, no es necesario repetir el nombre del data frame en cada condición, lo que simplifica la sintaxis y mejora la legibilidad del código (Wickham et al., 2023).\n\n\n11.3.2 Selección de columnas con select()\nLa función select() permite extraer un subconjunto de columnas de un data frame o tibble. Esta función es útil para simplificar el análisis, enfocándose únicamente en las variables relevantes para la investigación (Wickham & Grolemund, 2017).\n\n# Sintaxis general de la función select()\nselect(.data, ...)\n\nDonde:\n\n.data: Especifica el data frame o tibble de entrada.\n...: Representa los nombres de las columnas a seleccionar, o funciones auxiliares que permiten patrones de selección más complejos.\n\nEjemplos:\n\n# Ejemplo 1: Seleccionar las columnas altura_cm y peso_gr\ndatos_mediciones &lt;- select(datos_cultivo, altura_cm, peso_gr)\nhead(datos_mediciones) # Visualizar las primeras filas del resultado\n\n  altura_cm peso_gr\n1      59.4    93.3\n2      62.7   114.6\n3        NA    94.3\n4      65.7   101.8\n5      66.3   104.4\n6      82.2    77.8\n\n\nEn este ejemplo, se crea un nuevo data frame llamado datos_mediciones que contiene únicamente las columnas altura_cm y peso_gr del data frame original.\n\n# Ejemplo 2: Excluir la columna fecha_siembra\ndatos_sin_fecha &lt;- select(datos_cultivo, -fecha_siembra)\nhead(datos_sin_fecha) # Visualizar las primeras filas del resultado\n\n  parcela    tratamiento bloque altura_cm peso_gr daño_plaga\n1       1        Control      1      59.4    93.3      Medio\n2       2        Control      2      62.7   114.6      Medio\n3       3        Control      3        NA    94.3       Bajo\n4       4        Control      4      65.7   101.8      Medio\n5       5        Control      5      66.3   104.4      Medio\n6       6 Fertilizante A      1      82.2    77.8       Bajo\n\n\nEn este caso, se utiliza el operador - para excluir la columna fecha_siembra del resultado. El nuevo data frame datos_sin_fecha contiene todas las columnas del original, excepto la columna excluida.\n\n# Ejemplo 3: Seleccionar columnas que terminan en \"cm\" o \"gr\"\ndatos_numericos &lt;- select(datos_cultivo, ends_with(\"cm\"), ends_with(\"gr\"))\nhead(datos_numericos) # Visualizar las primeras filas del resultado\n\n  altura_cm peso_gr\n1      59.4    93.3\n2      62.7   114.6\n3        NA    94.3\n4      65.7   101.8\n5      66.3   104.4\n6      82.2    77.8\n\n\nEste ejemplo demuestra el uso de la función auxiliar ends_with() para seleccionar columnas cuyos nombres terminan con “cm” o “gr”. El resultado es un data frame que contiene únicamente las columnas que cumplen con este patrón. El uso de funciones auxiliares permite seleccionar columnas de manera flexible, lo que resulta útil en bases de datos extensas (Wickham et al., 2023).\n\n\n11.3.3 Creación y transformación de variables con mutate()\nLa función mutate() permite crear nuevas columnas o modificar las existentes en un data frame o tibble. Esta función es fundamental para la ingeniería de variables, que consiste en transformar los datos originales para generar nuevas variables que capturen información relevante para el análisis (Wickham & Grolemund, 2017).\n\n# Sintaxis general de la función mutate()\nmutate(.data, ...)\n\n\n.data: Especifica el data frame o tibble de entrada.\n...: Representa una o más expresiones que definen las nuevas columnas o transformaciones.\n\nEjemplos:\n\n# Ejemplo 1: Crear una nueva variable: índice de crecimiento\ndatos_cultivo &lt;- mutate(datos_cultivo,\n                       indice_crecimiento = \n                         altura_cm / peso_gr)\n\nEn este ejemplo, se crea una nueva columna llamada indice_crecimiento que se calcula como la razón entre la altura y el peso de cada planta. La nueva columna se añade al data frame original datos_cultivo.\n\n# Ejemplo 2: Crear varias variables nuevas\ndatos_cultivo &lt;- mutate(datos_cultivo,\n                       altura_m = altura_cm / 100,\n                       peso_kg = peso_gr / 1000,\n                       categoria_altura = ifelse(\n                         altura_cm &gt; 65, \"Alto\", \"Bajo\"))\n\nEn este caso, se crean tres nuevas columnas: altura_m (altura en metros), peso_kg (peso en kilogramos) y categoria_altura (categoría de altura basada en un umbral). La función ifelse(condición, valor_si_verdadero, valor_si_falso) permite crear variables categóricas a partir de condiciones lógicas, lo que es común en la estadística clásica para definir grupos o categorías (Wickham & Grolemund, 2017).\n\n\n11.3.4 Agrupamiento y resumen con group_by() y summarize()\nLas funciones group_by() y summarize() son herramientas poderosas para el análisis exploratorio de datos y la generación de estadísticas descriptivas por grupos. La función group_by() permite dividir un data frame en grupos basados en los valores de una o más variables, mientras que summarize() calcula estadísticas resumen para cada grupo (Wickham & Grolemund, 2017).\n\n# Sintaxis general de las funciones group_by() y summarize()\ngroup_by(.data, ...)\nsummarize(.data, ...)\n\nDonde:\n\n.data: Especifica el data frame o tibble de entrada.\n...: Representa las variables de agrupamiento (en group_by()) o las expresiones de resumen (en summarize()).\n\nEjemplos:\n\n# Ejemplo: Agrupar por tratamiento y calcular estadísticas descriptivas\nresumen_tratamiento &lt;- datos_cultivo %&gt;%\n    group_by(tratamiento) %&gt;%\n    summarize(\n        media_altura = mean(altura_cm, na.rm = TRUE),\n        sd_altura = sd(altura_cm, na.rm = TRUE),\n        n = n()\n    )\nresumen_tratamiento\n\n# A tibble: 4 × 4\n  tratamiento    media_altura sd_altura     n\n  &lt;chr&gt;                 &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt;\n1 Control                63.5      3.17     5\n2 Fertilizante A         64.5     11.7      5\n3 Fertilizante B         70.2      4.82     5\n4 Fertilizante C         66.1     14.1      5\n\n\nEn este ejemplo, se utiliza el operador pipe (%&gt;%) para encadenar las funciones group_by() y summarize(). Primero, se agrupan los datos por la variable tratamiento. Luego, para cada grupo, se calculan las siguientes estadísticas:\n\nmedia_altura = mean(altura_cm, na.rm = TRUE): Calcula la media de la altura, excluyendo los valores faltantes (NA). El argumento na.rm = TRUE es crucial para evitar que los valores faltantes afecten el cálculo de la media.\nsd_altura = sd(altura_cm, na.rm = TRUE): Calcula la desviación estándar de la altura, también excluyendo los valores faltantes.\nn = n(): Cuenta el número de observaciones en cada grupo. La función n() es una función especial de dplyr que cuenta el tamaño de cada grupo.\n\nEl resultado es un nuevo data frame llamado resumen_tratamiento que contiene las estadísticas descriptivas para cada tratamiento. Estas funciones son esenciales para obtener resúmenes estadísticos por grupo, como promedios por tratamiento en un experimento clásico (Wickham et al., 2023).\n\n\n11.3.5 Ordenamiento de datos con arrange()\nLa función arrange() permite ordenar las filas de un data frame o tibble según los valores de una o más variables. El ordenamiento es útil para identificar valores extremos, preparar tablas para reportes o facilitar la visualización de datos (Wickham & Grolemund, 2017).\n\n# Sintaxis general de la función arrange()\narrange(.data, ...)\n\nDonde:\n\n.data: Especifica el data frame o tibble de entrada.\n...: Representa las variables por las que se desea ordenar. Se puede utilizar la función desc() para ordenar en orden descendente.\n\nEjemplos\n\n# Ejemplo 1: Ordenar por altura de menor a mayor\ndatos_ordenados &lt;- arrange(datos_cultivo, altura_cm)\n\nEn este ejemplo, se crea un nuevo data frame llamado datos_ordenados que contiene las mismas filas que datos_cultivo, pero ordenadas de menor a mayor según los valores de la columna altura_cm.\n\n# Ejemplo 2: Ordenar por tratamiento y peso descendente\ndatos_ordenados_multi &lt;- arrange(datos_cultivo, \n                                 tratamiento, \n                                 desc(peso_gr))\n\nEn este caso, se ordenan los datos por dos variables: primero por tratamiento (en orden ascendente por defecto) y luego por peso_gr (en orden descendente, gracias a la función desc()). El resultado es un data frame donde las filas están ordenadas alfabéticamente por tratamiento, y dentro de cada tratamiento, las filas están ordenadas de mayor a menor según el peso.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Manipulación de datos con dplyr y tidyr</span>"
    ]
  },
  {
    "objectID": "08.3_manipulacion.html#introducción-a-los-pipes",
    "href": "08.3_manipulacion.html#introducción-a-los-pipes",
    "title": "11  Manipulación de datos con dplyr y tidyr",
    "section": "11.4 Introducción a los pipes (%>%)",
    "text": "11.4 Introducción a los pipes (%&gt;%)\nEl operador pipe (%&gt;%), introducido por el paquete magrittr y adoptado como parte fundamental del tidyverse, representa una innovación significativa en la sintaxis de R. Este operador permite construir secuencias de operaciones de manera clara y lógica, siguiendo un flujo natural de procesamiento de datos. El pipe toma el resultado de una expresión a su izquierda y lo pasa como primer argumento a la función a su derecha (Wickham & Grolemund, 2017).\nLa sintaxis básica del operador pipe es:\n\n# Estructura usando pipes\ndatos %&gt;% funcion()\n\n# Equivalente a la siguiente estructura anidada\nfuncion(datos)\n\n\n11.4.1 Ventajas del uso de pipes\nEl uso de pipes ofrece múltiples ventajas en el análisis estadístico (Wickham et al., 2023):\n\nLegibilidad mejorada: Las operaciones se leen de izquierda a derecha y de arriba hacia abajo, siguiendo el orden natural de lectura. Esto facilita la comprensión del flujo de trabajo y reduce la probabilidad de errores.\nReducción de objetos intermedios: No es necesario crear variables temporales para almacenar resultados intermedios. Esto simplifica el código y reduce el riesgo de errores asociados con la gestión de múltiples objetos.\nFacilidad de depuración: Cada paso puede ser comentado o modificado independientemente. Esto facilita la identificación y corrección de errores en el código.\nClaridad en la secuencia de operaciones: El flujo de trabajo se hace explícito y fácil de seguir. Esto mejora la mantenibilidad del código y facilita la colaboración entre analistas.\n\n\n\n11.4.2 Ejemplo práctico\nPara ilustrar las ventajas del uso de pipes, consideremos el siguiente ejemplo, donde se calcula la media de la altura por tratamiento, excluyendo los valores faltantes:\nSin pipe (anidado): En la sintaxis tradicional, las funciones deben anidarse, lo que puede dificultar la lectura:\n\n# Calcular la media de altura por tratamiento, excluyendo valores NA\nresumen_tratamiento &lt;- summarize(\n  group_by(\n    filter(datos_cultivo, !is.na(altura_cm)),\n    tratamiento\n  ),\n  media_altura = mean(altura_cm)\n)\n\nEn este ejemplo, primero se filtran las filas sin valores faltantes en altura_cm, luego se agrupan por tratamiento y finalmente se calcula la media de altura para cada grupo. La anidación de funciones dificulta la lectura y comprensión del código.\nCon pipe (más legible): El mismo análisis, usando pipes, resulta más claro y fácil de seguir:\n\n# Calcular la media de altura por tratamiento, excluyendo valores NA\nresumen_tratamiento &lt;- datos_cultivo %&gt;%\n    # 1. Eliminar filas con NA en altura_cm\n    filter(!is.na(altura_cm)) %&gt;%    \n    # 2. Agrupar los datos por tratamiento\n    group_by(tratamiento) %&gt;%    \n    # 3. Calcular la media de altura por grupo\n    summarize(media_altura = mean(altura_cm))\n\nEn este ejemplo, el código se lee de arriba hacia abajo, siguiendo el orden lógico de las operaciones:\n\nEn la primera línea, se eliminan las filas donde la altura es NA.\nEn la segunda línea, se agrupan los datos por el tipo de tratamiento.\nEn la tercera línea, se calcula la media de la altura para cada tratamiento.\n\nCada paso es explícito y se puede leer de arriba hacia abajo, lo que facilita la comprensión y depuración del análisis (Wickham & Grolemund, 2017). El uso de pipes mejora significativamente la legibilidad y mantenibilidad del código, facilitando la colaboración y reduciendo la probabilidad de errores.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Manipulación de datos con dplyr y tidyr</span>"
    ]
  },
  {
    "objectID": "08.3_manipulacion.html#transformaciones-de-datos-con-tidyr",
    "href": "08.3_manipulacion.html#transformaciones-de-datos-con-tidyr",
    "title": "11  Manipulación de datos con dplyr y tidyr",
    "section": "11.5 Transformaciones de datos con tidyr",
    "text": "11.5 Transformaciones de datos con tidyr\nEl paquete tidyr es una herramienta fundamental para la reorganización y transformación de datos en R, permitiendo adaptar la estructura de los conjuntos de datos a los requerimientos de los métodos estadísticos clásicos. Estas transformaciones son esenciales para preparar los datos antes de aplicar técnicas como ANOVA, regresión o análisis descriptivos, ya que muchos procedimientos requieren que los datos estén en un formato específico (Wickham & Grolemund, 2017).\n\n11.5.1 Transformación de formato ancho a largo con pivot_longer()\nLa función pivot_longer() convierte varias columnas de un data frame en pares de nombre-valor, generando un formato largo. Este formato es especialmente útil en análisis estadísticos donde cada observación debe ocupar una fila y las variables medidas se representan en una columna adicional, como en el caso de ANOVA de medidas repetidas (Wickham & Grolemund, 2017).\nLa sintaxis principal es:\n\n# Sintaxis principal de la funcion pivot_longer ()\npivot_longer(\n  data,              # Data frame o tibble de entrada\n  cols,              # Columnas a transformar\n  names_to = \"name\", # Nombre de la nueva columna para las columnas originales\n  values_to = \"value\" # Nombre de la nueva columna para los valores originales\n)\n\nPara ilustrar el uso de pivot_longer(), consideremos un ejemplo simplificado:\n\n# Crear un data frame de ejemplo\ndatos_ancho &lt;- data.frame(\n  parcela = 1:3,\n  altura_2023 = c(150, 160, 155),\n  peso_2023 = c(80, 85, 82),\n  altura_2024 = c(165, 170, 168),\n  peso_2024 = c(88, 90, 89)\n)\n\ndatos_ancho\n\n  parcela altura_2023 peso_2023 altura_2024 peso_2024\n1       1         150        80         165        88\n2       2         160        85         170        90\n3       3         155        82         168        89\n\n\nEste data frame representa mediciones de altura y peso de tres parcelas en dos años diferentes. Para transformar este data frame a formato largo, podemos usar pivot_longer() de la siguiente manera:\n\n# Transformar a formato largo\ndatos_largo &lt;- datos_ancho %&gt;%\n  pivot_longer(\n    cols = c(altura_2023, peso_2023, altura_2024, peso_2024),\n    names_to = \"variable\",\n    values_to = \"valor\"\n  )\n\ndatos_largo\n\n# A tibble: 12 × 3\n   parcela variable    valor\n     &lt;int&gt; &lt;chr&gt;       &lt;dbl&gt;\n 1       1 altura_2023   150\n 2       1 peso_2023      80\n 3       1 altura_2024   165\n 4       1 peso_2024      88\n 5       2 altura_2023   160\n 6       2 peso_2023      85\n 7       2 altura_2024   170\n 8       2 peso_2024      90\n 9       3 altura_2023   155\n10       3 peso_2023      82\n11       3 altura_2024   168\n12       3 peso_2024      89\n\n\nEn este ejemplo:\n\ncols = c(altura_2023, peso_2023, altura_2024, peso_2024): Especifica las columnas que se van a transformar.\nnames_to = \"variable\": Indica que los nombres de las columnas originales se almacenarán en una nueva columna llamada “variable”.\nvalues_to = \"valor\": Indica que los valores de las columnas originales se almacenarán en una nueva columna llamada “valor”.\n\nEl resultado es un data frame en formato largo, donde cada fila representa una medición de altura o peso para una parcela en un año específico. Este formato es ideal para realizar análisis estadísticos que requieren que cada observación ocupe una fila, como ANOVA de medidas repetidas o modelos mixtos (Kutner et al., 2005).\n\n\n11.5.2 Transformación de formato largo a ancho con pivot_wider()\nLa función pivot_wider() realiza la operación inversa a pivot_longer(), transformando un data frame de formato largo a formato ancho. Esta función es útil cuando se necesita organizar los datos de manera que diferentes valores de una variable se conviertan en columnas separadas, facilitando la comparación entre grupos o condiciones (Wickham & Grolemund, 2017).\nLa sintaxis principal es:\n\n# Sintaxis principal de la funcion pivot_wider ()\npivot_wider(\n  # Data frame o tibble de entrada\n  data,              \n  # Columna cuyos valores se usarán como nombres de las nuevas columnas\n  names_from = , \n  # Columna cuyos valores se usarán para llenar las nuevas columnas\n  values_from =        \n)\n\nPara ilustrar el uso de pivot_wider(), consideremos el data frame datos_largo creado en la sección anterior. Para transformar este data frame de nuevo a formato ancho, podemos usar pivot_wider() de la siguiente manera:\n\n# Transformar a formato ancho\ndatos_ancho &lt;- datos_largo %&gt;%\n  pivot_wider(\n    names_from = variable,\n    values_from = valor\n  )\n\ndatos_ancho\n\n# A tibble: 3 × 5\n  parcela altura_2023 peso_2023 altura_2024 peso_2024\n    &lt;int&gt;       &lt;dbl&gt;     &lt;dbl&gt;       &lt;dbl&gt;     &lt;dbl&gt;\n1       1         150        80         165        88\n2       2         160        85         170        90\n3       3         155        82         168        89\n\n\nEn este ejemplo:\n\nnames_from = variable: Especifica que los valores de la columna variable (altura_2023, peso_2023, altura_2024, peso_2024) se utilizarán como nombres de las nuevas columnas.\nvalues_from = valor: Especifica que los valores de la columna valor se utilizarán para llenar las nuevas columnas.\n\nEl resultado es un data frame en formato ancho, donde cada fila representa una parcela y cada columna representa una medición de altura o peso en un año específico. Este formato facilita la comparación directa de las mediciones entre años para cada parcela.\n\n\n11.5.3 Separación y Unión de Columnas con separate() y unite()\nLas funciones separate() y unite() permiten manipular variables compuestas, dividiendo una columna en varias o combinando varias columnas en una sola. Estas funciones son útiles para limpiar y estructurar datos que contienen información combinada en una sola columna (Wickham & Grolemund, 2017).\nLa función separate() divide una columna en varias, utilizando un carácter separador. Su sintaxis principal es:\n\n# Sintaxis de la funcion separate ()\nseparate(\n  data,    # Data frame o tibble de entrada\n  col,     # Columna a dividir\n  into,    # Vector con los nombres de las nuevas columnas\n  sep      # Carácter separador (por defecto, cualquier carácter no alfanumérico)\n)\n\nPor ejemplo, considérese el siguiente subconjunto:\n\n# Crear el dataframe para el ejemplo\nmini_datos_comp &lt;- data.frame(\n  parcela_bloque = c(\"1-1\", \"2-2\", \"3-3\"),\n  altura_cm = c(70, 65, 60)\n)\n\nmini_datos_comp\n\n  parcela_bloque altura_cm\n1            1-1        70\n2            2-2        65\n3            3-3        60\n\n\nPara separar la columna parcela_bloque en dos columnas llamadas parcela y bloque, se utiliza:\n\nmini_separado &lt;- separate(\n  data = mini_datos_comp,\n  col = parcela_bloque,    # Columna a dividir\n  into = c(\"parcela\", \"bloque\"),# Nombres de las nuevas columnas\n  sep = \"-\"                # Carácter separador\n)\n\nmini_separado\n\n  parcela bloque altura_cm\n1       1      1        70\n2       2      2        65\n3       3      3        60\n\n\nEl argumento col indica la columna a dividir, into define los nombres de las nuevas columnas y sep especifica el carácter separador (Wickham & Grolemund, 2017).\nLa función unite() combina dos o más columnas en una sola, utilizando un carácter separador. Su sintaxis principal es:\n\n# Sintaxis de la funcion unite ()\nunite(\n  data,    # Data frame o tibble de entrada\n  col,     # Nombre de la nueva columna\n  ...,     # Columnas a unir\n  sep      # Carácter separador\n)\n\nPor ejemplo, para volver a unir las columnas parcela y bloque en una sola columna parcela_bloque:\n\nmini_unido &lt;- unite(\n  data = mini_separado,\n  col = \"parcela_bloque\", # Nombre de la nueva columna\n  parcela, bloque,        # Columnas a unir\n  sep = \"-\"                # Carácter separador\n)\n\nmini_unido\n\n  parcela_bloque altura_cm\n1            1-1        70\n2            2-2        65\n3            3-3        60\n\n\nEl argumento col define el nombre de la nueva columna resultante, los siguientes argumentos son las columnas a unir y sep indica el carácter separador (Wickham & Grolemund, 2017).",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Manipulación de datos con dplyr y tidyr</span>"
    ]
  },
  {
    "objectID": "08.3_manipulacion.html#ejemplo-integrado-preparación-de-datos-para-análisis-estadístico-clásico",
    "href": "08.3_manipulacion.html#ejemplo-integrado-preparación-de-datos-para-análisis-estadístico-clásico",
    "title": "11  Manipulación de datos con dplyr y tidyr",
    "section": "11.6 Ejemplo integrado: Preparación de datos para análisis estadístico clásico",
    "text": "11.6 Ejemplo integrado: Preparación de datos para análisis estadístico clásico\nA continuación se muestra un flujo de trabajo típico para preparar los datos del experimento agrícola antes de realizar un análisis de varianza (ANOVA), utilizando dplyr y tidyr.\n\n# 1. Eliminar valores faltantes en altura y peso\ndatos_limpios &lt;- datos_cultivo %&gt;%\n  filter(!is.na(altura_cm), !is.na(peso_gr))\n\n# 2. Crear variables derivadas\ndatos_limpios &lt;- datos_limpios %&gt;%\n  mutate(\n    indice_crecimiento = altura_cm / peso_gr,\n    categoria_altura = ifelse(altura_cm &gt; 65, \"Alto\", \"Bajo\")\n  )\n\n# 3. Agrupar por tratamiento y calcular estadísticas descriptivas\nresumen_tratamiento &lt;- datos_limpios %&gt;%\n  group_by(tratamiento) %&gt;%\n  summarize(\n    media_altura = mean(altura_cm),\n    sd_altura = sd(altura_cm),\n    n = n()\n  )\nresumen_tratamiento\n\n# A tibble: 4 × 4\n  tratamiento    media_altura sd_altura     n\n  &lt;chr&gt;                 &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt;\n1 Control                63.5      3.17     4\n2 Fertilizante A         63.3     13.1      4\n3 Fertilizante B         70.2      4.82     4\n4 Fertilizante C         71.3      9.27     4\n\n# 4. Transformar a formato largo para análisis multivariado\ndatos_largo &lt;- datos_limpios %&gt;%\n  pivot_longer(\n    cols = c(altura_cm, peso_gr),\n    names_to = \"variable\",\n    values_to = \"valor\"\n  )\nhead(datos_largo)\n\n# A tibble: 6 × 11\n  parcela tratamiento bloque daño_plaga fecha_siembra indice_crecimiento\n    &lt;int&gt; &lt;chr&gt;        &lt;int&gt; &lt;chr&gt;      &lt;date&gt;                     &lt;dbl&gt;\n1       1 Control          1 Medio      2024-01-10                 0.637\n2       1 Control          1 Medio      2024-01-10                 0.637\n3       2 Control          2 Medio      2024-01-08                 0.547\n4       2 Control          2 Medio      2024-01-08                 0.547\n5       4 Control          4 Medio      2024-01-09                 0.645\n6       4 Control          4 Medio      2024-01-09                 0.645\n# ℹ 5 more variables: altura_m &lt;dbl&gt;, peso_kg &lt;dbl&gt;, categoria_altura &lt;chr&gt;,\n#   variable &lt;chr&gt;, valor &lt;dbl&gt;",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Manipulación de datos con dplyr y tidyr</span>"
    ]
  },
  {
    "objectID": "08.3_manipulacion.html#comparación-entre-la-manipulación-de-datos-con-r-base-y-tidyverse",
    "href": "08.3_manipulacion.html#comparación-entre-la-manipulación-de-datos-con-r-base-y-tidyverse",
    "title": "11  Manipulación de datos con dplyr y tidyr",
    "section": "11.6 Comparación entre la manipulación de datos con R base y tidyverse",
    "text": "11.6 Comparación entre la manipulación de datos con R base y tidyverse\nLa manipulación de datos constituye una etapa esencial en el análisis estadístico clásico, ya que permite preparar, transformar y explorar la información antes de aplicar técnicas inferenciales o modelos predictivos. En el entorno R, existen dos enfoques principales para realizar estas tareas: el uso de funciones base y el empleo de paquetes del tidyverse, como dplyr y tidyr. A continuación, se presenta una comparación estructurada de ambos enfoques, considerando aspectos clave como sintaxis, legibilidad, flexibilidad y reproducibilidad (Wickham & Grolemund, 2017).\n\n\n\n\n\n\n\n\nAspecto\nR base\ntidyverse (dplyr/tidyr)\n\n\n\n\nSintaxis\nUso de corchetes, funciones como subset(), apply(), y anidación.\nUso de funciones verbales (filter(), select(), mutate(), etc.) y pipes %&gt;%.\n\n\nLegibilidad\nEl código puede ser difícil de leer, especialmente con operaciones anidadas.\nEl flujo de trabajo es secuencial y fácil de seguir, cada paso en una línea.\n\n\nCreación de variables\nSe usa $ o transform().\nSe usa mutate(), que permite crear o modificar variables de forma clara.\n\n\nFiltrado de filas\nSe usan corchetes o subset().\nSe usa filter(), con sintaxis más intuitiva y sin necesidad de repetir el nombre del data frame.\n\n\nSelección de columnas\nSe usan corchetes o select().\nSe usa select(), con funciones auxiliares como starts_with(), ends_with().\n\n\nAgrupamiento y resumen\nSe usan tapply(), aggregate(), o bucles.\nSe usan group_by() y summarize(), facilitando el cálculo de estadísticas por grupo.\n\n\nTransformación de formato\nSe usan funciones como reshape(), melt(), cast().\nSe usan pivot_longer() y pivot_wider(), con sintaxis más clara y moderna.\n\n\nManejo de variables compuestas\nSe requiere manipulación manual con funciones como strsplit().\nSe usan separate() y unite(), que simplifican la división y combinación de columnas.\n\n\nReproducibilidad\nEl código puede ser menos reproducible y más propenso a errores.\nEl uso de pipes y funciones verbales mejora la reproducibilidad y la claridad del análisis.\n\n\nCurva de aprendizaje\nFamiliar para usuarios con experiencia previa en R, pero puede ser menos intuitivo para principiantes.\nMás accesible para principiantes, especialmente por la coherencia y claridad de la sintaxis.\n\n\n\nEn síntesis, el enfoque tidyverse ofrece ventajas notables en términos de claridad, reproducibilidad y facilidad de uso, especialmente en flujos de trabajo complejos o colaborativos. Sin embargo, el conocimiento de las funciones base de R sigue siendo valioso, ya que permite comprender el funcionamiento interno del lenguaje y resolver tareas específicas de manera eficiente (Wickham & Grolemund, 2017).",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Manipulación de datos con dplyr y tidyr</span>"
    ]
  },
  {
    "objectID": "09.1_visualizacion.html",
    "href": "09.1_visualizacion.html",
    "title": "12  Introducción a la visualización de datos",
    "section": "",
    "text": "12.1 Historia y evolución de la visualización en estadística\nLa visualización de datos se define como el proceso de representar información cuantitativa y cualitativa mediante gráficos, diagramas y otras formas visuales. Su objetivo principal es facilitar la comprensión, el análisis y la comunicación de los datos, permitiendo identificar patrones, tendencias, relaciones y anomalías que podrían pasar desapercibidas en tablas numéricas o descripciones textuales. En el contexto del análisis estadístico, la visualización es una herramienta esencial tanto en la fase exploratoria como en la presentación de resultados, ya que ayuda a validar supuestos, comunicar hallazgos y respaldar la toma de decisiones informadas (Wickham, 2016).\nLa importancia de la visualización radica en su capacidad para transformar datos complejos en representaciones accesibles y comprensibles, promoviendo la transparencia y la reproducibilidad en la investigación científica. Además, los gráficos permiten detectar errores en los datos, identificar valores atípicos y comprender la distribución de las variables antes de aplicar técnicas estadísticas formales (Tufte, 2001).\nLa visualización de datos tiene una larga tradición en la historia de la estadística. Sus orígenes se remontan al siglo XVIII, cuando se comenzaron a utilizar gráficos para representar información demográfica y económica. Uno de los hitos más importantes fue la invención del gráfico de barras por William Playfair en 1786, quien también introdujo el gráfico de líneas y el gráfico circular. Posteriormente, Florence Nightingale empleó diagramas de área para comunicar la mortalidad en hospitales militares, demostrando el poder de los gráficos para influir en la opinión pública y en la toma de decisiones políticas (Friendly, 2008).\nA lo largo del siglo XX, la visualización se consolidó como una disciplina fundamental en la estadística, especialmente con el desarrollo de la computación y el software estadístico, que permitieron la creación de gráficos más complejos y personalizados. En la actualidad, la visualización de datos es un componente central en el análisis exploratorio de datos (EDA) y en la comunicación científica, siendo reconocida como una herramienta indispensable para el trabajo estadístico (Wickham, 2016).",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introducción a la visualización de datos</span>"
    ]
  },
  {
    "objectID": "09.1_visualizacion.html#principios-básicos-de-la-visualización-efectiva",
    "href": "09.1_visualizacion.html#principios-básicos-de-la-visualización-efectiva",
    "title": "12  Introducción a la visualización de datos",
    "section": "12.2 Principios básicos de la visualización efectiva",
    "text": "12.2 Principios básicos de la visualización efectiva\nLa visualización efectiva de datos es un componente esencial para asegurar que la información transmitida sea comprensible, precisa y útil en la toma de decisiones estadísticas. Para lograr este objetivo, es fundamental considerar tres principios clave: claridad, precisión y eficiencia. Estos principios han sido ampliamente discutidos en la literatura especializada, destacando su relevancia en la comunicación gráfica de datos (Tufte, 2001; Cleveland, 1993).\n\n12.2.1 Claridad\nLa claridad en la visualización implica que el gráfico sea comprensible y transmita el mensaje principal de manera directa, sin ambigüedades ni elementos distractores. Para lograr claridad, se deben considerar los siguientes aspectos (Tufte, 2001):\n\nEliminar elementos decorativos innecesarios, como fondos llamativos, sombras o efectos tridimensionales que no aportan información relevante.\nUtilizar títulos descriptivos y etiquetas claras en los ejes, de modo que el lector comprenda de inmediato qué variables se están representando.\nIncluir leyendas explicativas cuando se utilicen colores, símbolos o líneas para diferenciar grupos o categorías.\nMantener un diseño limpio y ordenado, evitando la sobrecarga visual y el uso excesivo de colores o tipografías.\nPresentar la información de manera secuencial y lógica, facilitando la interpretación del gráfico desde el primer vistazo.\n\n\n\n12.2.2 Precisión\nLa precisión se refiere a la representación fiel y exacta de los datos, evitando distorsiones que puedan inducir a interpretaciones erróneas. Para asegurar la precisión en los gráficos, se recomienda (Cleveland, 1993):\n\nUtilizar escalas proporcionales y adecuadas al rango de los datos, evitando truncar ejes o manipular escalas que alteren la percepción de las diferencias o relaciones.\nRepresentar todos los datos relevantes, sin omitir valores atípicos o subconjuntos importantes que puedan influir en la interpretación.\nSeleccionar el tipo de gráfico adecuado para el tipo de variable y el objetivo del análisis, por ejemplo, no usar gráficos de barras para variables continuas.\nEvitar la exageración visual de diferencias mediante el uso de áreas, volúmenes o efectos visuales que no correspondan a la magnitud real de los datos.\nRevisar cuidadosamente los datos y la codificación del gráfico para prevenir errores de transcripción o interpretación.\n\n\n\n12.2.3 Eficiencia\nLa eficiencia en la visualización implica transmitir la mayor cantidad de información relevante con el menor esfuerzo cognitivo posible para el usuario. Para lograr eficiencia, se deben seguir estas recomendacionesndefined(Tufte, 2001; Cleveland, 1993):\n\nResumir la información de manera que el gráfico muestre los aspectos más importantes sin saturar de detalles innecesarios.\nUtilizar gráficos sintéticos, como diagramas de caja o gráficos de dispersión, que permiten visualizar múltiples características de los datos en una sola imagen.\nPriorizar la información relevante para el objetivo del análisis, evitando la inclusión de variables o elementos que no aportan al mensaje principal.\nFacilitar la comparación entre grupos o categorías mediante el uso de colores, formas o posiciones consistentes y fácilmente distinguibles.\nOptimizar el tamaño y la resolución del gráfico para que sea legible tanto en pantalla como en impresiones.\n\n\n\n12.2.4 Errores comunes a evitar en la visualización de datos\nExisten errores frecuentes que pueden comprometer la efectividad de una visualización. Entre los más relevantes se encuentran (Tufte, 2001; Cleveland, 1993):\n\nUso excesivo de colores, degradados o efectos visuales que dificultan la interpretación y distraen del mensaje principal.\nOmitir etiquetas, títulos o leyendas, lo que genera confusión sobre el significado de los elementos representados.\nElegir un tipo de gráfico inadecuado para el tipo de datos, como utilizar gráficos circulares para comparar muchas categorías o gráficos de líneas para variables categóricas.\nManipular escalas de los ejes para exagerar o minimizar diferencias, lo que puede inducir a conclusiones erróneas.\nPresentar demasiada información en un solo gráfico, lo que sobrecarga al usuario y dificulta la extracción de conclusiones claras.\n\n\n\n12.2.5 Recomendaciones para una visualización efectiva\nPara garantizar la integridad y la transparencia en la presentación de los datos, se recomienda (Tufte, 2001; Cleveland, 1993):\n\nSeleccionar el tipo de gráfico más adecuado según el objetivo del análisis y la naturaleza de las variables.\nMantener un diseño simple, claro y directo, priorizando la comprensión del mensaje principal.\nRevisar y validar los gráficos antes de su presentación, asegurando que representen fielmente los datos y sean interpretables por el público objetivo.\nUtilizar recursos visuales (colores, formas, tamaños) de manera coherente y justificada, evitando la sobrecarga visual.\nDocumentar las decisiones tomadas en la construcción del gráfico, facilitando la reproducibilidad y la transparencia en el análisis.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introducción a la visualización de datos</span>"
    ]
  },
  {
    "objectID": "09.1_visualizacion.html#tipos-de-gráficos-y-su-utilidad-en-estadística-clásica",
    "href": "09.1_visualizacion.html#tipos-de-gráficos-y-su-utilidad-en-estadística-clásica",
    "title": "12  Introducción a la visualización de datos",
    "section": "12.3 Tipos de gráficos y su utilidad en estadística clásica",
    "text": "12.3 Tipos de gráficos y su utilidad en estadística clásica\nEn la estadística clásica, la selección adecuada del tipo de gráfico es fundamental para explorar los datos, validar supuestos y comunicar resultados de manera efectiva. A continuación, se describen los principales tipos de gráficos utilizados, sus características y su utilidad específica en el análisis estadístico, siguiendo las recomendaciones de la literatura especializada (Venables & Ripley, 2002; Cleveland, 1993).\n\n12.3.1 Gráficos de barras\nLos gráficos de barras permiten comparar frecuencias o proporciones entre categorías de una variable cualitativa. Cada barra representa una categoría y su altura es proporcional a la frecuencia o porcentaje correspondiente. Este tipo de gráfico facilita la identificación de categorías dominantes o poco representadas y es especialmente útil en el análisis de variables como sexo, grupo de tratamiento o respuestas dicotómicas. Además, los gráficos de barras ayudan a detectar patrones de distribución y posibles sesgos en la recolección de datos (Cleveland, 1993).\n\n\n\n\n\n\n\n12.3.2 Histogramas\nEl histograma es la herramienta principal para visualizar la distribución de variables cuantitativas continuas. Agrupa los datos en intervalos y muestra la frecuencia de observaciones en cada uno. Esta representación permite identificar la forma de la distribución, detectar asimetrías, curtosis, valores atípicos y la presencia de múltiples modos. Los histogramas son esenciales para evaluar el supuesto de normalidad, requisito frecuente en pruebas como el ANOVA y la regresión lineal (Venables & Ripley, 2002).\n\n\n\n\n\n\n\n12.3.3 Diagramas de caja (boxplots)\nEl diagrama de caja, o boxplot, resume la distribución de una variable cuantitativa mostrando la mediana, los cuartiles y los valores extremos. Este gráfico facilita la comparación entre grupos y la identificación de valores atípicos. Además, permite evaluar la homogeneidad de la varianza, aspecto crucial en el análisis de varianza. Su interpretación sencilla y su capacidad para sintetizar información lo convierten en una herramienta indispensable en la estadística descriptiva y comparativa (Cleveland, 1993).\n\n\n\n\n\n\n\n12.3.4 Gráficos de dispersión (scatterplots)\nLos gráficos de dispersión se utilizan para analizar la relación entre dos variables cuantitativas. Cada punto representa una observación y su posición refleja los valores de ambas variables. Este tipo de gráfico permite identificar patrones de asociación, linealidad, presencia de valores atípicos y posibles agrupamientos. Además, es fundamental para explorar la existencia de correlaciones y para evaluar el supuesto de linealidad en modelos de regresión (Venables & Ripley, 2002).\n\n\n\n\n\n\n\n12.3.5 Gráficos QQ (quantile-quantile)\nEl gráfico QQ compara la distribución de los datos observados con una distribución teórica, generalmente la normal. Si los puntos del gráfico se alinean sobre la diagonal, se puede concluir que los datos siguen la distribución teórica. Este gráfico es esencial para evaluar el supuesto de normalidad en pruebas paramétricas y para detectar desviaciones sistemáticas, colas pesadas o asimetrías en la distribución de los datos (Cleveland, 1993).\n\n\n\n\n\n\n\n12.3.6 Gráficos de residuos\nLos gráficos de residuos muestran la diferencia entre los valores observados y los valores ajustados por un modelo estadístico. Un patrón aleatorio en estos gráficos indica que el modelo es adecuado, mientras que la presencia de patrones sistemáticos sugiere problemas de especificación, heterocedasticidad o autocorrelación. Estos gráficos son fundamentales en la validación de modelos de regresión y en la toma de decisiones sobre la necesidad de transformar variables o ajustar el modelo (Venables & Ripley, 2002).",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Introducción a la visualización de datos</span>"
    ]
  },
  {
    "objectID": "09.2_visualizacion.html",
    "href": "09.2_visualizacion.html",
    "title": "13  Sistema Gráfico Base de R",
    "section": "",
    "text": "13.1 Arquitectura del sistema gráfico base\nLa representación gráfica de la información constituye un componente indispensable para la comprensión, la comunicación y la validación de resultados estadísticos. El sistema gráfico base de R ofrece un conjunto de herramientas versátiles que permiten construir visualizaciones de alta calidad siguiendo un enfoque incremental, en el cual cada elemento puede añadirse o modificarse de forma independiente (Murrell, 2018). A continuación se describe, de manera detallada y pedagógica, la arquitectura de este sistema y las funciones esenciales para el análisis exploratorio y la comprobación de supuestos en la estadística clásica.\nEl propósito principal de la visualización es facilitar la detección de patrones, tendencias y anomalías que resultan difíciles de advertir mediante inspección numérica (Cleveland, 1993). Además, las gráficas permiten evaluar supuestos tales como normalidad, homocedasticidad y linealidad, que son cruciales para la validez de métodos paramétricos como la regresión lineal y el ANOVA (Venables & Ripley, 2002). Bajo esta perspectiva, la elaboración de gráficos debe regirse por principios de claridad, precisión y economía visual (Tufte, 2001).\nEl sistema gráfico base de R se sustenta en tres principios operativos:\n# Ejemplo ilustrativo de construcción modular\nplot(NULL,                         # Lienzo vacío\n     xlim = c(0, 10), ylim = c(0, 10),\n     xlab = \"Eje X\", ylab = \"Eje Y\",\n     main = \"Demostración de modularidad\")\n\ngrid(col = \"gray90\")               # Capa 1: cuadrícula\n\n# Capa 2: puntos de datos simulados\nset.seed(123)\nx &lt;- runif(50, 0, 10)\ny &lt;- 0.8 * x + rnorm(50, 0, 1)\npoints(x, y, pch = 16, col = \"navy\")\n\n# Capa 3: línea de tendencia\nabline(lm(y ~ x), col = \"red\", lwd = 2, lty = 2)",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Sistema Gráfico Base de R</span>"
    ]
  },
  {
    "objectID": "09.2_visualizacion.html#funciones-gráficas-básicas-de-r",
    "href": "09.2_visualizacion.html#funciones-gráficas-básicas-de-r",
    "title": "13  Sistema Gráfico Base de R",
    "section": "13.2 Funciones gráficas básicas de R",
    "text": "13.2 Funciones gráficas básicas de R\nEl sistema gráfico base de R constituye una de las herramientas más accesibles y versátiles para la visualización de datos en estadística clásica. Estas funciones permiten crear gráficos de manera rápida y flexible, facilitando tanto la exploración inicial de los datos como la comprobación de supuestos estadísticos fundamentales. El enfoque de R base se basa en la construcción secuencial de gráficos, donde cada elemento puede ser añadido o modificado mediante argumentos y funciones auxiliares, lo que resulta especialmente útil en el análisis exploratorio y diagnóstico (Murrell, 2018; Venables & Ripley, 2002).\nEntre las funciones más utilizadas se encuentran:\n\nplot(): función genérica para gráficos de dispersión, líneas y otros tipos de visualizaciones.\nhist(): para la creación de histogramas que muestran la distribución de variables cuantitativas.\nboxplot(): para diagramas de caja que resumen la dispersión y los valores atípicos.\nbarplot(): para gráficos de barras de frecuencias o proporciones.\nqqnorm() y qqline(): para gráficos Q-Q que evalúan la normalidad de los datos.\npairs(): para matrices de gráficos de dispersión entre varias variables.\n\nEstas funciones son la base para la mayoría de los análisis gráficos en estadística clásica, permitiendo una rápida inspección visual de los datos y la validación de supuestos (Venables & Ripley, 2002).",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Sistema Gráfico Base de R</span>"
    ]
  },
  {
    "objectID": "09.2_visualizacion.html#creación-de-gráficos-exploratorios",
    "href": "09.2_visualizacion.html#creación-de-gráficos-exploratorios",
    "title": "13  Visualizaciones Base de R",
    "section": "13.2 Creación de gráficos exploratorios",
    "text": "13.2 Creación de gráficos exploratorios\nLa creación de gráficos exploratorios en R base es fundamental para el análisis inicial de los datos y la detección de patrones, tendencias y anomalías. A continuación se detallan los principales tipos de gráficos, su sintaxis y los argumentos más relevantes, acompañados de ejemplos y explicaciones pedagógicas (Venables & Ripley, 2002; Murrell, 2018).\n\n13.2.1 Histogramas\nLa función principal para crear histogramas en R es hist(). Su sintaxis general es:\n\nhist(x, \n     breaks = \"Sturges\", \n     freq = TRUE, \n     col = NULL, \n     border = NULL, \n     main = NULL, \n     xlab = NULL, \n     ylab = NULL, \n     ...)\n\nExplicación de los argumentos:\n\nx: Vector numérico con los datos a graficar.\nbreaks: Define el número de intervalos (bins) o el método para calcularlos. Puede ser un número, un vector de puntos de corte, o un método como “Sturges”, “Scott”, “FD”.\nfreq: Si es TRUE, el eje Y muestra frecuencias absolutas; si es FALSE, muestra densidades.\ncol: Color de las barras.\nborder: Color del borde de las barras.\nmain: Título principal del gráfico.\nxlab, ylab: Etiquetas de los ejes X e Y.\n...: Otros argumentos gráficos adicionales.\n\nEjemplo detallado:\n\n# Simulación de datos\nset.seed(123)\nnotas &lt;- rnorm(200, mean = 70, sd = 10)\n\n# Histograma personalizado\nhist(notas,\n     breaks = 15,                # Número de intervalos\n     freq = TRUE,                # Mostrar frecuencias absolutas\n     col = \"lightblue\",          # Color de las barras\n     border = \"darkblue\",        # Color del borde\n     main = \"Histograma de calificaciones\", # Título\n     xlab = \"Puntaje\",           # Etiqueta eje X\n     ylab = \"Frecuencia\")        # Etiqueta eje Y\n\n\n\n\n\n\n\n\n\n\n13.2.2 Diagramas de caja (boxplots)\nLa función principal es boxplot(). Su sintaxis general es:\n\nboxplot(formula, \n        data = NULL, \n        main = NULL, \n        xlab = NULL, \n        ylab = NULL, \n        col = NULL, \n        border = NULL, \n        notch = FALSE, \n        outline = TRUE, \n        ...)\n\n\nformula: Expresión del tipo y ~ grupo para comparar grupos.\ndata: Data frame donde buscar las variables.\nmain, xlab, ylab: Títulos y etiquetas.\ncol: Colores de las cajas.\nborder: Color del borde de las cajas.\nnotch: Si es TRUE, añade una muesca para comparar medianas.\noutline: Si es TRUE, muestra valores atípicos.\n...: Otros argumentos gráficos.\n\nEjemplo detallado:\n\n# Simulación de datos para dos grupos\nset.seed(123)\ngrupo &lt;- factor(rep(c(\"Control\", \"Tratamiento\"), each = 100))\nvalores &lt;- c(rnorm(100, 70, 8), rnorm(100, 75, 10))\n\n# Boxplot personalizado\nboxplot(valores ~ grupo,\n        main = \"Comparación entre Grupos\",\n        xlab = \"Grupo\",\n        ylab = \"Valores\",\n        col = c(\"lightgreen\", \"lightcoral\"),\n        border = \"darkgray\",\n        notch = TRUE,             # Mostrar muesca para comparar medianas\n        outline = TRUE)           # Mostrar valores atípicos\n\n\n\n\n\n\n\n\n\n\n13.2.3 Gráficos de dispersión\nLa función principal es plot(). Su sintaxis general para dos variables es:\n\nplot(x, y, \n     type = \"p\", \n     main = NULL, \n     sub = NULL, \n     xlab = NULL, \n     ylab = NULL, \n     pch = 1, \n     col = NULL, \n     cex = 1, \n     ...)\n\nExplicación de los argumentos:\n\nx, y: Vectores numéricos de igual longitud.\ntype: Tipo de gráfico (“p” para puntos, “l” para líneas, “b” para ambos).\nmain, sub: Título principal y subtítulo.\nxlab, ylab: Etiquetas de los ejes.\npch: Tipo de símbolo para los puntos (1: círculo, 16: círculo sólido, 17: triángulo, etc.).\ncol: Color de los puntos.\ncex: Tamaño relativo de los puntos.\n...: Otros argumentos gráficos.\n\nEjemplo detallado:\n\n# Simulación de datos\nset.seed(123)\nx &lt;- rnorm(100, mean = 10, sd = 2)\ny &lt;- 2 * x + rnorm(100, 0, 3)\n\n# Gráfico de dispersión personalizado\nplot(x, y,\n     type = \"p\",                  # Tipo de gráfico: puntos\n     main = \"Relación entre X e Y\",\n     sub = \"Datos simulados\",\n     xlab = \"Variable X\",\n     ylab = \"Variable Y\",\n     pch = 16,                    # Círculo sólido\n     col = \"navy\",                # Color de los puntos\n     cex = 1.2)                   # Tamaño de los puntos\n\n# Añadir línea de regresión\nabline(lm(y ~ x), col = \"red\", lwd = 2, lty = 2)\n\n\n\n\n\n\n\n\n\n\n13.2.4 Gráficos de líneas\nPara series temporales o secuencias, se usa plot() con type = \"l\":\n\nplot(x, y, \n     type = \"l\", \n     main = NULL, \n     xlab = NULL, \n     ylab = NULL, \n     col = NULL, \n     lwd = 1, \n     ...)\n\n\ntype = \"l\": Dibuja una línea.\nlwd: Grosor de la línea.\n\nEjemplo detallado:\n\n# Simulación de serie temporal\nset.seed(123)\ntiempo &lt;- 1:50\nmedidas &lt;- cumsum(rnorm(50))\n\n# Gráfico de líneas\nplot(tiempo, medidas,\n     type = \"l\",                  # Tipo de gráfico: línea\n     main = \"Serie temporal simulada\",\n     xlab = \"Tiempo\",\n     ylab = \"Medida\",\n     col = \"darkred\",\n     lwd = 2)                     # Grosor de la línea\n\n# Añadir puntos sobre la línea\npoints(tiempo, medidas, pch = 16, col = \"black\")",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Visualizaciones Base de R</span>"
    ]
  },
  {
    "objectID": "09.2_visualizacion.html#visualización-para-la-comprobación-de-supuestos-estadísticos",
    "href": "09.2_visualizacion.html#visualización-para-la-comprobación-de-supuestos-estadísticos",
    "title": "13  Sistema Gráfico Base de R",
    "section": "13.4 Visualización para la comprobación de supuestos estadísticos",
    "text": "13.4 Visualización para la comprobación de supuestos estadísticos\nLa validación gráfica de los supuestos estadísticos es un paso esencial para garantizar la validez de los análisis en la estadística clásica. Antes de aplicar pruebas como ANOVA o modelos de regresión lineal, es fundamental verificar visualmente la normalidad, la homocedasticidad y la linealidad de los datos. El sistema gráfico base de R proporciona herramientas específicas para evaluar estos supuestos de manera eficiente y pedagógica (Venables & Ripley, 2002; Murrell, 2018).\n\n13.4.1 Gráficos Q-Q: Evaluación visual de la normalidad\nEl gráfico Q-Q (quantile-quantile) es una herramienta visual poderosa para comparar la distribución de los datos observados con una distribución teórica, generalmente la normal. Si los puntos del gráfico se alinean sobre la diagonal, se puede inferir que los datos siguen la distribución de referencia. Las desviaciones sistemáticas de esta línea indican alejamientos de la normalidad, lo que puede requerir transformaciones de los datos o el uso de métodos no paramétricos (Cleveland, 1993).\nSintaxis básica y explicación:\n\nqqnorm(): Genera el gráfico Q-Q de los datos frente a la normal.\nqqline(): Añade la línea de referencia teórica.\n\nEjemplo:\n\n# Simulación de tres conjuntos de datos con diferentes distribuciones\nset.seed(123)\n# Datos con distribución normal\ndatos_normales &lt;- rnorm(100, mean = 0, sd = 1)   \n# Datos con distribución exponencial (asimétrica)\ndatos_asimetricos &lt;- rexp(100, rate = 1)        \n# Datos con distribución uniforme\ndatos_uniformes &lt;- runif(100, min = -3, max = 3)   \n\n# Configuración de la ventana gráfica para mostrar tres gráficos en una fila\npar(mfrow = c(1, 3))\n\n# Gráfico Q-Q para datos normales\nqqnorm(datos_normales,\n       main = \"Normal\",\n       pch = 16,                # Círculo sólido\n       col = \"navy\")            # Color de los puntos\nqqline(datos_normales, col = \"red\", lwd = 2)  # Línea de referencia\n\n# Gráfico Q-Q para datos asimétricos\nqqnorm(datos_asimetricos,\n       main = \"Exponencial\",\n       pch = 16,\n       col = \"darkgreen\")\nqqline(datos_asimetricos, col = \"red\", lwd = 2)\n\n# Gráfico Q-Q para datos uniformes\nqqnorm(datos_uniformes,\n       main = \"Uniforme\",\n       pch = 16,\n       col = \"purple\")\nqqline(datos_uniformes, col = \"red\", lwd = 2)\n\n\n\n\n\n\n\n# Restaurar la configuración original de la ventana gráfica\npar(mfrow = c(1, 1))\n\nLa interpretación de estos gráficos se basa en el patrón que forman los puntos en relación con la línea de referencia. Según Venables & Ripley (2002), las desviaciones más comunes incluyen:\n\nColas pesadas: cuando los extremos se alejan de la línea.\nAsimetría: cuando se forma un patrón curvilíneo.\nBimodalidad: cuando aparece un patrón en forma de S.\n\n\n\n13.4.2 Gráficos de diagnóstico para modelos de regresión\nLa regresión lineal clásica asume linealidad, normalidad de los residuos, homocedasticidad (varianza constante) e independencia. R facilita la evaluación simultánea de estos supuestos mediante gráficos de diagnóstico automáticos generados con la función plot() aplicada a objetos de clase lm (Murrell, 2018).\nEjemplo:\n\n# Simulación de datos para regresión lineal\nset.seed(123)\nx &lt;- seq(1, 100)                           # Variable predictora\ny &lt;- 2 * x + rnorm(100, 0, 10)             # Variable respuesta con error normal\ndatos &lt;- data.frame(x = x, y = y)          # Crear data frame\n\n# Ajuste del modelo de regresión lineal\nmodelo &lt;- lm(y ~ x, data = datos)          # Ajustar modelo\n\n# Configuración de la ventana gráfica para mostrar cuatro gráficos\npar(mfrow = c(2, 2))\nplot(modelo)                               # Generar gráficos diagnósticos\n\n\n\n\n\n\n\npar(mfrow = c(1, 1))                       # Restaurar configuración\n\nDescripción e interpretación de los gráficos generados:\n\nResiduos vs valores ajustados: Permite evaluar la linealidad y la homogeneidad de la varianza. Un patrón aleatorio indica que se cumplen los supuestos; patrones sistemáticos sugieren problemas de especificación del modelo.\nQ-Q de residuos: Evalúa la normalidad de los residuos. Desviaciones de la línea diagonal indican que los residuos no son normales.\nScale-Location (raíz cuadrada de los residuos estandarizados vs valores ajustados): Permite examinar la homogeneidad de la varianza. Una banda horizontal indica homogeneidad de la varianza.\nResiduos vs leverage: Identifica observaciones influyentes. Puntos alejados o con gran leverage pueden indicar outliers o casos influyentes que afectan el ajuste del modelo (Murrell, 2018).",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Sistema Gráfico Base de R</span>"
    ]
  },
  {
    "objectID": "09.2_visualizacion.html#personalización-de-gráficos-en-r-base",
    "href": "09.2_visualizacion.html#personalización-de-gráficos-en-r-base",
    "title": "13  Sistema Gráfico Base de R",
    "section": "13.5 Personalización de gráficos en R base",
    "text": "13.5 Personalización de gráficos en R base\nLa personalización de gráficos es un aspecto fundamental para lograr visualizaciones claras, informativas y estéticamente agradables. En el sistema gráfico base de R, la personalización se realiza mediante la modificación de los argumentos de las funciones gráficas principales y la incorporación de elementos adicionales a través de funciones auxiliares. Esta flexibilidad permite adaptar cada gráfico a las necesidades específicas del análisis y a los estándares de comunicación científica (Murrell, 2018).\n\n13.5.1 Argumentos y funciones clave para la personalizaciónA continuación se describen los argumentos y funciones más relevantes para la personalización de gráficos en R base:\nA continuación se describen los argumentos y funciones más relevantes para la personalización de gráficos en R base:\n\nmain, sub, xlab, ylab: Permiten definir el título principal, subtítulo y las etiquetas de los ejes X e Y, respectivamente, facilitando la interpretación del gráfico.\ncol, border, pch, lty, lwd: Controlan el color de los elementos, el color del borde, el tipo de símbolo para los puntos, el tipo de línea y el grosor de las líneas, respectivamente.\ncex, cex.axis, cex.lab, cex.main: Ajustan el tamaño relativo de los símbolos, los textos de los ejes, las etiquetas y el título principal.\nlegend(): Añade leyendas explicativas en posiciones específicas del gráfico, mejorando la comprensión de los elementos representados.\ntext(): Permite agregar texto en coordenadas específicas, útil para destacar valores o anotar observaciones relevantes.\nabline(): Añade líneas horizontales, verticales o de regresión, facilitando la identificación de tendencias o referencias.\ngrid(): Incorpora una cuadrícula de fondo, lo que ayuda a la lectura precisa de las coordenadas y la comparación visual de los datos.\n\n\n\n13.5.2 Ejemplo integral\nA continuación se presenta un ejemplo completo que ilustra cómo combinar estos argumentos y funciones para lograr una visualización profesional y clara:\n\n# Simulación de datos para el ejemplo\nset.seed(123)\nx &lt;- rnorm(100, mean = 10, sd = 2)\ny &lt;- 2 * x + rnorm(100, 0, 3)\n\n# Gráfico de dispersión personalizado\nplot(x, y,\n     main = \"Gráfico personalizado\",      # Título principal\n     sub = \"Datos simulados\",             # Subtítulo\n     xlab = \"Variable X\",                 # Etiqueta eje X\n     ylab = \"Variable Y\",                 # Etiqueta eje Y\n     col = \"black\",                        # Color de los puntos\n     pch = 18,                            # Símbolo: rombo sólido\n     cex = 1.5,                           # Tamaño de los puntos\n     cex.main = 1.2,                      # Tamaño del título\n     cex.lab = 1.1)                       # Tamaño de las etiquetas\n\n# Añadir línea de regresión lineal\nabline(lm(y ~ x), col = \"red\", lwd = 2, lty = 2)  # Línea de tendencia\n\n# Añadir leyenda explicativa\nlegend(\"topleft\",\n       legend = c(\"Datos\", \"Ajuste lineal\"),\n       pch = c(18, NA),                  # Símbolo para los datos\n       lty = c(NA, 2),                   # Línea discontinua para el ajuste\n       col = c(\"black\", \"red\"),\n       bty = \"n\",                        # Sin borde en la leyenda\n       cex = 0.8)                        # Tamaño de la leyenda\n\n# Añadir cuadrícula de fondo\ngrid(col = \"gray80\", lty = \"dotted\")     # Cuadrícula con líneas punteadas grises\n\n\n\n\n\n\n\n\nLa personalización adecuada de los gráficos no solo mejora la estética, sino que también facilita la interpretación y la comunicación de los resultados, permitiendo resaltar los aspectos más relevantes del análisis (Murrell, 2018; Venables & Ripley, 2002).",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Sistema Gráfico Base de R</span>"
    ]
  },
  {
    "objectID": "09.3_visualizacion.html",
    "href": "09.3_visualizacion.html",
    "title": "14  Visualización de datos con ggplot2",
    "section": "",
    "text": "14.1 Ventajas principales de ggplot2\nEn el contexto del análisis estadístico moderno, la visualización de datos constituye una herramienta esencial para la exploración, interpretación y comunicación de resultados. Si bien el sistema gráfico base de R ofrece una amplia variedad de funciones para la creación de gráficos, la creciente demanda de visualizaciones más sofisticadas, reproducibles y estéticamente profesionales ha impulsado el desarrollo de herramientas especializadas, entre las cuales destaca el paquete ggplot2 (Wickham, 2016).\nggplot2 es un paquete de R diseñado para la creación de gráficos estadísticos de alta calidad, basado en la “gramática de los gráficos” (Grammar of Graphics) propuesta por Wilkinson (2005). Esta gramática proporciona un marco conceptual que permite construir visualizaciones complejas a partir de componentes independientes y combinables, facilitando la personalización y la integración de múltiples capas de información en un solo gráfico.\nEl uso de ggplot2 se ha consolidado como un estándar en la comunidad científica y profesional debido a varias razones fundamentales:\nEn síntesis, ggplot2 es una herramienta indispensable para quienes buscan comunicar resultados estadísticos de manera clara, precisa y profesional. Su adopción en entornos académicos y profesionales responde a la necesidad de contar con visualizaciones que no solo sean informativas, sino también estéticamente adecuadas para su inclusión en documentos formales y publicaciones científicas.\nEl paquete ggplot2 se ha consolidado como una de las herramientas más utilizadas para la visualización de datos en R, tanto en el ámbito académico como profesional. Su popularidad se debe a una serie de ventajas que lo distinguen frente a otros sistemas gráficos, especialmente en el contexto del análisis estadístico y la elaboración de documentos formales.\nEn conjunto, estas ventajas hacen de ggplot2 una herramienta indispensable para quienes buscan comunicar resultados estadísticos de manera clara, precisa y profesional, cumpliendo con los estándares de calidad exigidos en la ciencia y la industria.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Visualización de datos con ggplot2</span>"
    ]
  },
  {
    "objectID": "09.3_visualizacion.html#introducción-al-paquete-ggplot2",
    "href": "09.3_visualizacion.html#introducción-al-paquete-ggplot2",
    "title": "14  Visualización de datos con ggplot2",
    "section": "14.2 Introducción al paquete ggplot2",
    "text": "14.2 Introducción al paquete ggplot2\nggplot2 es un paquete de R que se utiliza para crear gráficos estadísticos de alta calidad de manera sencilla y flexible. Forma parte del conjunto de herramientas conocido como tidyverse, que está diseñado para facilitar el análisis y la visualización de datos. La principal característica de ggplot2 es que se basa en la “gramática de los gráficos”, una idea desarrollada por Wilkinson (2005) y adaptada por Wickham (2016), que permite construir gráficos complejos a partir de piezas simples y combinables.\nA diferencia de los gráficos base de R, donde cada tipo de gráfico tiene su propia función y la personalización puede ser complicada, ggplot2 utiliza una estructura modular. Esto significa que se puede empezar con un gráfico básico y, poco a poco, ir añadiendo o modificando elementos para adaptarlo a lo que se necesita. Así, se pueden crear gráficos claros, atractivos y personalizados para comunicar los resultados de un análisis de datos de forma efectiva (Wickham, 2016).\n\n14.2.1 Ventajas principales de ggplot2\n\nPermite crear muchos tipos de gráficos, como barras, líneas, puntos, histogramas y boxplots, entre otros.\nCada parte del gráfico se puede personalizar fácilmente: colores, títulos, etiquetas, escalas, temas y más.\nSe integra muy bien con otras herramientas del tidyverse, lo que facilita trabajar con datos y visualizarlos en un solo flujo de trabajo.\nUtiliza una lógica de “capas”, lo que significa que se pueden añadir diferentes elementos (como puntos, líneas o etiquetas) uno sobre otro, de manera ordenada y controlada.\n\n\n\n14.2.2 ¿Cómo funciona la gramática de los gráficos?\nLa gramática de los gráficos es como una receta que indica qué ingredientes debe tener un gráfico y cómo combinarlos. En ggplot2, cada gráfico se construye a partir de varios componentes básicos (Wickham, 2016; Wilkinson, 2005):\n\nDatos: Es el conjunto de información que se quiere visualizar, normalmente en forma de tabla o data frame.\nMapeos estéticos (aes): Son las instrucciones que indican cómo se relacionan las variables de los datos con los elementos visuales del gráfico, como la posición en los ejes, el color o el tamaño de los puntos.\nGeometrías (geoms): Son las formas que se usan para mostrar los datos, por ejemplo, barras para un gráfico de barras, puntos para un gráfico de dispersión, o cajas para un boxplot.\nEscalas: Permiten controlar cómo se muestran los valores en el gráfico, por ejemplo, los colores, los tamaños o los intervalos de los ejes.\nSistemas de coordenadas: Determinan el tipo de espacio en el que se dibuja el gráfico, como el sistema cartesiano (el más común) o el sistema polar (para gráficos circulares).\nFacetas: Sirven para dividir el gráfico en varios paneles, mostrando diferentes grupos de datos uno al lado del otro, lo que facilita la comparación entre categorías.\nTemas: Permiten cambiar el aspecto general del gráfico, como el fondo, los textos y las líneas de cuadrícula, para que el resultado sea más claro y profesional.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Visualización de datos con ggplot2</span>"
    ]
  },
  {
    "objectID": "09.3_visualizacion.html#estructura-básica-de-un-gráfico-en-ggplot2",
    "href": "09.3_visualizacion.html#estructura-básica-de-un-gráfico-en-ggplot2",
    "title": "14  Visualización de datos con ggplot2",
    "section": "14.3 Estructura básica de un gráfico en ggplot2",
    "text": "14.3 Estructura básica de un gráfico en ggplot2\nLa construcción de un gráfico en ggplot2 sigue una lógica de capas, donde cada componente se añade mediante el operador +. El proceso básico incluye:\n\nIniciar el objeto gráfico con la función ggplot(), especificando el conjunto de datos y los mapeos estéticos principales mediante aes(). Por ejemplo, se puede representar la EDAD en el eje X y el IMC en el eje Y.\nAñadir una o más capas geométricas, como geom_point() para puntos, geom_histogram() para histogramas, o geom_boxplot() para diagramas de caja.\nIncorporar escalas para controlar la interpretación de los valores, como escalas de color o tamaño.\nAñadir etiquetas y títulos con labs() o ggtitle(), y modificar la apariencia general del gráfico con funciones de tema como theme_minimal().\nOpcionalmente, añadir facetas para dividir el gráfico en paneles según una variable categórica, como SEXO o JORNADA.\n\nEjemplo básico:\n\nggplot(data = USAC2002, aes(x = EDAD, y = IMC)) +\n  geom_point() +\n  labs(title = \"Relación entre edad e IMC\", \n       x = \"Edad (años)\", \n       y = \"Índice de Masa Corporal (IMC)\")   \n\n\n\n\n\n\n\n\nEn este ejemplo, cada línea añade un componente al gráfico. El mapeo estético aes(x = EDAD, y = IMC) define qué variables se representan en los ejes, mientras que geom_point() indica que se utilizarán puntos para visualizar la relación.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Visualización de datos con ggplot2</span>"
    ]
  },
  {
    "objectID": "09.3_visualizacion.html#creación-de-gráficos-exploratorios-y-descriptivos",
    "href": "09.3_visualizacion.html#creación-de-gráficos-exploratorios-y-descriptivos",
    "title": "14  Visualización de datos con ggplot2",
    "section": "14.4 Creación de gráficos exploratorios y descriptivos",
    "text": "14.4 Creación de gráficos exploratorios y descriptivos\nEn el análisis de datos, la visualización inicial es clave para comprender la estructura y las características principales de las variables. ggplot2 permite construir de manera eficiente los gráficos más utilizados en la exploración y descripción de datos, facilitando la identificación de patrones, tendencias y diferencias entre grupos (Wickham, 2016).\n\n14.4.1 Gráficos de barras para variables categóricas\nEl gráfico de barras es una herramienta fundamental para mostrar la cantidad de observaciones en cada categoría de una variable cualitativa. En ggplot2, este tipo de gráfico se genera de forma automática a partir de los datos, permitiendo comparar visualmente la frecuencia de cada grupo.\n\n# Crear gráfico de barras para la variable JORNADA\n# Este gráfico muestra la distribución de estudiantes en las diferentes jornadas\n\nggplot(data = USAC2002,                    # Especificamos la base de datos\n       aes(x = JORNADA)) +                 # Variable categórica en el eje X\n  geom_bar(fill = \"orange\",                # Color de relleno de las barras\n           color = \"black\") +              # Color del borde de las barras\n  labs(title = \"Distribución de estudiantes por jornada\",  # Título del gráfico\n       x = \"Jornada\",                      # Etiqueta del eje X\n       y = \"Frecuencia\")                   # Etiqueta del eje Y\n\n\n\n\n\n\n\n\nExplicación del código:\n\nLa función ggplot() inicia la construcción del gráfico especificando los datos y el mapeo estético.\ngeom_bar() crea automáticamente las barras contando las observaciones en cada categoría.\nLos argumentos fill y color personalizan la apariencia de las barras.\nLa función labs() añade las etiquetas necesarias para la interpretación del gráfico.\n\n\n\n14.4.2 Histogramas para variables continuas\nEl histograma es el gráfico más adecuado para examinar la distribución de una variable numérica. Permite observar la forma general de los datos, la presencia de asimetrías y la existencia de valores extremos.\n\n# Crear histograma para la variable PESO_lbs\n# Este gráfico muestra la distribución del peso de los estudiantes\n\nggplot(data = USAC2002,                    # Especificamos la base de datos\n       aes(x = PESO_lbs)) +                # Variable numérica en el eje X\n  geom_histogram(bins = 15,                # Número de intervalos\n                 fill = \"lightblue\",       # Color de relleno de las barras\n                 color = \"darkblue\") +     # Color del borde de las barras\n  labs(title = \"Histograma del peso en libras\",  # Título del gráfico\n       x = \"Peso en libras\",              # Etiqueta del eje X\n       y = \"Frecuencia\")                  # Etiqueta del eje Y\n\n\n\n\n\n\n\n\nExplicación del código:\n\nEl parámetro bins determina el número de intervalos en que se dividirán los datos.\nLos colores se eligen para contrastar el relleno con el borde de las barras.\nLas etiquetas proporcionan contexto sobre la variable analizada.\n\n\n\n14.4.3 Gráficos de dispersión para relaciones entre variables numéricas\nEl gráfico de dispersión es la opción principal para explorar la relación entre dos variables cuantitativas. Cada punto representa una observación, ubicándose según sus valores en los ejes X e Y.\n\n# Crear gráfico de dispersión para TALLA vs PESO_lbs\n# Este gráfico muestra la relación entre la talla y el peso de los estudiantes\n\nggplot(data = USAC2002,                    # Especificamos la base de datos\n       aes(x = TALLA,                      # Variable numérica en el eje X\n           y = PESO_lbs)) +                # Variable numérica en el eje Y\n  geom_point(color = \"red\",                # Color de los puntos\n             size = 2) +                   # Tamaño de los puntos\n  labs(title = \"Relación entre talla y peso\",  # Título del gráfico\n       x = \"Talla (metros)\",              # Etiqueta del eje X\n       y = \"Peso (libras)\")               # Etiqueta del eje Y\n\n\n\n\n\n\n\n\nExplicación del código:\n\ngeom_point() crea un punto por cada par de valores (TALLA, PESO_lbs).\nEl argumento size controla el tamaño de los puntos para mejorar su visibilidad.\nLas etiquetas incluyen las unidades de medida para mayor claridad.\n\n\n\n14.4.4 Boxplots para comparación de grupos\nEl boxplot es un gráfico que resume la distribución de una variable numérica y facilita la comparación entre diferentes grupos definidos por una variable categórica (Wickham, 2016).\n\n# Crear boxplot para PESO_lbs por SEXO\n# Este gráfico compara la distribución del peso entre hombres y mujeres\n\nggplot(data = USAC2002,                    # Especificamos la base de datos\n       aes(x = SEXO,                       # Variable categórica en el eje X\n           y = PESO_lbs,                   # Variable numérica en el eje Y\n           fill = SEXO)) +                 # Color según el sexo\n  geom_boxplot() +                         # Crear el boxplot\n  labs(title = \"Distribución del peso por sexo\",  # Título del gráfico\n       x = \"Sexo\",                         # Etiqueta del eje X\n       y = \"Peso (libras)\")               # Etiqueta del eje Y\n\n\n\n\n\n\n\n\nExplicación del código:\n\nEl mapeo estético fill = SEXO asigna automáticamente diferentes colores a cada grupo.\ngeom_boxplot() crea las cajas que muestran la distribución de cada grupo.\nLas etiquetas ayudan a interpretar la comparación entre grupos.\n\nEstos gráficos constituyen la base del análisis exploratorio y descriptivo en R con ggplot2, permitiendo obtener una visión clara y rápida de los datos antes de aplicar técnicas estadísticas más avanzadas (Wickham, 2016).",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Visualización de datos con ggplot2</span>"
    ]
  },
  {
    "objectID": "09.3_visualizacion.html#personalización-de-gráficos-en-ggplot2",
    "href": "09.3_visualizacion.html#personalización-de-gráficos-en-ggplot2",
    "title": "14  Visualización de datos con ggplot2",
    "section": "14.5 Personalización de gráficos en ggplot2",
    "text": "14.5 Personalización de gráficos en ggplot2\nLa personalización es una de las fortalezas principales de ggplot2, permitiendo adaptar cada gráfico a los estándares de comunicación científica, a las necesidades de la audiencia y a los requisitos de publicaciones profesionales. El flujo de trabajo de personalización en ggplot2 es progresivo y modular: cada aspecto visual puede ajustarse mediante capas adicionales o argumentos específicos, lo que facilita la reproducibilidad y la claridad en la presentación de resultados (Wickham, 2016).\n\n14.5.1 Modificación de colores y escalas\nEl control de los colores es fundamental para mejorar la interpretación, la accesibilidad y la estética de los gráficos. ggplot2 permite modificar los colores de los elementos gráficos tanto de forma automática como manual, utilizando funciones de escala específicas. Para variables categóricas, se emplea scale_fill_manual() (relleno) o scale_color_manual() (bordes, líneas y puntos). Para variables continuas, existen escalas como scale_fill_gradient() o scale_color_gradient(), que permiten definir paletas de colores personalizadas o predefinidas.\n\n# Simulación de los datos\nset.seed(123)\ngrupo &lt;- factor(rep(c(\"Control\", \"Tratamiento\"), each = 100))\nvalores &lt;- c(rnorm(100, 70, 8), rnorm(100, 75, 10))\ndatos_box &lt;- data.frame(grupo = grupo, valores = valores)\n# Ejemplo de personalización de colores en un boxplot\nggplot(datos_box, aes(x = grupo, y = valores, fill = grupo)) +\n  geom_boxplot(color = \"gray30\", outlier.colour = \"red\", outlier.shape = 8) +\n  scale_fill_manual(values = c(\"skyblue\", \"salmon\")) +\n  scale_color_manual(values = c(\"gray30\", \"gray30\")) +\n  labs(title = \"Boxplot personalizado por grupo\")\n\n\n\n\n\n\n\n\nExplicación:\n\nSe crea un data frame con variables categórica y numérica.\naes(fill = grupo) mapea el color de relleno a la variable de grupo.\ngeom_boxplot() permite personalizar el color del borde y el estilo de los valores atípicos.\nscale_fill_manual() asigna colores específicos a cada grupo.\nscale_color_manual() ajusta el color del borde de las cajas.\n\nPara variables continuas, se puede utilizar una escala de gradiente:\n\n# Simulación de los datos\nset.seed(123)\nx &lt;- rnorm(100)\ny &lt;- 2 * x + rnorm(100)\nz &lt;- rnorm(100, mean = 50, sd = 10)\ndatos_disp &lt;- data.frame(x = x, y = y, z = z)\n# Ejemplo de gradiente de color en un gráfico de dispersión\nggplot(datos_disp, aes(x = x, y = y, color = z)) +\n  geom_point(size = 3) +\n  scale_color_gradient(low = \"yellow\", high = \"blue\") +\n  labs(title = \"Gradiente de color según variable continua\")\n\n\n\n\n\n\n\n\nExplicación:\n\nSe crea un data frame con dos variables numéricas y una variable adicional para el color.\naes(color = z) mapea la variable continua al color de los puntos.\nscale_color_gradient() define los colores mínimo y máximo del gradiente.\n\n\n\n14.5.2 Etiquetas, títulos y leyendas\nLa función labs() es la herramienta principal para añadir y personalizar títulos, subtítulos, etiquetas de ejes y leyendas. Una correcta rotulación es esencial para la interpretación y la comunicación efectiva de los resultados. Además, la posición y el formato de la leyenda pueden ajustarse mediante argumentos en la función theme().\n\n# Ejemplo de personalización de etiquetas y leyendas\nggplot(datos_box, aes(x = grupo, y = valores, fill = grupo)) +\n  geom_boxplot() +\n  labs(\n    title = \"Comparación de valores por grupo\",\n    subtitle = \"Datos simulados\",\n    x = \"Grupo experimental\",\n    y = \"Medición\",\n    fill = \"Condición experimental\"\n  ) +\n  theme(legend.position = \"bottom\", legend.title = element_text(face = \"bold\"))\n\n\n\n\n\n\n\n\nExplicación:\n\nlabs() define el título, subtítulo, etiquetas de ejes y el texto de la leyenda.\ntheme(legend.position = \"bottom\") coloca la leyenda debajo del gráfico.\nlegend.title = element_text(face = \"bold\") resalta el título de la leyenda.\n\n\n\n14.5.3 Aplicación y personalización de temas\nLos temas en ggplot2 controlan la apariencia global del gráfico, incluyendo el fondo, las fuentes, las líneas de cuadrícula y otros elementos estéticos. Existen temas predefinidos como theme_minimal(), theme_classic(), theme_bw(), y theme_light(), que pueden ser utilizados directamente o modificados mediante la función theme() para ajustar detalles específicos. La personalización de temas es clave para adaptar los gráficos a los estándares de publicaciones científicas y presentaciones profesionales (Tufte, 2001; Wickham, 2016).\n\n# Ejemplo de aplicación y ajuste de un tema\nggplot(datos_box, aes(x = grupo, y = valores, fill = grupo)) +\n  geom_boxplot() +\n  theme_minimal(base_size = 14) +\n  theme(\n    plot.title = element_text(face = \"bold\", color = \"navy\", size = 18),\n    plot.subtitle = element_text(face = \"italic\", color = \"gray40\"),\n    axis.title = element_text(face = \"italic\", size = 14),\n    axis.text = element_text(color = \"gray30\"),\n    panel.grid.major = element_line(color = \"gray80\"),\n    panel.grid.minor = element_blank(),\n    legend.position = \"right\"\n  )\n\n\n\n\n\n\n\n\nExplicación:\n\nSe aplica el tema theme_minimal() para un estilo limpio y profesional.\nplot.title y plot.subtitle ajustan el estilo y color de los títulos.\naxis.title y axis.text modifican el estilo y tamaño de los textos de ejes.\npanel.grid.major y panel.grid.minor controlan la visibilidad y color de las líneas de cuadrícula.\nlegend.position define la ubicación de la leyenda.\n\n\n\n14.5.4 Personalización avanzada: fuentes, márgenes y elementos gráficos\nggplot2 permite un control detallado sobre elementos como el tipo y tamaño de fuente, los márgenes del gráfico, la orientación de las etiquetas y la visibilidad de los ejes. Estas opciones avanzadas se gestionan principalmente a través de la función theme(), lo que permite adaptar el gráfico a los requisitos específicos de cada publicación o presentación.\n\n# Ejemplo de personalización avanzada\nggplot(datos_box, aes(x = grupo, y = valores, fill = grupo)) +\n  geom_boxplot() +\n  theme_classic(base_size = 13) +\n  theme(\n    plot.margin = margin(20, 20, 20, 20),\n    axis.text.x = element_text(angle = 45, hjust = 1),\n    axis.line = element_line(color = \"black\", linewidth = 1),\n    legend.background = element_rect(fill = \"gray95\", color = \"gray80\")\n  )\n\n\n\n\n\n\n\n\nExplicación:\n\nplot.margin ajusta los márgenes del gráfico.\naxis.text.x rota las etiquetas del eje X para mejorar la legibilidad.\naxis.line resalta los ejes con líneas más gruesas y visibles.\nlegend.background modifica el fondo de la leyenda.\n\nLa personalización progresiva y modular de los gráficos en ggplot2 permite adaptar cada visualización a los estándares de comunicación científica y a las necesidades específicas de cada proyecto, garantizando resultados reproducibles y de alta calidad (Wickham, 2016).",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Visualización de datos con ggplot2</span>"
    ]
  },
  {
    "objectID": "09.3_visualizacion.html#uso-de-facetas-para-comparación-de-grupos",
    "href": "09.3_visualizacion.html#uso-de-facetas-para-comparación-de-grupos",
    "title": "14  Visualización de datos con ggplot2",
    "section": "14.6 Uso de facetas para comparación de grupos",
    "text": "14.6 Uso de facetas para comparación de grupos\nLas facetas en ggplot2 son una herramienta poderosa para dividir un gráfico en varios subgráficos, cada uno correspondiente a un grupo definido por una o más variables categóricas. Esto facilita la comparación visual entre diferentes segmentos de los datos, permitiendo identificar patrones, similitudes o diferencias que podrían pasar desapercibidas en un solo gráfico global (Wickham, 2016).\n\n14.6.1 Facet_wrap y facet_grid: sintaxis y aplicaciones\nggplot2 ofrece dos funciones principales para crear facetas: facet_wrap() y facet_grid(). Cada una tiene una lógica y utilidad específica.\n\n# Ejemplo 1: Uso de facet_wrap para comparar por facultad\n# Este gráfico muestra la relación entre talla y peso, generando un subgráfico para cada facultad\n\nggplot(data = USAC2002, aes(x = TALLA, y = PESO_lbs)) +  # Variables numéricas en los ejes\n  geom_point(color = \"darkgreen\", alpha = 0.6) +         # Puntos verdes con transparencia\n  facet_wrap(~ FACULTAD) +                               # Un panel por cada facultad\n  labs(\n    title = \"Relación entre talla y peso por facultad\",   # Título principal\n    x = \"Talla (metros)\",                                # Etiqueta eje X\n    y = \"Peso (libras)\"                                  # Etiqueta eje Y\n  ) +\n  theme_minimal()                                        # Tema limpio y profesional\n\n\n\n\n\n\n\n\n\nfacet_wrap(~ FACULTAD) divide el gráfico en tantos paneles como valores únicos tenga la variable FACULTAD, permitiendo comparar la relación entre talla y peso en cada facultad de manera individual.\nEs útil cuando se desea comparar un solo criterio de agrupación y se prefiere que los paneles se organicen en una cuadrícula flexible.\n\n\n# Ejemplo 2: Uso de facet_grid para comparar por sexo y jornada\n# Este gráfico muestra la relación entre talla y peso, generando una matriz de subgráficos según sexo y jornada\n\nggplot(data = USAC2002, aes(x = TALLA, y = PESO_lbs)) +  # Variables numéricas en los ejes\n  geom_point(color = \"purple\", alpha = 0.5) +             # Puntos morados con transparencia\n  facet_grid(SEXO ~ JORNADA) +                            # Filas por SEXO, columnas por JORNADA\n  labs(\n    title = \"Relación entre talla y peso por sexo y jornada\",  # Título principal\n    x = \"Talla (metros)\",                                    # Etiqueta eje X\n    y = \"Peso (libras)\"                                      # Etiqueta eje Y\n  ) +\n  theme_minimal()                                            # Tema limpio y profesional\n\n\n\n\n\n\n\n\n\nfacet_grid(SEXO ~ JORNADA) crea una matriz de paneles, donde las filas corresponden a los niveles de SEXO y las columnas a los niveles de JORNADA.\nEs especialmente útil para comparar dos criterios de agrupación de manera cruzada y ordenada.\n\n\n\n14.6.2 Cuadro resumen de diferencias de las funciones face_wrap y face_grid\n\n\n\n\n\n\n\n\n\n\nFunción\nUso principal\nOrganización de paneles\nVariables categóricas involucradas\nEjemplo de sintaxis\n\n\n\n\nfacet_wrap()\nComparar grupos definidos por una sola variable\nCuadrícula flexible (ajuste automático)\nUna variable categórica\nfacet_wrap(~ FACULTAD)\n\n\nfacet_grid()\nComparar grupos definidos por dos variables\nMatriz (filas y columnas fijas)\nDos variables categóricas (filas y columnas)\nfacet_grid(SEXO ~ JORNADA)",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Visualización de datos con ggplot2</span>"
    ]
  },
  {
    "objectID": "09.3_visualizacion.html#comparación-entre-ggplot2-y-el-sistema-gráfico-base-de-r",
    "href": "09.3_visualizacion.html#comparación-entre-ggplot2-y-el-sistema-gráfico-base-de-r",
    "title": "14  Visualización de datos con ggplot2",
    "section": "14.7 Comparación entre ggplot2 y el sistema gráfico base de R",
    "text": "14.7 Comparación entre ggplot2 y el sistema gráfico base de R\nLa comparación entre ggplot2 y el sistema gráfico base de R es fundamental para quienes inician en la visualización de datos, ya que permite comprender las ventajas y limitaciones de cada enfoque. Para garantizar una comparación justa, se utilizará el conjunto de datos iris, uno de los más clásicos y ampliamente utilizados en la literatura estadística y en la enseñanza de R. Este conjunto contiene mediciones de longitud y ancho de sépalos y pétalos de tres especies de flores, y es ideal para ilustrar gráficos de dispersión con agrupamiento por especie (Anderson, 1935).\n\n14.7.1 Sintaxis y filosofía\n\nggplot2 se basa en la gramática de los gráficos, permitiendo construir visualizaciones complejas mediante la combinación modular de componentes. Su sintaxis es declarativa, lo que facilita la especificación de qué se desea visualizar.\nEl sistema gráfico base utiliza una sintaxis imperativa, donde cada paso debe ser definido explícitamente. Es más directo para gráficos simples, pero menos flexible para personalizaciones avanzadas.\n\nEjemplo comparativo:\n\n# 1. ggplot2\nlibrary(ggplot2)\nggplot(iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +\n  geom_point(size = 2) +\n  labs(\n    title = \"Relación entre longitud y ancho del sépalo\",\n    x = \"Longitud del sépalo (cm)\",\n    y = \"Ancho del sépalo (cm)\",\n    color = \"Especie\"\n  ) +\n  theme_minimal(base_size = 13)\n\n\n\n\n\n\n\n# 2. Sistema gráfico base\nplot(iris$Sepal.Length, iris$Sepal.Width,\n     col = as.numeric(iris$Species),\n     pch = 16,\n     main = \"Relación entre longitud y ancho del sépalo\",\n     xlab = \"Longitud del sépalo (cm)\",\n     ylab = \"Ancho del sépalo (cm)\")\nlegend(\"topright\",\n       legend = levels(iris$Species),\n       col = 1:3,\n       pch = 16,\n       title = \"Especie\")\n\n\n\n\n\n\n\n\nAmbos gráficos utilizan el mismo conjunto de datos y presentan título, etiquetas de ejes y leyenda, lo que permite una comparación equitativa de la sintaxis y el resultado visual.\n\n\n14.7.2 Flexibilidad y personalización\n\nggplot2 permite personalizar cada elemento del gráfico de manera eficiente, utilizando capas y funciones específicas para colores, escalas, temas y leyendas.\nEl sistema gráfico base requiere argumentos adicionales y, en ocasiones, funciones externas para lograr el mismo nivel de personalización, lo que puede dificultar la reproducibilidad y la claridad del código.\n\nEjemplo:\n\n# 1. ggplot2\nggplot(iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +\n  geom_point(size = 2) +\n  scale_color_manual(values = c(\"red\", \"green\", \"blue\")) +\n  labs(\n    title = \"Relación entre longitud y ancho del sépalo\",\n    subtitle = \"Datos del conjunto iris\",\n    x = \"Longitud del sépalo (cm)\",\n    y = \"Ancho del sépalo (cm)\",\n    color = \"Especie\"\n  ) +\n  theme_minimal(base_size = 13)\n\n\n\n\n\n\n\n# 2. Sistema gráfico base\nplot(iris$Sepal.Length, iris$Sepal.Width,\n     col = c(\"red\", \"green\", \"blue\")[as.numeric(iris$Species)],\n     pch = 16,\n     main = \"Relación entre longitud y ancho del sépalo\",\n     xlab = \"Longitud del sépalo (cm)\",\n     ylab = \"Ancho del sépalo (cm)\")\nlegend(\"topright\",\n       legend = levels(iris$Species),\n       col = c(\"red\", \"green\", \"blue\"),\n       pch = 16,\n       title = \"Especie\")\nmtext(\"Datos del conjunto iris\", side = 3, line = 0.5, cex = 0.9, col = \"gray40\")\n\n\n\n\n\n\n\n\nAmbos gráficos incluyen personalización de colores, subtítulo, título, etiquetas de ejes y leyenda, asegurando una comparación justa.\n\n\n14.7.3 Resumen comparativo\nA continuación se presenta un cuadro comparativo que integra los aspectos más relevantes discutidos, facilitando la consulta rápida y la toma de decisiones informada:\n\n\n\n\n\n\n\n\nCaracterística\nggplot2\nSistema gráfico base\n\n\n\n\nSintaxis\nDeclarativa, modular, basada en la gramática de los gráficos\nImperativa, secuencial\n\n\nFlexibilidad\nAlta, personalización eficiente y detallada\nLimitada, requiere mayor esfuerzo\n\n\nEstética\nModerna y profesional por defecto, fácil de mantener entre gráficos\nTradicional, requiere ajustes manuales\n\n\nFlujo de trabajo\nModular, reproducible y documentado; facilita colaboración y revisión\nMenos modular, menos reproducible\n\n\nReutilización\nAlta, gracias a la estructura por capas y la posibilidad de guardar objetos gráficos\nBaja, requiere repetir comandos y ajustes\n\n\nCurva de aprendizaje\nInicialmente más pronunciada, pero ventajosa en proyectos complejos\nMás sencilla para gráficos básicos\n\n\nIntegración\nExcelente con el ecosistema tidyverse y flujos de trabajo modernos\nIntegración limitada con otros paquetes\n\n\nComunidad y recursos\nAmplia documentación, comunidad activa y abundantes ejemplos\nDocumentación tradicional, menos recursos\n\n\n\nEn conclusión, ggplot2 es la opción preferente para la elaboración de gráficos complejos, personalizados y de alta calidad, especialmente en contextos académicos y científicos donde la reproducibilidad, la estética y la flexibilidad son prioritarias. El sistema gráfico base de R, por su parte, sigue siendo útil para la creación rápida de gráficos exploratorios y para usuarios que requieren soluciones sencillas y directas, o que priorizan la inmediatez sobre la personalización (Murrell, 2018; Wickham, 2016).",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Visualización de datos con ggplot2</span>"
    ]
  },
  {
    "objectID": "10.1_exportacion.html",
    "href": "10.1_exportacion.html",
    "title": "15  Introducción a la Gestión de Proyectos en R",
    "section": "",
    "text": "15.1 Importancia de la gestión de proyectos en análisis estadístico\nLa gestión de proyectos en R es esencial para mantener el orden, la claridad y la eficiencia en el análisis estadístico de datos, incluso en proyectos sencillos. Adoptar buenas prácticas desde el inicio permite evitar errores, facilita la revisión del trabajo y mejora la comunicación de los resultados, tanto para el propio usuario como para otros que puedan consultar el proyecto en el futuro (Wickham & Grolemund, 2017).\nEn el análisis estadístico, la gestión de proyectos consiste en organizar todos los elementos necesarios para el trabajo en un solo lugar. Esto incluye los datos, los scripts de análisis, los resultados exportados y cualquier archivo adicional relevante. Mantener todos estos archivos juntos en una carpeta específica para cada proyecto ayuda a evitar confusiones, pérdidas de información y errores al ejecutar los análisis.\nLa gestión adecuada de proyectos permite:\nEn proyectos simples, donde el análisis se limita a un solo conjunto de datos y un flujo de trabajo lineal, una carpeta por proyecto es suficiente para mantener el orden y la trazabilidad del trabajo (Wickham & Grolemund, 2017).",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introducción a la Gestión de Proyectos en R</span>"
    ]
  },
  {
    "objectID": "10.1_exportacion.html#importancia-de-la-gestión-de-proyectos-en-análisis-estadístico",
    "href": "10.1_exportacion.html#importancia-de-la-gestión-de-proyectos-en-análisis-estadístico",
    "title": "15  Introducción a la Gestión de Proyectos en R",
    "section": "",
    "text": "Retomar el trabajo en cualquier momento sin perder el contexto.\nCompartir el proyecto con otras personas de manera sencilla.\nGarantizar que los resultados obtenidos sean reproducibles y verificables.\nEvitar la mezcla de archivos de diferentes análisis, lo que puede llevar a errores o a la utilización de datos incorrectos.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introducción a la Gestión de Proyectos en R</span>"
    ]
  },
  {
    "objectID": "10.1_exportacion.html#organización-de-archivos-en-proyectos-de-r",
    "href": "10.1_exportacion.html#organización-de-archivos-en-proyectos-de-r",
    "title": "15  Introducción a la Gestión de Proyectos en R",
    "section": "15.2 Organización de archivos en proyectos de R",
    "text": "15.2 Organización de archivos en proyectos de R\nPara proyectos de análisis estadístico simples, se recomienda crear una carpeta exclusiva para cada proyecto. Dentro de esta carpeta deben almacenarse todos los archivos relacionados con el análisis, lo que incluye:\n\nEl archivo de datos (por ejemplo, un archivo CSV o Excel).\nEl archivo del proyecto de RStudio (con extensión .Rproj), que facilita la gestión y el acceso al proyecto.\nEl script de análisis en R (por ejemplo, analisis.R), donde se escribe y ejecuta el código.\nLos resultados exportados, como gráficos en formato PNG o PDF y tablas en formato CSV o Excel.\n\nEsta organización básica permite que, al abrir la carpeta del proyecto, se tenga acceso inmediato a todos los elementos necesarios para reproducir el análisis o continuar trabajando. Además, facilita la identificación de los archivos y su propósito, evitando la dispersión de información.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introducción a la Gestión de Proyectos en R</span>"
    ]
  },
  {
    "objectID": "10.1_exportacion.html#uso-de-rstudio-projects-para-la-gestión-eficiente",
    "href": "10.1_exportacion.html#uso-de-rstudio-projects-para-la-gestión-eficiente",
    "title": "15  Introducción a la Gestión de Proyectos en R",
    "section": "15.3 Uso de RStudio Projects para la gestión eficiente",
    "text": "15.3 Uso de RStudio Projects para la gestión eficiente\nRStudio Projects es una herramienta integrada en el entorno RStudio que permite gestionar proyectos de manera eficiente, incluso en análisis simples. Al crear un proyecto en RStudio, se genera un archivo con extensión .Rproj dentro de la carpeta del proyecto. Este archivo define el directorio de trabajo y centraliza todos los archivos relacionados.\nLas ventajas de utilizar RStudio Projects en proyectos simples incluyen:\n\nFacilita la apertura y cierre del proyecto, restaurando el entorno de trabajo tal como se dejó la última vez.\nAsegura que el directorio de trabajo sea siempre el correcto, evitando errores al cargar o guardar archivos.\nPermite mantener separados los análisis de diferentes proyectos, lo que reduce el riesgo de mezclar datos o resultados.\n\nPara crear un proyecto en RStudio, se debe seleccionar la opción “File &gt; New Project”, elegir “New Directory” y luego “New Project”. Se asigna un nombre y una ubicación a la carpeta del proyecto, y RStudio creará automáticamente el archivo .Rproj en esa carpeta. A partir de ese momento, todos los archivos del análisis deben guardarse en esa misma carpeta para mantener la organización y la reproducibilidad (Wickham & Grolemund, 2017).",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introducción a la Gestión de Proyectos en R</span>"
    ]
  },
  {
    "objectID": "10.1_exportacion.html#principios-de-reproducibilidad-y-documentación-en-proyectos-de-r",
    "href": "10.1_exportacion.html#principios-de-reproducibilidad-y-documentación-en-proyectos-de-r",
    "title": "15  Introducción a la Gestión de Proyectos en R",
    "section": "15.4 Principios de reproducibilidad y documentación en proyectos de R",
    "text": "15.4 Principios de reproducibilidad y documentación en proyectos de R\nLa reproducibilidad es un principio fundamental en el análisis estadístico. Consiste en la capacidad de repetir un análisis y obtener los mismos resultados, utilizando los mismos datos y scripts. Para lograrlo, es esencial mantener todos los archivos del proyecto juntos y documentar adecuadamente cada paso del proceso.\nEn proyectos simples, la reproducibilidad se puede asegurar mediante:\n\nEl uso de scripts bien comentados, donde se explique cada parte del análisis.\nLa inclusión de los datos originales en la carpeta del proyecto.\nLa exportación de los resultados (gráficos y tablas) en formatos accesibles y guardados en la misma carpeta.\nEl uso del archivo .Rproj para centralizar el entorno de trabajo.\n\nAdemás, es recomendable agregar comentarios en el script de análisis que expliquen el propósito de cada sección del código, los pasos seguidos y cualquier decisión relevante tomada durante el análisis. Esta documentación facilita la revisión, el aprendizaje y la colaboración, incluso en proyectos individuales (Wickham & Grolemund, 2017).",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Introducción a la Gestión de Proyectos en R</span>"
    ]
  },
  {
    "objectID": "10.2_exportacion.html",
    "href": "10.2_exportacion.html",
    "title": "16  Exportación de Resultados de Análisis en R",
    "section": "",
    "text": "16.1 Exportación de gráficos: formatos PNG y PDF\nLa exportación de resultados constituye una etapa fundamental en el análisis estadístico de datos, ya que permite almacenar y compartir los productos del análisis, como gráficos y tablas, para su posterior utilización en informes, presentaciones o análisis adicionales. La correcta elección del formato de exportación garantiza la accesibilidad, reutilización y compatibilidad de los resultados con otras herramientas y plataformas (R Core Team, 2023; Wickham, 2016).\nEn el contexto del análisis estadístico clásico, la exportación de resultados facilita la comunicación de hallazgos y la integración de los mismos en documentos científicos, reportes técnicos o presentaciones. R ofrece funciones específicas para exportar tanto gráficos como tablas de datos en los formatos más utilizados en la práctica profesional y académica, asegurando la calidad y la fidelidad de la información exportada (R Core Team, 2023).\nLa exportación de gráficos es fundamental para documentar visualmente los resultados del análisis. En R, la función ggsave() del paquete ggplot2 permite guardar gráficos en diversos formatos, siendo PNG y PDF los más empleados en la estadística clásica.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Exportación de Resultados de Análisis en R</span>"
    ]
  },
  {
    "objectID": "10.2_exportacion.html#exportación-de-gráficos-formatos-png-y-pdf",
    "href": "10.2_exportacion.html#exportación-de-gráficos-formatos-png-y-pdf",
    "title": "16  Exportación de Resultados de Análisis en R",
    "section": "",
    "text": "16.1.1 Sintaxis general de ggsave()\nLa función ggsave() del paquete ggplot2 permite guardar gráficos en diferentes formatos. Su sintaxis básica es:\n\nggsave(\n  filename,\n  plot = last_plot(),\n  device = NULL,\n  path = NULL,\n  scale = 1,\n  width = NA,\n  height = NA,\n  units = c(\"in\", \"cm\", \"mm\"),\n  dpi = 300,\n  limitsize = TRUE\n)\n\nA continuación, se describen los argumentos principales de la función:\n\nfilename: Es el nombre del archivo de salida, incluyendo la extensión (por ejemplo, \"grafico.png\" o \"grafico.pdf\"). La extensión determina el formato del archivo.\nplot: Permite especificar el objeto gráfico que se desea guardar. Si se omite, se guarda el último gráfico creado en la sesión de R.\ndevice: Indica el tipo de formato del archivo, como \"png\" o \"pdf\". Si no se especifica, el formato se deduce automáticamente a partir de la extensión del archivo.\npath: Define el directorio donde se guardará el archivo. Si no se proporciona, el archivo se guarda en el directorio de trabajo actual.\nscale: Ajusta el tamaño del gráfico multiplicando las dimensiones especificadas en width y height por el valor de scale. El valor predeterminado es 1 (tamaño original).\nwidth y height: Determinan el ancho y la altura del gráfico en las unidades especificadas por units. Si no se definen, se usan las dimensiones predeterminadas.\nunits: Especifica las unidades de medida para width y height. Puede ser “in” (pulgadas), “cm” (centímetros) o “mm” (milímetros).\ndpi: Define la resolución del gráfico en puntos por pulgada, relevante para formatos rasterizados como PNG. El valor predeterminado es 300, adecuado para impresión.\nlimitsize: Controla si se permite guardar gráficos con dimensiones muy grandes (mayores a 50 pulgadas). Si está en TRUE, se genera un error al intentar guardar gráficos excesivamente grandes.\n\nEsta explicación permite comprender tanto la estructura general de la función como el propósito de cada argumento, facilitando su uso correcto en la exportación de gráficos en R (Wickham, 2016).\n\n\n16.1.2 Ejemplo práctico: creación y exportación de un gráfico\nSupóngase que se ha creado un gráfico de barras con ggplot2:\n\n# Cargar el paquete tidyverse, que incluye ggplot2\nif (!require(\"tidyverse\")) install.packages(\"tidyverse\")\nlibrary(tidyverse)\n\n# Importar una base de datos de ejemplo\ndatos &lt;- read_csv(\"datos_estudiantes.csv\")\n\n# Crear un gráfico de barras\nmi_grafico &lt;- ggplot(data = datos, aes(x = FACULTAD)) +\n  geom_bar(fill = \"steelblue\", color = \"black\", alpha = 0.8) +\n  labs(\n    title = \"Distribución de estudiantes por facultad\",\n    subtitle = \"Datos del estudio de 2002, USAC\",\n    x = \"Facultad\",\n    y = \"Cantidad de estudiantes\",\n    caption = \"Fuente: Estudio realizado en 2002\"\n  ) +\n  theme_minimal()+                  \n  theme(\n    axis.text.x = element_text( \n      angle = 45,                       \n      hjust = 1                       \n    )\n  )\nmi_grafico\n\n\n\n\n\n\n\n\nGuardar el gráfico en formato PNG\n\n# Guardar el gráfico en formato PNG con dimensiones de 8x6 pulgadas\nggsave(\n  filename = \"grafico.png\", # Nombre del archivo de salida\n  plot = mi_grafico,        # Objeto gráfico a guardar\n  width = 8,                # Ancho en pulgadas\n  height = 6,               # Alto en pulgadas\n  dpi = 300                 # Resolución adecuada para impresión\n)\n\nEn este ejemplo, el archivo “grafico.png” se guardará en el directorio de trabajo actual, con alta calidad para impresión o presentaciones digitales.\nGuardar el gráfico en formato PDF\n\n# Guardar el gráfico en formato PDF con dimensiones de 8x6 pulgadas\nggsave(\n  filename = \"grafico.pdf\", # Nombre del archivo de salida\n  plot = mi_grafico,        # Objeto gráfico a guardar\n  width = 8,                # Ancho en pulgadas\n  height = 6                # Alto en pulgadas\n  # No es necesario especificar dpi, ya que PDF es un formato vectorial\n)\n\nEl formato PDF es ideal para informes y publicaciones científicas, ya que permite escalar el gráfico sin pérdida de calidad (Wickham, 2016).",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Exportación de Resultados de Análisis en R</span>"
    ]
  },
  {
    "objectID": "10.2_exportacion.html#exportación-de-tablas-de-datos-formatos-csv-y-excel",
    "href": "10.2_exportacion.html#exportación-de-tablas-de-datos-formatos-csv-y-excel",
    "title": "16  Exportación de Resultados de Análisis en R",
    "section": "16.2 Exportación de tablas de datos: formatos CSV y Excel",
    "text": "16.2 Exportación de tablas de datos: formatos CSV y Excel\nLa exportación de tablas de datos es fundamental para compartir información, documentar resultados o realizar análisis adicionales en otras herramientas. Los formatos más utilizados en la estadística clásica son CSV y Excel, por su compatibilidad y facilidad de uso.\n\n16.2.1 Exportar a CSV con write.csv()\nLa función write.csv() permite exportar un data frame o matriz a un archivo de texto plano en formato CSV (Comma Separated Values). Este formato es ampliamente compatible con programas de hojas de cálculo y software estadístico.\nSintaxis general de write.csv():\n\nwrite.csv(\n  x,           \n  file,        \n  row.names = TRUE,   \n  na = \"NA\",          \n  fileEncoding = \"\",  \n)\n\nExplicación de los argumentos principales:\n\nx: Es el objeto de datos que se desea exportar, generalmente un data frame o una matriz.\nfile: Especifica el nombre del archivo de salida, incluyendo la extensión .csv. El archivo se guardará en el directorio de trabajo actual, a menos que se indique una ruta diferente.\nrow.names: Indica si se deben incluir los nombres de las filas como una columna adicional en el archivo exportado. El valor predeterminado es TRUE, pero es común establecerlo en FALSE para evitar agregar una columna innecesaria.\nna: Define la cadena de texto que se utilizará para representar los valores faltantes (NA) en el archivo exportado. El valor predeterminado es \"NA\".\nfileEncoding: Permite especificar la codificación del archivo de salida, útil para asegurar la compatibilidad con otros sistemas operativos o programas. El valor predeterminado es una cadena vacía, lo que significa que se utiliza la codificación por defecto del sistema.\n\nEjemplo:\n\n# Crear un data frame de ejemplo\nmi_tabla &lt;- data.frame(\n  Nombre = c(\"Ana\", \"Luis\", \"María\"),\n  Edad = c(25, 30, 22),\n  Ciudad = c(\"Madrid\", \"Barcelona\", \"Valencia\")\n)\n\n# Exportar el data frame a un archivo CSV\nwrite.csv(\n  x = mi_tabla,         # Objeto de datos a exportar\n  file = \"resultados.csv\", # Nombre del archivo de salida\n  row.names = FALSE     # No incluir los nombres de las filas \n)\n\nEl archivo “resultados.csv” se guardará en el directorio de trabajo actual y podrá ser abierto en cualquier editor de texto o programa de hojas de cálculo (R Core Team, 2023).\n\n\n16.2.2 Exportar a Excel con write_xlsx() del paquete writexl\nLa función write_xlsx() del paquete writexl permite exportar un data frame o una lista de data frames a un archivo en formato Excel (.xlsx). Este formato es ideal para compartir datos estructurados y aprovechar las funcionalidades avanzadas de hojas de cálculo.\nSintaxis general de write_xlsx()\n\nwrite_xlsx(\n  x,           # Objeto de datos a exportar \n  path,        # Nombre del archivo de salida \n  col_names = TRUE, \n  format_headers = TRUE \n)\n\nExplicación de los argumentos principales:\n\nx: Es el objeto de datos a exportar, que puede ser un data frame o una lista de data frames (en este caso, cada data frame se guardará en una hoja diferente del archivo Excel).\npath: Especifica el nombre del archivo de salida, incluyendo la extensión .xlsx. El archivo se guardará en el directorio de trabajo actual, a menos que se indique una ruta diferente.\ncol_names: Indica si se deben incluir los nombres de las columnas en la primera fila del archivo. El valor predeterminado es TRUE.\nformat_headers: Determina si los encabezados de las columnas deben tener un formato especial (por ejemplo, negrita). El valor predeterminado es TRUE.\n\nEjemplo:\n\n# Instalar y cargar el paquete writexl si no está disponible\nif (!require(\"writexl\")) install.packages(\"writexl\")\n\n# Exportar el data frame a un archivo Excel\nwrite_xlsx(\n  x = mi_tabla,            # Objeto de datos a exportar\n  path = \"resultados.xlsx\" # Nombre del archivo de salida\n  # col_names y format_headers se mantienen en TRUE por defecto\n)\n\nEl archivo “resultados.xlsx” se podrá abrir en Microsoft Excel o software compatible, permitiendo aprovechar las funcionalidades avanzadas de hojas de cálculo (R Core Team, 2023).",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Exportación de Resultados de Análisis en R</span>"
    ]
  },
  {
    "objectID": "10.2_exportacion.html#comparación-de-formatos-y-recomendaciones-de-uso",
    "href": "10.2_exportacion.html#comparación-de-formatos-y-recomendaciones-de-uso",
    "title": "16  Exportación de Resultados de Análisis en R",
    "section": "16.3 Comparación de formatos y recomendaciones de uso",
    "text": "16.3 Comparación de formatos y recomendaciones de uso\nLa elección del formato de exportación depende del objetivo y del público destinatario. A continuación se presenta un cuadro comparativo que distingue entre formatos de imágenes y de datos, resumiendo sus principales características, ventajas y desventajas (Wickham, 2016; R Core Team, 2023):\n\n\n\n\n\n\n\n\n\n\nTipo\nFormato\nUso principal\nVentajas\nDesventajas\n\n\n\n\nImagen\nPNG\nPresentaciones y documentos digitales\nAlta calidad, ampliamente compatible\nNo escalable sin pérdida de calidad\n\n\nImagen\nPDF\nPublicaciones científicas e informes impresos\nEscalable, ideal para impresión\nMenos compatible con editores básicos\n\n\nDatos\nCSV\nAnálisis de datos en herramientas simples\nLigero, multiplataforma, fácil de manipular\nNo admite formatos complejos (fórmulas, etc)\n\n\nDatos\nExcel\nCompartir datos estructurados y análisis\nCompatible con herramientas avanzadas\nRequiere software específico",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Exportación de Resultados de Análisis en R</span>"
    ]
  },
  {
    "objectID": "10.3_exportacion.html",
    "href": "10.3_exportacion.html",
    "title": "17  Uso de Git y GitHub en Proyectos de R",
    "section": "",
    "text": "17.1 Introducción al control de versiones y colaboración\nEl control de versiones y la colaboración en línea son prácticas cada vez más importantes en el análisis estadístico y la ciencia de datos. Git y GitHub permiten gestionar de manera eficiente los cambios en los archivos de un proyecto, compartir el trabajo con otros y mantener un historial completo de todas las modificaciones realizadas. Aunque estas herramientas pueden parecer complejas al principio, su integración con RStudio y su utilidad en proyectos de cualquier tamaño justifican su aprendizaje y uso desde etapas tempranas (Bryan, 2018).\nEl control de versiones es una metodología que permite registrar, organizar y recuperar los cambios realizados en los archivos de un proyecto a lo largo del tiempo. Git es el sistema de control de versiones más utilizado y se integra fácilmente con RStudio, lo que facilita su adopción en proyectos de análisis estadístico.\nVentajas del control de versiones con Git:\nGitHub es una plataforma en línea que permite alojar repositorios de Git, compartir proyectos y colaborar con otros usuarios. Además, ofrece herramientas para la gestión de proyectos, seguimiento de problemas (issues), revisión de código y documentación.\nEn el contexto de proyectos de R, Git y GitHub permiten mantener un registro ordenado de los scripts, datos y resultados, facilitando la colaboración y la reproducibilidad del análisis (Bryan, 2018).",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Uso de Git y GitHub en Proyectos de R</span>"
    ]
  },
  {
    "objectID": "10.3_exportacion.html#introducción-al-control-de-versiones-y-colaboración",
    "href": "10.3_exportacion.html#introducción-al-control-de-versiones-y-colaboración",
    "title": "17  Uso de Git y GitHub en Proyectos de R",
    "section": "",
    "text": "Permite guardar el historial de cambios, facilitando la recuperación de versiones anteriores de los archivos.\nAyuda a identificar cuándo, cómo y por qué se realizaron modificaciones, lo que mejora la trazabilidad y la transparencia.\nFacilita la colaboración entre varios usuarios, permitiendo que cada uno trabaje en su propia copia del proyecto y luego integre los cambios.\nReduce el riesgo de pérdida de información, ya que los archivos pueden ser restaurados a cualquier estado anterior.\nPermite experimentar con nuevas ideas sin temor a perder el trabajo anterior, gracias a la posibilidad de crear ramas (branches) y fusionarlas posteriormente.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Uso de Git y GitHub en Proyectos de R</span>"
    ]
  },
  {
    "objectID": "10.3_exportacion.html#subida-de-un-proyecto-de-r-a-github",
    "href": "10.3_exportacion.html#subida-de-un-proyecto-de-r-a-github",
    "title": "17  Uso de Git y GitHub en Proyectos de R",
    "section": "17.2 Subida de un proyecto de R a GitHub",
    "text": "17.2 Subida de un proyecto de R a GitHub\nSubir un proyecto de R a GitHub implica crear un repositorio en la plataforma y sincronizarlo con la carpeta local del proyecto. Este proceso puede realizarse desde la interfaz de RStudio o utilizando la línea de comandos. A continuación se describe el proceso paso a paso para un usuario principiante:\n1. Crear una cuenta en GitHub\nPara comenzar, es necesario registrarse en https://github.com/ y crear una cuenta personal.\n2. Crear un repositorio nuevo en GitHub\nUna vez dentro de la cuenta, se debe hacer clic en el botón “New repository”. Se recomienda asignar un nombre descriptivo al repositorio (por ejemplo, “analisis_estadistico”) y, opcionalmente, agregar una breve descripción. Es posible elegir si el repositorio será público (visible para todos) o privado (solo accesible para el usuario y quienes él autorice). Al crear el repositorio, se puede dejar vacío, ya que los archivos se agregarán desde la computadora local.\n3. Inicializar Git en la carpeta del proyecto local\nEn la computadora, se debe ubicar la carpeta del proyecto de R (la que contiene el archivo .Rproj, los datos, el script y los resultados exportados).\n\nSi se utiliza RStudio, se puede activar el control de versiones seleccionando “Tools &gt; Project Options &gt; Git/SVN” y eligiendo Git.\nSi se prefiere la terminal, se debe abrir una consola en la carpeta del proyecto y ejecutar el comando:\n\n\ngit init\n\nEsto crea una carpeta oculta llamada .git que permitirá a Git rastrear los cambios en los archivos del proyecto.\n4. Conectar el repositorio local con el remoto en GitHub\n\nPara vincular la carpeta local con el repositorio creado en GitHub, se debe copiar la URL del repositorio (por ejemplo, https://github.com/usuario/analisis_estadistico.git) y ejecutar el siguiente comando en la terminal:\n\ngit remote add origin https://github.com/usuario/analisis_estadistico.git\n\n5. Agregar y confirmar los archivos del proyecto\n\nSe deben agregar los archivos del proyecto al control de versiones con el comando:\n\ngit add .\n\nEl punto (.) indica que se agregarán todos los archivos de la carpeta.\n\nLuego, se realiza el primer “commit” (registro de cambios) con un mensaje descriptivo:\n\ngit commit -m \"Primer commit: subida inicial del proyecto\"\n\n6. Subir los archivos a GitHub\n\nFinalmente, se suben los archivos al repositorio remoto con el comando:\n\ngit push -u origin master\n\nEn algunos casos, la rama principal puede llamarse “main” en lugar de “master”, por lo que el comando sería:\n\ngit push -u origin main\n\nUna vez completados estos pasos, el proyecto estará disponible en GitHub, permitiendo su consulta, descarga y colaboración. Desde la interfaz web de GitHub, se pueden visualizar los archivos, el historial de cambios y la documentación del proyecto (Bryan, 2018).",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Uso de Git y GitHub en Proyectos de R</span>"
    ]
  },
  {
    "objectID": "10.3_exportacion.html#modificación-y-seguimiento-de-proyectos-en-github",
    "href": "10.3_exportacion.html#modificación-y-seguimiento-de-proyectos-en-github",
    "title": "17  Uso de Git y GitHub en Proyectos de R",
    "section": "17.3 Modificación y seguimiento de proyectos en GitHub",
    "text": "17.3 Modificación y seguimiento de proyectos en GitHub\nUna vez que el proyecto está en GitHub, es posible continuar trabajando en él y mantener un registro detallado de todas las modificaciones. El flujo de trabajo básico consiste en:\n1. Realizar cambios en los archivos del proyecto\nPor ejemplo, modificar el script de análisis, agregar nuevos datos, actualizar los resultados exportados o mejorar la documentación.\n2. Guardar los cambios en Git\nCada vez que se desee registrar un avance, se puede utilizar el punto para indicarle al software que suba todos los archivos que fueron modificados y realizar un commit con un mensaje descriptivo. Por ejemplo:\n\ngit add .\ngit commit -m \"Actualización del script con nuevos gráficos\"\n\nEs importante que el mensaje del commit sea claro y específico, para facilitar la comprensión del historial de cambios.\n3. Sincronizar los cambios con GitHub\n\nPara mantener el repositorio remoto actualizado y respaldado, se utiliza el comando:\n\ngit push\n\nEsto sube los cambios al repositorio en línea, donde pueden ser consultados por otros usuarios o por el propio autor desde cualquier lugar.\n4. Visualizar el historial y colaborar\nGitHub permite revisar el historial completo de commits, comparar versiones de archivos y, en proyectos colaborativos, gestionar solicitudes de cambio (pull requests) y comentarios. Esto facilita la colaboración y la revisión del trabajo en equipo.\nEl uso regular de Git y GitHub asegura que el proyecto esté siempre respaldado, documentado y listo para ser compartido o retomado en cualquier momento (Bryan, 2018).",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Uso de Git y GitHub en Proyectos de R</span>"
    ]
  },
  {
    "objectID": "11.1_M_apoyo.html",
    "href": "11.1_M_apoyo.html",
    "title": "18  Material de apoyo",
    "section": "",
    "text": "Tutorial en YouTube “Cómo instalar R y RStudio en menos de 2 minutos - 2024”. Elaborado por Herbert Lizama.\nR para ciencia de datos por Handley Wickham & Garrett Grolemund\nRegresión lineal usando R.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Material de apoyo</span>"
    ]
  },
  {
    "objectID": "estadistica_descriptiva.html",
    "href": "estadistica_descriptiva.html",
    "title": "19  Estadística descriptiva usando funciones en R",
    "section": "",
    "text": "19.1 Medidas principales en estadística descriptiva\nLa estadística descriptiva es una rama esencial de la estadística que se ocupa de resumir y describir las características principales de un conjunto de datos. Su propósito es proporcionar una visión clara y comprensible de los datos, permitiendo identificar patrones, tendencias y comportamientos generales sin realizar inferencias o predicciones. Este tipo de análisis es el primer paso en cualquier estudio estadístico, ya que organiza y presenta la información de manera que sea fácil de interpretar.\nEn R, la estadística descriptiva se puede realizar de manera eficiente gracias a una amplia variedad de herramientas y funciones predefinidas, así como paquetes especializados que amplían las capacidades del análisis. Estas herramientas permiten calcular medidas clave que se agrupan en tres categorías principales: medidas de tendencia central, medidas de dispersión y medidas de forma.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Estadística descriptiva usando funciones en R</span>"
    ]
  },
  {
    "objectID": "estadistica_descriptiva.html#medidas-principales-en-estadística-descriptiva",
    "href": "estadistica_descriptiva.html#medidas-principales-en-estadística-descriptiva",
    "title": "19  Estadística descriptiva usando funciones en R",
    "section": "",
    "text": "19.1.1 Medidas de tendencia central\nLas medidas de tendencia central describen el valor típico o central de un conjunto de datos. Estas medidas son fundamentales para resumir los datos en un solo valor representativo.\nMedia aritmética: Es el promedio aritmético de los datos. Se calcula sumando todos los valores y dividiendo entre el número total de observaciones. Es sensible a valores extremos (outliers).\nFórmula:\n\n\n\n\n\nEjemplo en R:\n\ndatos &lt;- c(10, 20, 30, 40, 50)\nmedia &lt;- mean(datos)\nprint(media)  # Resultado: \n\n[1] 30\n\n\nMediana: Es el valor que divide el conjunto de datos en dos partes iguales, de modo que el 50% de los valores son menores o iguales a la mediana y el otro 50% son mayores o iguales. Es menos sensible a valores extremos que la media.\nEjemplo en R:\n\nmediana &lt;- median(datos)\nprint(mediana)  # Resultado: \n\n[1] 30\n\n\nModa: Es el valor o los valores que ocurren con mayor frecuencia en un conjunto de datos. En R base, no existe una función predefinida para calcular la moda, pero se puede implementar fácilmente.\nEjemplo de función para calcular la moda:\n\nmoda_ej &lt;- function(x) {\n  tabla &lt;- table(x)\n  moda &lt;- names(tabla[tabla == max(tabla)])\n  return(moda)\n}\ndatos_moda &lt;- c(10, 20, 20, 30, 40)\nprint(moda_ej(datos_moda))  # Resultado: \n\n[1] \"20\"\n\n\n\n\n19.1.2 Medidas de dispersión\nLas medidas de dispersión describen la variabilidad o el grado de dispersión de los datos en torno a la media. Estas medidas son esenciales para entender la distribución de los datos.\nVarianza: Mide la dispersión de los datos respecto a la media. Es el promedio de las diferencias al cuadrado entre cada valor y la media. Una varianza alta indica que los datos están muy dispersos.\nFórmula de la varianza muestral:\n\n\n\n\n\n\nvarianza &lt;- var(datos)\nprint(varianza)  # Resultado: \n\n[1] 250\n\n\nDesviación estándar: Es la raíz cuadrada de la varianza. Proporciona una medida de dispersión en las mismas unidades que los datos originales.\nFórmula:\n\n\n\n\n\nEjemplo en R:\n\ndesviacion &lt;- sd(datos)\nprint(desviacion)  # Resultado:\n\n[1] 15.81139\n\n\nRango: Es la diferencia entre el valor máximo y el valor mínimo de los datos. Es una medida simple pero útil para entender la amplitud de los datos.\nEjemplo en R:\n\nrango &lt;- max(datos)-min(datos)\nprint(rango)  # Resultado: \n\n[1] 40\n\n\nRango intercuartílico (IQR): Es la diferencia entre el tercer cuartil (Q3) y el primer cuartil (Q1). Representa la dispersión de la mitad central de los datos y es menos sensible a valores extremos.\nFórmula:\n\n\n\n\n\nEjemplo en R:\n\niqr &lt;- IQR(datos)\nprint(iqr)  # Resultado: \n\n[1] 20\n\n\n\n\n19.1.3 Medidas de forma\nLas medidas de forma describen la distribución de los datos en términos de su simetría y concentración en torno a la media.\nAsimetría (skewness): Mide el grado de simetría de la distribución de los datos. Una asimetría positiva indica que la cola derecha es más larga, mientras que una asimetría negativa indica que la cola izquierda es más larga.\nGuía gráfica para interpretar asimetría:\n\n\n\n\n\nFórmula:\n\n\n\n\n\nEjemplo en R (usando el paquete psych):\n\n# Instalación y carga del paquete psych\nif (!require(\"psych\")) install.packages(\"psych\")\n\n# Análisis de asimetria\ndatos &lt;- c(10, 20, 30, 40, 50)\nasimetria &lt;- skew(datos)\nprint(asimetria)  # Resultado: 0 (distribución simétrica)\n\n[1] 0\n\n\nCurtosis (kurtosis): Mide la concentración de los datos en torno a la media. Una curtosis alta indica una distribución con colas más pesadas (leptocúrtica), mientras que una curtosis baja indica colas más ligeras (platicúrtica).\nGuía gráfica para interpretar curtosis:\n\n\n\n\n\nFórmula:\n\n\n\n\n\nEjemplo en R (usando el paquete psych):\n\ncurtosis &lt;- kurtosi(datos)\nprint(curtosis)  # Resultado: -1.912 (distribución platicúrtica)\n\n[1] -1.912",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Estadística descriptiva usando funciones en R</span>"
    ]
  },
  {
    "objectID": "estadistica_descriptiva.html#base-de-datos-para-los-ejemplos",
    "href": "estadistica_descriptiva.html#base-de-datos-para-los-ejemplos",
    "title": "19  Estadística descriptiva usando funciones en R",
    "section": "19.2 Base de datos para los ejemplos",
    "text": "19.2 Base de datos para los ejemplos\nEn 2002, se llevó a cabo un estudio en la Universidad de San Carlos de Guatemala, en el que se recopilaron datos de 460 estudiantes de diversas facultades, generando una base de datos que incluye una amplia variedad de variables como: FACULTAD, EDAD, SEXO, EST_CIVIL, PESO_lbs, TALLA, entre otras. Esta base de datos, disponible para su descarga en formato CSV, se utilizará a lo largo de esta sección del manual para ilustrar diferentes métodos de análisis descriptivo de datos, adaptando las herramientas y conceptos desarrollados a las características de las variables incluidas. Para seguir los ejemplos prácticos, se recomienda que el usuario descargue el archivo y lo guarde en la carpeta correspondiente al proyecto en curso.\n\n19.2.1 Preparación del área de trabajo\nAntes de comenzar con el análisis, es necesario preparar el entorno de trabajo instalando y cargando los paquetes necesarios, estableciendo el directorio de trabajo y revisando la estructura de los datos.\n\n# Instalación y carga de paquetes  \n # Incluye ggplot2, dplyr, tidyr\nif (!require(\"tidyverse\")) install.packages(\"tidyverse\")\n\n # Exportación a Excel\nif (!require(\"writexl\")) install.packages(\"writexl\")  \n\n # Realiza analisis de estaística descriptiva completos\nif (!require(\"psych\")) install.packages(\"psych\")\n\n # Se utiliza para establecer el directorio de trabajo \nif (!require(\"rstudioapi\")) install.packages(\"rstudioapi\")\n\n\n\n19.2.2 Establecer directorio de trabajo\nEs importante asegurarse de que el archivo de datos esté en el directorio de trabajo correcto. Esto se puede hacer con el siguiente código una vez ya se ha guardado el script:\n\n# Establecer y verificar directorio de trabajo\nsetwd(dirname(rstudioapi::getActiveDocumentContext()$path))\ngetwd()\n\n\n\n19.2.3 Importar la base de datos\nUna vez establecido el directorio de trabajo, se puede importar la base de datos en formato CSV:\n\n# Importar la base de datos\nestudiantes&lt;- read_csv(\"datos_estudiantes.csv\")\n\n\n\n19.2.4 Revisar de la estructura de los datos\nEs fundamental revisar la estructura de los datos para entender el tipo de variables y su formato:\n\n# Revisar la estructura de los datos\nsapply(estudiantes, class)\n\n# Convertir todos los nombres de las columnas a minúsculas\nnames(estudiantes)&lt;- tolower(names(estudiantes))\n\n# Tener todas las variables en minúsculas facilita su manipulación\n\n# Revisar los valores de las variables categoricas \ncategoricas&lt;-list(facultad = c(unique(estudiantes$facultad)),\n           sexo = c(unique(estudiantes$sexo)), \n           est_civil = c(unique(estudiantes$est_civil)),\n           trabaja = c(unique(estudiantes$trabaja)),\n           jornada = c(unique(estudiantes$jornada)),\n           fuma = c(unique(estudiantes$fuma)),\n           alcohol = c(unique(estudiantes$alcohol)))\n\n\n\n19.2.5 Limpieza de la base de datos\nAntes de realizar el análisis, es necesario limpiar los datos para corregir valores inconsistentes y asegurarse de que las variables estén en el formato adecuado:\n\n# Corregir los valores incorrectos de la variable \"fuma\"\nestudiantes$fuma &lt;- ifelse(tolower(estudiantes$fuma) == \"sí\",\n                           1, estudiantes$fuma)\nunique(estudiantes$fuma)\n\n[1] \"2\" \"1\"\n\n# Establecer como varibales tipo factor a las variables categoricas\nestudiantes &lt;- estudiantes %&gt;%\n  mutate(across(\n    c(facultad, sexo, est_civil, trabaja, jornada, fuma, alcohol), \n    as.factor))",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Estadística descriptiva usando funciones en R</span>"
    ]
  },
  {
    "objectID": "estadistica_descriptiva.html#funciones-por-defecto-en-r-para-estadística-descriptiva",
    "href": "estadistica_descriptiva.html#funciones-por-defecto-en-r-para-estadística-descriptiva",
    "title": "19  Estadística descriptiva usando funciones en R",
    "section": "19.3 Funciones por defecto en R para estadística descriptiva",
    "text": "19.3 Funciones por defecto en R para estadística descriptiva\nR base proporciona varias funciones útiles para realizar análisis descriptivos básicos. A continuación, se presentan ejemplos utilizando la base de datos de estudiantes.\n\n19.3.1 Medidas de tendencia central\nLas medidas de tendencia central describen el valor típico o central de un conjunto de datos.\nMedia: Promedio de los valores.\nMediana: Valor que divide los datos en dos partes iguales.\nModa: Valor más frecuente (no existe una función por defecto en R para calcular la moda).\n\n# Calcular medidas de tendencia central para la variable \"edad\"\nmedia_edad &lt;- mean(estudiantes$edad, na.rm = TRUE)\nmediana_edad &lt;- median(estudiantes$edad, na.rm = TRUE)\n\n# Resultados\nprint(paste(\"Media:\", media_edad))\n\n[1] \"Media: 24.0195652173913\"\n\nprint(paste(\"Mediana:\", mediana_edad))\n\n[1] \"Mediana: 22\"\n\n\n\n\n19.3.2 Medidas de dispersión\nLas medidas de dispersión describen la variabilidad de los datos.\nVarianza: Dispersión respecto a la media.\nDesviación estándar: Raíz cuadrada de la varianza.\nRango: Diferencia entre el valor máximo y mínimo.\nRango intercuartílico (IQR): Diferencia entre el tercer y primer cuartil.\n\n# Calcular medidas de dispersión para la variable \"peso_lbs\"\nvarianza_peso &lt;- var(estudiantes$peso_lbs, na.rm = TRUE)\ndesviacion_peso &lt;- sd(estudiantes$peso_lbs, na.rm = TRUE)\nrango_peso &lt;- max(estudiantes$peso_lbs)-min(estudiantes$peso_lbs)\niqr_peso &lt;- IQR(estudiantes$peso_lbs, na.rm = TRUE)\n\n# Resultados\nprint(paste(\"Varianza:\", varianza_peso))\n\n[1] \"Varianza: 872.415330870512\"\n\nprint(paste(\"Desviación estándar:\", desviacion_peso))\n\n[1] \"Desviación estándar: 29.5366777222915\"\n\nprint(paste(\"Rango:\", paste(rango_peso)))\n\n[1] \"Rango: 170\"\n\nprint(paste(\"IQR:\", iqr_peso))\n\n[1] \"IQR: 40\"\n\n\n\n\n19.3.3 Medidas de forma\nLas medidas de forma describen la distribución de los datos en términos de simetría y concentración.\nAsimetría: Grado de simetría de la distribución.\nCurtosis: Concentración de los datos en torno a la media.\nLa asimetría y curtosis no se pueden calcular con la funciones base de R para ello se debe emplear el paquete psych, con este las medidas de forma se calculan fácilmente:\n\n# Calcular asimetría y curtosis para la variable \"talla\"\nasimetria_talla &lt;- skew(estudiantes$talla, na.rm = TRUE)\ncurtosis_talla &lt;- kurtosi(estudiantes$talla, na.rm = TRUE)\n\n# Resultados\nprint(paste(\"Asimetría:\", asimetria_talla))\n\n[1] \"Asimetría: 0.0362232638780007\"\n\nprint(paste(\"Curtosis:\", curtosis_talla))\n\n[1] \"Curtosis: -0.177190677008652\"\n\n\n\n\n19.3.4 Resumen general con summary()\nLa función summary() proporciona un resumen estadístico básico para variables numéricas y categóricas:\n\n# Resumen general de la variable talla\nresumen_general &lt;- summary(estudiantes$talla)\nprint(resumen_general)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.280   1.570   1.630   1.639   1.700   1.900",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Estadística descriptiva usando funciones en R</span>"
    ]
  },
  {
    "objectID": "estadistica_descriptiva.html#paquetes-especializados-para-estadística-descriptiva-el-paquete-psych",
    "href": "estadistica_descriptiva.html#paquetes-especializados-para-estadística-descriptiva-el-paquete-psych",
    "title": "19  Estadística descriptiva usando funciones en R",
    "section": "19.4 Paquetes especializados para estadística descriptiva (el paquete psych)",
    "text": "19.4 Paquetes especializados para estadística descriptiva (el paquete psych)\nEl paquete psych es una herramienta poderosa y versátil para realizar análisis estadísticos descriptivos avanzados en R. Este paquete es especialmente útil cuando se trabaja con variables categóricas y numéricas simultáneamente, ya que permite calcular estadísticas detalladas, realizar análisis por grupos y obtener medidas de forma como asimetría y curtosis. Además, incluye opciones para calcular errores estándar e intervalos de confianza, lo que lo convierte en una excelente opción para análisis más completos.\n\n19.4.1 Instalación y carga del paquete\n\n# Instalación y carga del paquete\nif (!require(\"psych\")) install.packages(\"psych\")\n\n\n\n19.4.2 Análisis descriptivo general\nA continuación, se utilizará la base de datos de estudiantes para realizar un análisis descriptivo detallado. Este análisis incluirá medidas de tendencia central, dispersión y forma.\nLa función describe() del paquete psych permite calcular estadísticas descriptivas detalladas para variables numéricas. Estas estadísticas incluyen: Media, Desviación estándar, Mediana, Rango, Asimetría, Curtosis, Errores estándar.\n\n# Análisis descriptivo general para variables numéricas\nresultado_general &lt;- describe(estudiantes[, c(\"edad\", \"peso_lbs\", \"talla\")])\n\n# Mostrar resultados\nprint(resultado_general)\n\n         vars   n   mean    sd median trimmed   mad   min   max  range skew\nedad        1 460  24.02  5.74  22.00   22.99  2.97 17.00  55.0  38.00 1.94\npeso_lbs    2 460 139.44 29.54 134.00  137.46 29.65 79.00 249.0 170.00 0.68\ntalla       3 460   1.64  0.09   1.63    1.64  0.10  1.28   1.9   0.62 0.04\n         kurtosis   se\nedad         4.39 0.27\npeso_lbs     0.42 1.38\ntalla       -0.18 0.00\n\n\nSalida esperada: El resultado incluye un resumen detallado de cada variable numérica, con estadísticas como la media, desviación estándar, asimetría y curtosis.\n\n\n19.4.3 Análisis descriptivo categorizado\nLa función describeBy() permite realizar un análisis descriptivo agrupado por una o más variables categóricas. Esto es útil para comparar estadísticas entre diferentes grupos.\n\n# Análisis descriptivo agrupado por sexo y trabaja\nresultado_agrupado &lt;- describeBy(\n  estudiantes[, c(\"edad\", \"peso_lbs\", \"talla\")], \n  group = list(estudiantes$sexo, estudiantes$trabaja)\n)\n\n# Mostrar resultados\nprint(resultado_agrupado)\n\n\n Descriptive statistics by group \n: F\n: 1\n         vars  n   mean    sd median trimmed   mad   min    max  range  skew\nedad        1 76  26.04  5.69  25.00   25.37  5.93 18.00  42.00  24.00  0.91\npeso_lbs    2 76 126.67 25.32 122.50  124.05 20.76 79.00 199.00 120.00  0.97\ntalla       3 76   1.57  0.08   1.57    1.57  0.07  1.28   1.75   0.47 -0.42\n         kurtosis   se\nedad         0.14 0.65\npeso_lbs     0.86 2.90\ntalla        1.73 0.01\n------------------------------------------------------------ \n: M\n: 1\n         vars   n   mean    sd median trimmed   mad    min    max  range  skew\nedad        1 105  27.58  7.49   25.0   26.59  5.93  18.00  55.00  37.00  1.26\npeso_lbs    2 105 156.31 26.94  152.0  154.75 25.20 102.00 238.00 136.00  0.61\ntalla       3 105   1.70  0.07    1.7    1.70  0.07   1.49   1.85   0.36 -0.09\n         kurtosis   se\nedad         1.34 0.73\npeso_lbs     0.41 2.63\ntalla       -0.15 0.01\n------------------------------------------------------------ \n: F\n: 2\n         vars   n   mean    sd median trimmed   mad  min    max  range skew\nedad        1 154  22.57  4.51  21.00   21.77  2.97 17.0  44.00  27.00 2.53\npeso_lbs    2 154 125.66 24.56 120.00  123.27 17.79 84.0 227.00 143.00 1.07\ntalla       3 154   1.59  0.07   1.58    1.58  0.06  1.4   1.78   0.38 0.44\n         kurtosis   se\nedad         7.79 0.36\npeso_lbs     1.37 1.98\ntalla       -0.07 0.01\n------------------------------------------------------------ \n: M\n: 2\n         vars   n   mean    sd median trimmed   mad   min   max  range skew\nedad        1 125  21.58  2.87   21.0   21.24  1.48 17.00  34.0  17.00 1.66\npeso_lbs    2 125 149.99 28.27  147.0  148.49 26.69 88.00 249.0 161.00 0.64\ntalla       3 125   1.69  0.08    1.7    1.69  0.07  1.52   1.9   0.38 0.10\n         kurtosis   se\nedad         4.34 0.26\npeso_lbs     0.92 2.53\ntalla       -0.05 0.01\n\n\nNota importante: Cuando se utiliza describeBy(), el paquete psych genera una tabla separada para cada combinación de las variables categóricas. Esto puede ser útil para análisis simples, pero puede volverse limitante en casos donde se necesite consolidar los resultados en un solo dataframe o realizar análisis más complejos.\n\n\n19.4.4 Exportación de resultados\nLos resultados del análisis descriptivo pueden exportarse fácilmente a un archivo Excel para su revisión o presentación:\n\n# Exportar resultados agrupados a Excel\nwrite_xlsx(resultado_agrupado, \"analisis_descriptivo_psych.xlsx\")\n\n\n\n19.4.5 Ventajas del paquete psych\nEl paquete psych ofrece varias ventajas para el análisis descriptivo:\n\nEstadísticas más detalladas: Incluye medidas avanzadas como asimetría, curtosis, errores estándar e intervalos de confianza.\nAnálisis por grupos: Permite calcular estadísticas descriptivas para diferentes combinaciones de variables categóricas.\nFlexibilidad: Facilita la categorización de variables numéricas en rangos personalizados.\nExportación de resultados: Los resultados pueden exportarse fácilmente a formatos como Excel para su análisis posterior.\n\n\n\n19.4.6 Limitaciones del paquete psych\nAunque el paquete psych es muy útil, presenta algunas limitaciones:\n\nResultados separados por combinación de categorías: La función describeBy() genera una tabla separada para cada combinación de las variables categóricas, lo que puede dificultar la consolidación de los resultados en un solo archivo o dataframe.\nFalta de personalización: No permite agregar estadísticas personalizadas, como percentiles específicos o medidas adicionales que no estén incluidas en las funciones predefinidas.\nManejo de valores faltantes: Aunque maneja valores faltantes de manera básica, no ofrece opciones avanzadas para imputación o análisis detallado de datos incompletos.\nExportación limitada: Los resultados no están listos para exportarse directamente en un formato tabular consolidado, lo que requiere pasos adicionales para su preparación.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Estadística descriptiva usando funciones en R</span>"
    ]
  },
  {
    "objectID": "estadistica_descriptiva.html#función-personalizada-para-análisis-descriptivo-completo",
    "href": "estadistica_descriptiva.html#función-personalizada-para-análisis-descriptivo-completo",
    "title": "19  Estadística descriptiva usando funciones en R",
    "section": "19.5 Función personalizada para análisis descriptivo completo",
    "text": "19.5 Función personalizada para análisis descriptivo completo\nPara superar las limitaciones del paquete psych, se puede utilizar una función personalizada que ofrezca mayor flexibilidad y personalización. A continuación, se presenta una solución que incluye funciones auxiliares para calcular medidas avanzadas como la moda, asimetría y curtosis.\n\n19.5.1 Establecer funciones auxiliares\n\n19.5.1.1 Moda\n\n# Función de la moda\nmoda &lt;- function(x) {\n  # Eliminar valores NA\n  x &lt;- na.omit(x)\n  \n  # Verificar si el vector está vacío\n  if (length(x) == 0) return(NA_character_)\n  \n  # Calcular la frecuencia de cada valor\n  tabla &lt;- table(x)\n  \n  # Identificar el/los valores con mayor frecuencia\n  max_frecuencia &lt;- max(tabla)\n  modas &lt;- names(tabla[tabla == max_frecuencia])\n  \n  # Verificar si todos los valores son únicos (sin moda)\n  if (max_frecuencia == 1) return(NA_character_)\n  \n  # Retornar la moda como un string separado por comas\n  return(paste(modas, collapse = \", \"))\n}\n\n\n\n19.5.1.2 Asimetría\n\n# Función Asimentría\ncalcular_asimetria &lt;- function(x) {\n  # Eliminar valores NA\n  x &lt;- na.omit(x)\n  \n  # Verificar que haya suficientes datos\n  if (length(x) &lt; 3) return(NA_real_)\n  \n  # Calcular media y desviación estándar\n  m &lt;- mean(x)\n  s &lt;- sd(x)\n  \n  # Manejar el caso de desviación estándar cero\n  if (s == 0) return(0)\n  \n  # Calcular asimetría\n  asimetria &lt;- sum((x - m)^3) / (length(x) * s^3)\n  return(asimetria)\n}\n\n\n\n19.5.1.3 Curtosis\n\n# Función Curtosis \ncalcular_curtosis &lt;- function(x) {\n  # Eliminar valores NA\n  x &lt;- na.omit(x)\n  \n  # Verificar que haya suficientes datos\n  if (length(x) &lt; 4) return(NA_real_)\n  \n  # Calcular media y desviación estándar\n  m &lt;- mean(x)\n  s &lt;- sd(x)\n  \n  # Manejar el caso de desviación estándar cero\n  if (s == 0) return(0)\n  \n  # Calcular curtosis\n  curtosis &lt;- sum((x - m)^4) / (length(x) * s^4) - 3\n  return(curtosis)\n}\n\n\n\n\n19.5.2 Función principal: análisis por categorías\nLa función principal realiza el análisis descriptivo agrupando los datos según las variables categóricas especificadas. Calcula estadísticas clave como la media, mediana, moda, desviación estándar, varianza, rango, cuartiles, asimetría y curtosis.\n\n# Función Análisis por Categoría\nanalisis_por_categoria &lt;- function(datos, \n                                   columna_numerica,\n                                   columnas_categoricas) {\n  datos %&gt;%\n    group_by(across(all_of(columnas_categoricas))) %&gt;%\n    summarise(\n      Variable = columna_numerica,\n      N_validos = sum(!is.na(.data[[columna_numerica]])),\n      N_missing = sum(is.na(.data[[columna_numerica]])),\n      Media = mean(.data[[columna_numerica]], na.rm = TRUE),\n      Mediana = median(.data[[columna_numerica]], na.rm = TRUE),\n      Moda = moda(.data[[columna_numerica]]),\n      Desviacion_estandar = sd(.data[[columna_numerica]], na.rm = TRUE),\n      Varianza = var(.data[[columna_numerica]], na.rm = TRUE),\n      Rango_min = min(.data[[columna_numerica]], na.rm = TRUE),\n      Rango_max = max(.data[[columna_numerica]], na.rm = TRUE),\n      Rango = Rango_max - Rango_min,\n      IQR = IQR(.data[[columna_numerica]], na.rm = TRUE),\n      Q1 = quantile(.data[[columna_numerica]], probs = 0.25, na.rm = TRUE),\n      Q2 = quantile(.data[[columna_numerica]], probs = 0.50, na.rm = TRUE),\n      Q3 = quantile(.data[[columna_numerica]], probs = 0.75, na.rm = TRUE),\n      Asimetria = calcular_asimetria(.data[[columna_numerica]]),\n      Curtosis = calcular_curtosis(.data[[columna_numerica]]),\n      .groups = 'drop'\n    )\n}\n\n\n\n19.5.3 Función para análisis de múltiples variables numéricas\nPara analizar varias columnas numéricas simultáneamente, se puede usar la siguiente función, que aplica analisis_por_categoria a cada columna numérica especificada:\n\n# Función para anlisar multiples variables numericas simultaneamente \nanalisis_multiple &lt;- function(datos, \n                              columnas_numericas,\n                              columnas_categoricas) {\n  resultados &lt;- list()\n  for (col in columnas_numericas) {\n    resultados[[col]] &lt;- analisis_por_categoria(\n      datos = datos,\n      columna_numerica = col,\n      columnas_categoricas = columnas_categoricas\n    )\n  }\n  bind_rows(resultados)  # Combina todos los resultados en un dataframe\n}\n\n\n\n19.5.4 Ejemplo de uso\nPara ilustrar el uso de la función personalizada, se realizará un análisis descriptivo completo utilizando la base de datos de estudiantes de la Universidad de San Carlos de Guatemala. Este ejemplo mostrará cómo analizar múltiples variables numéricas categorizadas por diferentes variables cualitativas.\n\n19.5.4.1 Preparación del análisis\nAntes de ejecutar el análisis, es necesario definir las columnas numéricas y categóricas que se incluirán en el estudio. Las columnas numéricas representan las variables cuantitativas que se analizarán, mientras que las columnas categóricas se utilizarán para agrupar los datos.\n\n# Definir las columnas numéricas y categóricas\n# Variables cuantitativas\ncolumnas_numericas &lt;- c(\"talla\", \"peso_lbs\", \"edad\")  \n\n# Variables cualitativas\ncolumnas_categoricas &lt;- c(\"sexo\", \"trabaja\")         \n\nA continuación, se ejecuta la función analisis_multiple, que aplica el análisis descriptivo a cada variable numérica, agrupando los resultados según las categorías especificadas.\n\n# Ejecutar el análisis descriptivo completo\nresultados_finales &lt;- analisis_multiple(\n  datos = estudiantes,\n  columnas_numericas = columnas_numericas,\n  columnas_categoricas = columnas_categoricas\n)\n\n\n\n19.5.4.2 Visualización de resultados\nLos resultados del análisis se almacenan en un objeto llamado resultados_finales, que contiene un resumen estadístico detallado para cada combinación de las variables categóricas. Este resumen incluye medidas como la media, mediana, moda, desviación estándar, varianza, rango, asimetría y curtosis, entre otras.\nPara visualizar los resultados directamente en la consola, se utiliza la función print(). El resultado es una tabla organizada que muestra las estadísticas descriptivas para cada combinación de las categorías sexo y trabaja.\n\n# Mostrar resultados\nprint(resultados_finales)\n\n# A tibble: 12 × 19\n   sexo  trabaja Variable N_validos N_missing  Media Mediana Moda \n   &lt;fct&gt; &lt;fct&gt;   &lt;chr&gt;        &lt;int&gt;     &lt;int&gt;  &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;\n 1 F     1       talla           76         0   1.57    1.57 1.57 \n 2 F     2       talla          154         0   1.59    1.58 1.6  \n 3 M     1       talla          105         0   1.70    1.7  1.7  \n 4 M     2       talla          125         0   1.69    1.7  1.7  \n 5 F     1       peso_lbs        76         0 127.    122.   130  \n 6 F     2       peso_lbs       154         0 126.    120    114  \n 7 M     1       peso_lbs       105         0 156.    152    150  \n 8 M     2       peso_lbs       125         0 150.    147    150  \n 9 F     1       edad            76         0  26.0    25    21   \n10 F     2       edad           154         0  22.6    21    21   \n11 M     1       edad           105         0  27.6    25    22   \n12 M     2       edad           125         0  21.6    21    21   \n# ℹ 11 more variables: Desviacion_estandar &lt;dbl&gt;, Varianza &lt;dbl&gt;,\n#   Rango_min &lt;dbl&gt;, Rango_max &lt;dbl&gt;, Rango &lt;dbl&gt;, IQR &lt;dbl&gt;, Q1 &lt;dbl&gt;,\n#   Q2 &lt;dbl&gt;, Q3 &lt;dbl&gt;, Asimetria &lt;dbl&gt;, Curtosis &lt;dbl&gt;\n\n\nAdemás de visualizar los resultados en la consola, es posible exportarlos a un archivo Excel para facilitar su revisión o presentación:\n\n# Exportar resultados a Excel para mejor visualización\nwrite_xlsx(resultados_finales, \"analisis_descriptivo_estudiantes.xlsx\")\n\nEste ejemplo demuestra cómo utilizar la función personalizada para realizar un análisis descriptivo completo y categorizado. La tabla resultante proporciona una visión detallada de las características de las variables numéricas, agrupadas por categorías cualitativas. Además, la posibilidad de exportar los resultados a Excel permite compartir y analizar los datos de manera más eficiente.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Estadística descriptiva usando funciones en R</span>"
    ]
  },
  {
    "objectID": "estadistica_descriptiva.html#resumen-comparativo-funciones-base-de-r-paquete-psych-y-función-personalizada",
    "href": "estadistica_descriptiva.html#resumen-comparativo-funciones-base-de-r-paquete-psych-y-función-personalizada",
    "title": "19  Estadística descriptiva usando funciones en R",
    "section": "19.6 Resumen Comparativo: Funciones Base de R, Paquete psych y Función Personalizada",
    "text": "19.6 Resumen Comparativo: Funciones Base de R, Paquete psych y Función Personalizada\nA continuación, se presenta una comparación entre el paquete psych y la función personalizada para realizar análisis descriptivos, destacando las fortalezas y limitaciones de cada enfoque. Esta comparación permite identificar cuál es la mejor opción según las necesidades específicas del análisis.\n\n\n\n\n\n\n\n\n\nCaracterística\nFunciones Base de R\nPaquete psych\nFunción Personalizada\n\n\n\n\nEstadísticas avanzadas\nCalcula medidas básicas como media, mediana, desviación estándar, varianza y rango.\nIncluye medidas como asimetría y curtosis.\nIncluye asimetría, curtosis, moda y más estadísticas avanzadas.\n\n\nAnálisis por grupos\nRequiere pasos adicionales para agrupar y calcular estadísticas por categorías.\nGenera tablas separadas para cada combinación de categorías, lo que puede dificultar la consolidación.\nConsolida todos los resultados en un único dataframe, facilitando su manejo y análisis.\n\n\nFlexibilidad\nLimitada a las funciones predefinidas, sin opciones para personalización avanzada.\nLimitada a las funciones predefinidas del paquete.\nTotalmente personalizable, permitiendo agregar o modificar estadísticas según las necesidades.\n\n\nExportación\nRequiere pasos adicionales para preparar los resultados antes de exportarlos.\nRequiere pasos adicionales para preparar los resultados antes de exportarlos.\nLos resultados están listos para exportarse directamente en un formato tabular.\n\n\nFacilidad de uso\nMuy fácil de usar para cálculos básicos, pero limitada en análisis avanzados.\nFácil de usar para análisis avanzados estándar.\nRequiere más configuración inicial, pero ofrece mayor control y personalización.\n\n\nManejo de valores faltantes\nManejo básico con argumentos como na.rm = TRUE.\nManejo básico de valores faltantes.\nPermite un manejo avanzado y personalizado de valores faltantes.\n\n\n\nLas funciones base de R son ideales para cálculos rápidos y sencillos, como la media (mean()), mediana (median()), desviación estándar (sd()), varianza (var()), rango (range()), y el resumen general (summary()). Estas funciones son fáciles de usar y están disponibles de forma predeterminada, lo que las convierte en una excelente opción para análisis básicos. Sin embargo, su alcance es limitado cuando se requiere un análisis más detallado o agrupado, ya que no incluyen medidas avanzadas como asimetría o curtosis, ni permiten un análisis categorizado sin pasos adicionales.\nEl paquete psych es una herramienta poderosa y fácil de usar para realizar análisis descriptivos avanzados, especialmente cuando se busca rapidez y simplicidad en el cálculo de estadísticas estándar. Ofrece medidas avanzadas como asimetría y curtosis, y permite realizar análisis agrupados con la función describeBy(). Sin embargo, su enfoque en generar tablas separadas para cada combinación de categorías puede ser una limitación en proyectos que requieren consolidar resultados o realizar análisis más complejos. Además, la personalización de las estadísticas calculadas es limitada, ya que depende de las funciones predefinidas del paquete.\nPor otro lado, la función personalizada destaca por su flexibilidad y capacidad de adaptación. Permite incluir estadísticas adicionales, como la moda, y consolidar resultados en un único dataframe, lo que facilita su manejo y exportación en un formato listo para su uso. Además, ofrece un control total sobre el análisis, permitiendo adaptarlo a necesidades específicas, como el manejo avanzado de valores faltantes o la categorización de variables numéricas. Esto la convierte en una opción ideal para proyectos que demandan un mayor nivel de personalización y control.",
    "crumbs": [
      "<span class='chapter-number'>19</span>  <span class='chapter-title'>Estadística descriptiva usando funciones en R</span>"
    ]
  },
  {
    "objectID": "regresion_simple.html",
    "href": "regresion_simple.html",
    "title": "20  Regresión lineal simple usando R",
    "section": "",
    "text": "20.1 Conceptos fundamentales\nLa regresión lineal simple es una técnica estadística utilizada para modelar la relación entre una variable dependiente y una variable independiente. Su propósito principal es predecir el valor de la variable dependiente a partir de la variable independiente, lo que permite entender cómo varía una en función de la otra. Esta técnica es fundamental en el análisis estadístico, ya que proporciona una base para la inferencia y la toma de decisiones en diversos campos (Montgomery, Peck, & Vining, 2012).\nPor ejemplo, en economía, la regresión lineal simple puede utilizarse para predecir el consumo en función del ingreso. En biología, se aplica para analizar la relación entre la dosis de un fármaco y la respuesta de un organismo. En ciencias sociales, se utiliza para estudiar cómo factores como la educación influyen en los ingresos de una población (Field, 2013).\nLas variables dependientes son aquellas que se desean predecir o explicar, mientras que las variables independientes son las que se utilizan para realizar dicha predicción. La diferencia radica en que la variable dependiente es el resultado que se estudia, mientras que la variable independiente es el factor que se manipula o se observa (Cohen, Cohen, West, & Aiken, 2003).\nLa relación lineal implica que existe una conexión directa entre las variables, de tal manera que un cambio en la variable independiente produce un cambio proporcional en la variable dependiente.",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Regresión lineal simple usando R</span>"
    ]
  },
  {
    "objectID": "regresion_simple.html#modelo-de-regresión-lineal-simple",
    "href": "regresion_simple.html#modelo-de-regresión-lineal-simple",
    "title": "20  Regresión lineal simple usando R",
    "section": "20.2 Modelo de Regresión Lineal Simple",
    "text": "20.2 Modelo de Regresión Lineal Simple\nEl modelo de regresión lineal simple se expresa mediante la siguiente ecuación:\n\n\n\n\n\nEn esta ecuación, Y representa la variable dependiente, X es la variable independiente, ß0 es la intersección (el valor de Y) cuando X) es cero) y ß1 es la pendiente (que indica el cambio en Y) por cada unidad de cambio en X. El término epilson (ε) representa el error del modelo, que captura la variabilidad en Y que no se explica por X (Kutner, Nachtsheim, Neter, & Li, 2005).",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Regresión lineal simple usando R</span>"
    ]
  },
  {
    "objectID": "regresion_simple.html#supuestos-de-la-regresión-lineal",
    "href": "regresion_simple.html#supuestos-de-la-regresión-lineal",
    "title": "20  Regresión lineal simple usando R",
    "section": "20.3 Supuestos de la Regresión Lineal",
    "text": "20.3 Supuestos de la Regresión Lineal\nPara que el modelo de regresión lineal sea válido, es fundamental que se cumplan ciertos supuestos:\n\nLinealidad: La relación entre las variables debe ser lineal.\nIndependencia: Las observaciones deben ser independientes entre sí.\nHomoscedasticidad: La varianza de los errores debe ser constante a lo largo de todos los niveles de X.\nNormalidad: Los errores deben seguir una distribución normal (Tabachnick & Fidell, 2013).",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Regresión lineal simple usando R</span>"
    ]
  },
  {
    "objectID": "regresion_simple.html#evaluación-del-modelo",
    "href": "regresion_simple.html#evaluación-del-modelo",
    "title": "20  Regresión lineal simple usando R",
    "section": "20.4 Evaluación del Modelo",
    "text": "20.4 Evaluación del Modelo\nLa calidad del modelo de regresión lineal simple se evalúa a través de varias métricas:\n\nR-cuadrado: Indica la proporción de la variabilidad en la variable dependiente que es explicada por la variable independiente. Un valor cercano a 1 sugiere un buen ajuste del modelo.\nAnálisis de residuos: Es crucial analizar los residuos para verificar los supuestos del modelo, asegurando que no haya patrones sistemáticos que indiquen un mal ajuste (Belsley, Kuh, & Welsch, 1980).",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Regresión lineal simple usando R</span>"
    ]
  },
  {
    "objectID": "regresion_simple.html#ejemplo-práctico",
    "href": "regresion_simple.html#ejemplo-práctico",
    "title": "20  Regresión lineal simple usando R",
    "section": "20.5 Ejemplo Práctico",
    "text": "20.5 Ejemplo Práctico\nEste ejemplo práctico ilustra cómo realizar un análisis de regresión lineal simple utilizando un conjunto de datos sobre esporofitos. Los datos provienen de un estudio realizado en el laboratorio de cultivo de tejidos de la Facultad de Agronomía de la Universidad de San Carlos de Guatemala. En este estudio, se llevó a cabo la reproducción del helecho conocido como calahuala (Phlebodium pseudoaureum (Cav.) Lellinger).\nSe midió la altura de cada esporofito y se cuantificó la cantidad de esporofitos germinados en 30 frascos que contenían medio de cultivo Murashige y Skoog. Los resultados obtenidos se presentan en el siguiente archivo. Este análisis se basa en la investigación de Rosales Castillo (2005), quien realizó una micropropagación de Calahuala utilizando tres tipos de explantes en diferentes medios de cultivo in vitro.\nEl objetivo de este análisis es evaluar la relación entre la altura de los esporofitos y la cantidad de esporofitos germinados, utilizando la regresión lineal simple como herramienta estadística. A través de este proceso, se busca no solo ajustar un modelo que explique esta relación, sino también verificar los supuestos que sustentan la validez del modelo.\n\n20.5.1 Instalación y Carga de Paquetes\nPara comenzar, es necesario instalar y cargar los paquetes requeridos. Estos paquetes proporcionan funciones útiles para la manipulación de datos y la visualización.\n\n# Instalación y carga de paquetes  \n# Incluye ggplot2, dplyr, tidyr\nif (!require(\"tidyverse\")) install.packages(\"tidyverse\")\n\n# Se utiliza para evaluar el supuesto de homocedasticidad\nif (!require(\"car\")) install.packages(\"car\")\n\n# Se utiliza para importar archivos de Excel\nif (!require(\"readxl\")) install.packages(\"readxl\")\n\nExplicación:\n\ntidyverse: Este paquete incluye varias herramientas para la manipulación y visualización de datos, como ggplot2, dplyr y tidyr.\ncar: Proporciona funciones para realizar pruebas de hipótesis y diagnósticos de modelos, incluyendo la evaluación de homocedasticidad.\nreadxl: Permite importar datos desde archivos de Excel, facilitando la carga de conjuntos de datos.\n\n\n\n20.5.2 Importación de Datos\nA continuación, se importan los datos desde un archivo Excel.\n\n# Importar un archivo csv\ndatos &lt;- read_excel(\"esporofitos.xlsx\")\n\n# Visualizar los primeros registros del data frame\nhead(datos)\n\n# A tibble: 6 × 3\n  frasco cantidad_de_esporofitos altura_mm\n   &lt;dbl&gt;                   &lt;dbl&gt;     &lt;dbl&gt;\n1      1                      40      21.4\n2      2                      45      21  \n3      3                      60      20.5\n4      4                      55      20  \n5      5                      58      21  \n6      6                      40      21.7\n\n\nEste código carga el archivo de Excel que contiene los datos sobre esporofitos y muestra las primeras filas del conjunto de datos para verificar que se haya importado correctamente.\n\n\n20.5.3 Visualización de Datos\nSe elabora un gráfico de dispersión para observar la relación entre las variables.\n\n# Elaboración de un gráfico de dispersión entre altura y cantidad\nplot( datos$altura_mm, datos$cantidad_de_esporofitos)\n\n\n\n\n\n\n\n\nEste gráfico permite visualizar la relación entre la cantidad de esporofitos y la altura en milímetros, facilitando la identificación de patrones. Como se puede apreciar en el gráfico anterior hay una relación inversamente proporcional entre la altura de los esporofitos y la cantidad de esporofitos.\n\n\n20.5.4 Ajuste del Modelo de Regresión\nSe ajusta el modelo de regresión lineal simple utilizando la función lm().\n\n# Ajuste del modelo\nregsimple &lt;- lm(datos$altura_mm ~ datos $cantidad_de_esporofitos)\nsummary(regsimple)\n\n\nCall:\nlm(formula = datos$altura_mm ~ datos$cantidad_de_esporofitos)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.78523 -0.15056  0.01664  0.22403  0.62850 \n\nCoefficients:\n                                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                   22.8933399  0.1446248  158.29   &lt;2e-16 ***\ndatos$cantidad_de_esporofitos -0.0401248  0.0008826  -45.46   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3753 on 28 degrees of freedom\nMultiple R-squared:  0.9866,    Adjusted R-squared:  0.9862 \nF-statistic:  2067 on 1 and 28 DF,  p-value: &lt; 2.2e-16\n\n\nEl resumen del modelo ajustado proporciona información sobre los coeficientes, errores estándar y estadísticas de ajuste, permitiendo evaluar la calidad del modelo.\nExplicación del código:\n\nlm(): Esta función ajusta un modelo de regresión lineal donde la cantidad de esporofitos es la variable dependiente y la altura es la variable independiente.\nsummary(regsimple): Proporciona un resumen del modelo ajustado, incluyendo coeficientes, errores estándar y estadísticas de ajuste.\n\n\n\n20.5.5 Gráficos de Diagnóstico para evaluar los supuestos del modelo\nSe generan gráficos de diagnóstico para evaluar los supuestos del modelo.\n\n# Gráficos de diagnóstico de los supuestos\npar(mfrow=c(1,2)) # Crea una matriz de dos gráficos\nplot(regsimple, which=1:2)\n\n\n\n\n\n\n\npar(mfrow=c(1,1)) # Devuelve a su estado normal el área de gráficos\n\nEstos gráficos ayudan a verificar la linealidad y la homocedasticidad, asegurando que los supuestos del modelo se cumplan.\nExplicación del código:\nplot(regsimple, which=1:2): Genera los gráficos de residuos y ajuste, que ayudan a verificar la linealidad y la homocedasticidad.\n\n\n20.5.6 Prueba de Normalidad de los Residuos\nSe realiza la prueba de Shapiro-Wilk para evaluar la normalidad de los residuos.\n\n# Realizar la prueba de Shapiro-Wilk en los residuos\nshapiro.test(residuals(regsimple))\n\n\n    Shapiro-Wilk normality test\n\ndata:  residuals(regsimple)\nW = 0.95651, p-value = 0.2516\n\n\nExplicación:\nshapiro.test(residuals(regsimple)): Esta prueba evalúa si los residuos del modelo siguen una distribución normal. Un valor p por debajo del nivel de significancia ( &lt; 0.05) indica que los residuos no son normales. Para el ejemplo el valor de p es de 0.2516 por lo que no se rechaza la hipótesis nula y por lo tanto no hay suficiente evidencia estadística para indicar que los residuos no son normales.\n\n\n20.5.7 Prueba de Homocedasticidad de la varianza\nFinalmente, se evalúa el supuesto de homocedasticidad utilizando la prueba de heterocedasticidad.\n\n# Realizar prueba para el supuesto de homocedasticidad\nncvTest(regsimple)\n\nNon-constant Variance Score Test \nVariance formula: ~ fitted.values \nChisquare = 0.07681442, Df = 1, p = 0.78166\n\n\nExplicación:\nncvTest(regsimple): Esta función evalúa si la varianza de los residuos es constante a lo largo de los valores de la variable independiente. Un valor p menor al nivel de significancia (&lt;0.05) sugiere que hay heterocedasticidad, lo que puede invalidar el modelo. Para este ejemplo el valor de p es 0.78166 por lo que no hay evidencia estadística suficiente para indicar que la varianza de los residuos no es contante.",
    "crumbs": [
      "<span class='chapter-number'>20</span>  <span class='chapter-title'>Regresión lineal simple usando R</span>"
    ]
  },
  {
    "objectID": "regresion_multiple.html",
    "href": "regresion_multiple.html",
    "title": "21  Regresión múltiple usando R",
    "section": "",
    "text": "21.1 Notas:\nEn esta sección se desarrollará un ejemplo de regresión multiple\nEsta pendiente de finalización y carga",
    "crumbs": [
      "<span class='chapter-number'>21</span>  <span class='chapter-title'>Regresión múltiple usando R</span>"
    ]
  },
  {
    "objectID": "license.html",
    "href": "license.html",
    "title": "Licencia",
    "section": "",
    "text": "Creative Commons Legal Code\nCC0 1.0 Universal\nCREATIVE COMMONS CORPORATION IS NOT A LAW FIRM AND DOES NOT PROVIDE\nLEGAL SERVICES. DISTRIBUTION OF THIS DOCUMENT DOES NOT CREATE AN\nATTORNEY-CLIENT RELATIONSHIP. CREATIVE COMMONS PROVIDES THIS\nINFORMATION ON AN \"AS-IS\" BASIS. CREATIVE COMMONS MAKES NO WARRANTIES\nREGARDING THE USE OF THIS DOCUMENT OR THE INFORMATION OR WORKS\nPROVIDED HEREUNDER, AND DISCLAIMS LIABILITY FOR DAMAGES RESULTING FROM\nTHE USE OF THIS DOCUMENT OR THE INFORMATION OR WORKS PROVIDED\nHEREUNDER.\nStatement of Purpose\nThe laws of most jurisdictions throughout the world automatically confer exclusive Copyright and Related Rights (defined below) upon the creator and subsequent owner(s) (each and all, an “owner”) of an original work of authorship and/or a database (each, a “Work”).\nCertain owners wish to permanently relinquish those rights to a Work for the purpose of contributing to a commons of creative, cultural and scientific works (“Commons”) that the public can reliably and without fear of later claims of infringement build upon, modify, incorporate in other works, reuse and redistribute as freely as possible in any form whatsoever and for any purposes, including without limitation commercial purposes. These owners may contribute to the Commons to promote the ideal of a free culture and the further production of creative, cultural and scientific works, or to gain reputation or greater distribution for their Work in part through the use and efforts of others.\nFor these and/or other purposes and motivations, and without any expectation of additional consideration or compensation, the person associating CC0 with a Work (the “Affirmer”), to the extent that he or she is an owner of Copyright and Related Rights in the Work, voluntarily elects to apply CC0 to the Work and publicly distribute the Work under its terms, with knowledge of his or her Copyright and Related Rights in the Work and the meaning and intended legal effect of CC0 on those rights.\n\nCopyright and Related Rights. A Work made available under CC0 may be protected by copyright and related or neighboring rights (“Copyright and Related Rights”). Copyright and Related Rights include, but are not limited to, the following:\n\n\n\nthe right to reproduce, adapt, distribute, perform, display, communicate, and translate a Work;\nmoral rights retained by the original author(s) and/or performer(s);\npublicity and privacy rights pertaining to a person’s image or likeness depicted in a Work;\nrights protecting against unfair competition in regards to a Work, subject to the limitations in paragraph 4(a), below;\nrights protecting the extraction, dissemination, use and reuse of data in a Work;\ndatabase rights (such as those arising under Directive 96/9/EC of the European Parliament and of the Council of 11 March 1996 on the legal protection of databases, and under any national implementation thereof, including any amended or successor version of such directive); and\nother similar, equivalent or corresponding rights throughout the world based on applicable law or treaty, and any national implementations thereof.\n\n\n\nWaiver. To the greatest extent permitted by, but not in contravention of, applicable law, Affirmer hereby overtly, fully, permanently, irrevocably and unconditionally waives, abandons, and surrenders all of Affirmer’s Copyright and Related Rights and associated claims and causes of action, whether now known or unknown (including existing as well as future claims and causes of action), in the Work (i) in all territories worldwide, (ii) for the maximum duration provided by applicable law or treaty (including future time extensions), (iii) in any current or future medium and for any number of copies, and (iv) for any purpose whatsoever, including without limitation commercial, advertising or promotional purposes (the “Waiver”). Affirmer makes the Waiver for the benefit of each member of the public at large and to the detriment of Affirmer’s heirs and successors, fully intending that such Waiver shall not be subject to revocation, rescission, cancellation, termination, or any other legal or equitable action to disrupt the quiet enjoyment of the Work by the public as contemplated by Affirmer’s express Statement of Purpose.\nPublic License Fallback. Should any part of the Waiver for any reason be judged legally invalid or ineffective under applicable law, then the Waiver shall be preserved to the maximum extent permitted taking into account Affirmer’s express Statement of Purpose. In addition, to the extent the Waiver is so judged Affirmer hereby grants to each affected person a royalty-free, non transferable, non sublicensable, non exclusive, irrevocable and unconditional license to exercise Affirmer’s Copyright and Related Rights in the Work (i) in all territories worldwide, (ii) for the maximum duration provided by applicable law or treaty (including future time extensions), (iii) in any current or future medium and for any number of copies, and (iv) for any purpose whatsoever, including without limitation commercial, advertising or promotional purposes (the “License”). The License shall be deemed effective as of the date CC0 was applied by Affirmer to the Work. Should any part of the License for any reason be judged legally invalid or ineffective under applicable law, such partial invalidity or ineffectiveness shall not invalidate the remainder of the License, and in such case Affirmer hereby affirms that he or she will not (i) exercise any of his or her remaining Copyright and Related Rights in the Work or (ii) assert any associated claims and causes of action with respect to the Work, in either case contrary to Affirmer’s express Statement of Purpose.\nLimitations and Disclaimers.\n\n\n\nNo trademark or patent rights held by Affirmer are waived, abandoned, surrendered, licensed or otherwise affected by this document.\nAffirmer offers the Work as-is and makes no representations or warranties of any kind concerning the Work, express, implied, statutory or otherwise, including without limitation warranties of title, merchantability, fitness for a particular purpose, non infringement, or the absence of latent or other defects, accuracy, or the present or absence of errors, whether or not discoverable, all to the greatest extent permissible under applicable law.\nAffirmer disclaims responsibility for clearing rights of other persons that may apply to the Work or any use thereof, including without limitation any person’s Copyright and Related Rights in the Work. Further, Affirmer disclaims responsibility for obtaining any necessary consents, permissions or other rights required for any use of the Work.\nAffirmer understands and acknowledges that Creative Commons is not a party to this document and has no duty or obligation with respect to this CC0 or use of the Work."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introducción al entorno de programación R y su aplicación en el análisis estadístico de datos",
    "section": "",
    "text": "Introducción\nLa estadística clásica constituye un pilar esencial en la investigación científica y en la toma de decisiones fundamentadas en datos. Este manual ha sido elaborado para quienes se inician en el análisis estadístico, con el objetivo de introducir de manera gradual y comprensible las herramientas fundamentales del lenguaje R, ampliamente reconocido en la ciencia de datos y la estadística aplicada. A lo largo del texto, se abordan desde los conceptos básicos hasta técnicas más avanzadas, acompañando la teoría con ejemplos prácticos que facilitan la comprensión y la aplicación en contextos reales. El propósito central es ofrecer una base sólida que permita a los lectores utilizar R de forma efectiva en el análisis de datos, sin requerir experiencia previa en programación o estadística (Ihaka & Gentleman, 1996; R Core Team, 2023).",
    "crumbs": [
      "Introducción"
    ]
  },
  {
    "objectID": "index.html#propósito-del-manual",
    "href": "index.html#propósito-del-manual",
    "title": "Introducción al entorno de programación R y su aplicación en el análisis estadístico de datos",
    "section": "Propósito del manual",
    "text": "Propósito del manual\nEl manual está diseñado para guiar a personas principiantes en el uso de R, abarcando desde la instalación y configuración del entorno hasta la aplicación de técnicas estadísticas clásicas y modernas. Se exploran temas como la manipulación y visualización de datos, la gestión de proyectos, la exportación de resultados y la realización de análisis estadísticos descriptivos e inferenciales. Cada tema se desarrolla con ejemplos prácticos y ejercicios que permiten aplicar los conocimientos adquiridos en situaciones reales. Además, se enfatiza el uso de R como una herramienta de código abierto, destacando su flexibilidad, capacidad de extensión y su papel en la promoción de la reproducibilidad científica (Ihaka & Gentleman, 1996; R Core Team, 2023).\nA lo largo del manual, se presentan las principales características de R y su entorno de desarrollo integrado, RStudio, resaltando su utilidad tanto en proyectos académicos como profesionales. El texto está dirigido a estudiantes, investigadores y profesionales interesados en adquirir competencias en programación estadística, priorizando la claridad, la organización y la reproducibilidad en los análisis.",
    "crumbs": [
      "Introducción"
    ]
  },
  {
    "objectID": "index.html#organización-del-manual",
    "href": "index.html#organización-del-manual",
    "title": "Introducción al entorno de programación R y su aplicación en el análisis estadístico de datos",
    "section": "Organización del manual",
    "text": "Organización del manual\nEl contenido del manual se estructura de manera progresiva, iniciando con los aspectos más elementales y avanzando hacia herramientas y técnicas estadísticas de mayor complejidad. Cada sección incluye explicaciones detalladas, ejemplos prácticos y ejercicios diseñados para consolidar el aprendizaje. Asimismo, se incorporan recomendaciones y buenas prácticas que facilitan la asimilación de los conceptos y fomentan la reproducibilidad en los análisis realizados.\nEl manual se organiza en las siguientes secciones:\n\nIntroducción a R y RStudio: En este capítulo se presentan los conceptos básicos de R y RStudio, incluyendo sus características principales, el proceso de instalación y configuración, así como la preparación del entorno de trabajo para el análisis de datos.\nConceptos básicos de R: Se abordan los fundamentos esenciales del lenguaje R, tales como los primeros pasos en la consola, la estructura de los datos, la importación de información, el uso de operadores, funciones y la gestión de paquetes.\nManipulación de datos: Este apartado introduce las técnicas fundamentales para la manipulación de datos en R, utilizando tanto las herramientas base del lenguaje como los paquetes dplyr y tidyr, permitiendo transformar, organizar y preparar los datos para su análisis.\nVisualización de datos: Se exploran las distintas opciones para la visualización de datos, comenzando con las herramientas básicas de R y avanzando hacia la creación de gráficos personalizados mediante el paquete ggplot2, facilitando la interpretación y comunicación de los resultados.\nGestión y exportación de resultados: En este capítulo se explica cómo gestionar proyectos en R, exportar resultados de análisis, gráficos y tablas en formatos adecuados para su uso en informes y presentaciones, así como la integración con herramientas de control de versiones como Git y GitHub.\nReferencias: Se proporcionan las referencias bibliográficas empleadas para la construcción de este manual, que le permiten al lector profundizar en el aprendizaje y la aplicación de R en distintos contextos estadísticos.\nEjemplos de análisis estadístico con R: Finalmente, se desarrollan ejemplos detallados de análisis estadístico, incluyendo estadística descriptiva, regresión lineal simple y múltiple, mostrando la aplicación práctica de R en el análisis de datos reales.\n\nCada capítulo está diseñado para ser independiente, permitiendo que los lectores avancen a su propio ritmo y consulten las secciones según sus necesidades.",
    "crumbs": [
      "Introducción"
    ]
  },
  {
    "objectID": "index.html#pre-requisitos",
    "href": "index.html#pre-requisitos",
    "title": "Introducción al entorno de programación R y su aplicación en el análisis estadístico de datos",
    "section": "Pre requisitos",
    "text": "Pre requisitos\nEste manual no requiere conocimientos previos en programación ni en análisis estadístico. Está diseñado específicamente para principiantes, por lo que se parte desde cero, explicando cada concepto de manera clara y detallada. Todo lo que se necesita es:\n\nInterés por aprender: La curiosidad y disposición para explorar un nuevo lenguaje de programación.\nAcceso a una computadora: Con capacidad para instalar R y RStudio, herramientas que se explican paso a paso en el manual.\nPaciencia y práctica: Como cualquier habilidad nueva, aprender R requiere tiempo y dedicación. Los ejemplos y ejercicios incluidos están diseñados para facilitar este proceso.\n\nCon este enfoque, cualquier persona, independientemente de su experiencia previa, podrá utilizar este manual como una guía para iniciarse en el análisis estadístico con R.",
    "crumbs": [
      "Introducción"
    ]
  },
  {
    "objectID": "index.html#software-y-convenciones",
    "href": "index.html#software-y-convenciones",
    "title": "Introducción al entorno de programación R y su aplicación en el análisis estadístico de datos",
    "section": "Software y convenciones",
    "text": "Software y convenciones\nLa versión en línea de este manual está disponible en https://introduccion-r-cete.vercel.app/, y la fuente en español se encuentra alojada en el repositorio de GitHub https://github.com/Ludwing-MJ/introduccion_R_CETE. El desarrollo del manual se realizó utilizando Quarto, una herramienta que permite transformar archivos con extensión .qmd en formatos publicables como HTML, PDF y EPUB, facilitando la integración de código, resultados y texto en un solo documento reproducible.\nDurante la elaboración del manual se emplearon diversos paquetes del ecosistema de R, entre los que destacan knitr y bookdown, los cuales permiten combinar las ventajas de LaTeX y R para la generación de documentos dinámicos y reproducibles (Xie, 2015; Xie, 2024). Esta integración posibilita que los ejemplos de código y los resultados presentados sean fácilmente replicables por el lector.\nA lo largo del manual, se presentan fragmentos de código que pueden ser copiados y ejecutados directamente en la consola de R para obtener los mismos resultados que se muestran en el texto. Los bloques de código se destacan en recuadros similares al siguiente:\n\n4 + 6\na &lt;- c(1, 5, 6)\n5 * a\n1:10\n\nLos resultados generados por la ejecución de estos códigos se identifican con el numero uno encerrado entre cochetes ([1]) al inicio de cada línea, indicando que corresponden a la salida producida por R. Todo lo que comience con [1] representa resultados y no debe ser copiado como parte del código. Por ejemplo, al ejecutar el bloque anterior, se obtendrían los siguientes resultados:\n\n\n[1] 10\n\n\n[1]  5 25 30\n\n\n [1]  1  2  3  4  5  6  7  8  9 10\n\n\nPara garantizar la reproducibilidad y transparencia, se recomienda que el lector utilice versiones actualizadas de R y de los paquetes mencionados. La información sobre el entorno de desarrollo y las versiones de los paquetes utilizados en la construcción de este manual puede consultarse ejecutando el siguiente comando en R:\n\ndevtools::session_info()\n\nWarning in system2(\"quarto\", \"-V\", stdout = TRUE, env = paste0(\"TMPDIR=\", : el\ncomando ejecutado '\"quarto\"\nTMPDIR=C:/Users/FAUSAC/AppData/Local/Temp/RtmpyYY30y/file2144710a4e51 -V' tiene\nel estatus 1\n\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.4.3 (2025-02-28 ucrt)\n os       Windows 11 x64 (build 26100)\n system   x86_64, mingw32\n ui       RTerm\n language (EN)\n collate  Spanish_Guatemala.utf8\n ctype    Spanish_Guatemala.utf8\n tz       America/Guatemala\n date     2025-05-12\n pandoc   3.4 @ C:/Program Files/RStudio/resources/app/bin/quarto/bin/tools/ (via rmarkdown)\n quarto   NA @ C:\\\\PROGRA~1\\\\Quarto\\\\bin\\\\quarto.exe\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n cachem        1.1.0   2024-05-16 [1] CRAN (R 4.4.3)\n cli           3.6.5   2025-04-23 [1] CRAN (R 4.4.3)\n devtools      2.4.5   2022-10-11 [1] CRAN (R 4.4.3)\n digest        0.6.37  2024-08-19 [1] CRAN (R 4.4.3)\n ellipsis      0.3.2   2021-04-29 [1] CRAN (R 4.4.3)\n evaluate      1.0.3   2025-01-10 [1] CRAN (R 4.4.3)\n fastmap       1.2.0   2024-05-15 [1] CRAN (R 4.4.3)\n fs            1.6.6   2025-04-12 [1] CRAN (R 4.4.3)\n glue          1.8.0   2024-09-30 [1] CRAN (R 4.4.3)\n htmltools     0.5.8.1 2024-04-04 [1] CRAN (R 4.4.3)\n htmlwidgets   1.6.4   2023-12-06 [1] CRAN (R 4.4.3)\n httpuv        1.6.16  2025-04-16 [1] CRAN (R 4.4.3)\n jsonlite      2.0.0   2025-03-27 [1] CRAN (R 4.4.3)\n knitr         1.50    2025-03-16 [1] CRAN (R 4.4.3)\n later         1.4.2   2025-04-08 [1] CRAN (R 4.4.3)\n lifecycle     1.0.4   2023-11-07 [1] CRAN (R 4.4.3)\n magrittr      2.0.3   2022-03-30 [1] CRAN (R 4.4.3)\n memoise       2.0.1   2021-11-26 [1] CRAN (R 4.4.3)\n mime          0.13    2025-03-17 [1] CRAN (R 4.4.3)\n miniUI        0.1.2   2025-04-17 [1] CRAN (R 4.4.3)\n pkgbuild      1.4.7   2025-03-24 [1] CRAN (R 4.4.3)\n pkgload       1.4.0   2024-06-28 [1] CRAN (R 4.4.3)\n profvis       0.4.0   2024-09-20 [1] CRAN (R 4.4.3)\n promises      1.3.2   2024-11-28 [1] CRAN (R 4.4.3)\n purrr         1.0.4   2025-02-05 [1] CRAN (R 4.4.3)\n R6            2.6.1   2025-02-15 [1] CRAN (R 4.4.3)\n Rcpp          1.0.14  2025-01-12 [1] CRAN (R 4.4.3)\n remotes       2.5.0   2024-03-17 [1] CRAN (R 4.4.3)\n rlang         1.1.6   2025-04-11 [1] CRAN (R 4.4.3)\n rmarkdown     2.29    2024-11-04 [1] CRAN (R 4.4.3)\n rstudioapi    0.17.1  2024-10-22 [1] CRAN (R 4.4.3)\n sessioninfo   1.2.3   2025-02-05 [1] CRAN (R 4.4.3)\n shiny         1.10.0  2024-12-14 [1] CRAN (R 4.4.3)\n urlchecker    1.0.1   2021-11-30 [1] CRAN (R 4.4.3)\n usethis       3.1.0   2024-11-26 [1] CRAN (R 4.4.3)\n vctrs         0.6.5   2023-12-01 [1] CRAN (R 4.4.3)\n xfun          0.52    2025-04-02 [1] CRAN (R 4.4.3)\n xtable        1.8-4   2019-04-21 [1] CRAN (R 4.4.3)\n\n [1] C:/Users/FAUSAC/AppData/Local/R/win-library/4.4\n [2] C:/Program Files/R/R-4.4.3/library\n\n──────────────────────────────────────────────────────────────────────────────",
    "crumbs": [
      "Introducción"
    ]
  },
  {
    "objectID": "10.3_exportacion.html#importación-de-repositorios-de-github-para-trabajo-local-o-personal",
    "href": "10.3_exportacion.html#importación-de-repositorios-de-github-para-trabajo-local-o-personal",
    "title": "17  Uso de Git y GitHub en Proyectos de R",
    "section": "17.4 Importación de repositorios de GitHub para trabajo local o personal",
    "text": "17.4 Importación de repositorios de GitHub para trabajo local o personal\nImportar un repositorio de GitHub permite descargar una copia completa del proyecto para trabajar localmente, modificarlo o adaptarlo a nuevas necesidades. Este proceso se conoce como “clonar” un repositorio y es útil tanto para uso personal como para colaborar en proyectos de otros usuarios.\nPasos para clonar un repositorio de GitHub:\n1. Obtener la URL del repositorio\nEn la página del repositorio en GitHub, hacer clic en el botón “Code” y copiar la URL que aparece (por ejemplo, https://github.com/usuario/analisis_estadistico.git).\n2. Clonar el repositorio en la computadora local\nAbrir una terminal o la consola de RStudio y ejecutar el siguiente comando:\n\ngit clone https://github.com/usuario/analisis_estadistico.git\n\nEsto creará una carpeta local con todos los archivos y el historial del proyecto.\n3. Trabajar localmente\nUna vez clonado el repositorio, se puede abrir la carpeta en RStudio, modificar los archivos, ejecutar los scripts y exportar nuevos resultados. Si se tiene permiso para hacerlo, los cambios pueden subirse nuevamente a GitHub con los comandos git add, git commit y git push. Si el objetivo es solo uso personal, los cambios pueden mantenerse localmente sin necesidad de sincronizarlos con el repositorio remoto.\nVentajas de clonar repositorios:\n\nPermite reutilizar análisis existentes y aprender de otros proyectos.\nFacilita la colaboración en equipo, ya que todos los miembros trabajan con la misma versión del proyecto.\nAsegura la trazabilidad y la integridad del trabajo, ya que todo el historial de cambios se conserva.\n\nLa importación de repositorios es una práctica recomendada para quienes desean aprovechar recursos existentes, colaborar en proyectos abiertos o mantener una copia de seguridad de su trabajo (Bryan, 2018).",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Uso de Git y GitHub en Proyectos de R</span>"
    ]
  },
  {
    "objectID": "03_inicio.html#exploración-de-objetos-funciones-útiles",
    "href": "03_inicio.html#exploración-de-objetos-funciones-útiles",
    "title": "3  Primeros pasos en R",
    "section": "3.5 Exploración de objetos: funciones útiles",
    "text": "3.5 Exploración de objetos: funciones útiles\nUna vez creados los objetos, es importante poder identificar su naturaleza y estructura. R ofrece varias funciones para este propósito:\n\nclass(): Devuelve la clase del objeto, como “numeric”, “character”, “factor” o “logical”.\ntypeof(): Indica el tipo interno de almacenamiento del objeto, como “double”, “integer” o “character”.\nstr(): Muestra la estructura interna del objeto, proporcionando un resumen compacto de su contenido.\nlevels(): Específica para factores, devuelve los niveles o categorías posibles del objeto.\n\nEjemplo de uso de estas funciones:\n\n# Exploración de un objeto numérico\nclass(x)      # Devuelve \"numeric\"\n\n[1] \"numeric\"\n\ntypeof(x)     # Devuelve \"double\"\n\n[1] \"double\"\n\nstr(x)        # Muestra la estructura del objeto\n\n num 10\n\n# Exploración de un objeto de texto\nclass(nombre) # Devuelve \"character\"\n\n[1] \"character\"\n\nstr(nombre)   # Muestra la estructura del objeto\n\n chr \"Juan\"\n\n# Exploración de un factor\nestado_civil &lt;- factor(\"soltero\", levels = c(\"soltero\", \"casado\", \"divorciado\"))\nclass(estado_civil)   # Devuelve \"factor\"\n\n[1] \"factor\"\n\ntypeof(estado_civil)  # Devuelve \"integer\"\n\n[1] \"integer\"\n\nstr(estado_civil)     # Muestra la estructura del factor\n\n Factor w/ 3 levels \"soltero\",\"casado\",..: 1\n\nlevels(estado_civil)  # Devuelve los niveles posibles\n\n[1] \"soltero\"    \"casado\"     \"divorciado\"",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Primeros pasos en R</span>"
    ]
  },
  {
    "objectID": "05_importacion.html#verificación-de-la-importación-de-datos",
    "href": "05_importacion.html#verificación-de-la-importación-de-datos",
    "title": "5  Importación de datos",
    "section": "5.3 Verificación de la importación de datos",
    "text": "5.3 Verificación de la importación de datos\nLa verificación de la importación de datos representa una fase crítica en el proceso de análisis estadístico, ya que permite identificar y corregir posibles inconsistencias, errores de formato o problemas de codificación que puedan afectar la calidad y la validez de los resultados. Una revisión exhaustiva de los datos importados contribuye a la reproducibilidad y confiabilidad de los análisis, además de optimizar el flujo de trabajo y prevenir dificultades en etapas posteriores (Grolemund & Wickham, 2017; R Core Team, 2023).\n\n5.3.1 Inspección preliminar de los datos\nLa inspección preliminar consiste en una revisión rápida del contenido y la estructura del conjunto de datos recién importado. R proporciona funciones específicas para este propósito:\n\nhead(): Permite visualizar las primeras filas del data frame, facilitando la comprobación de la correcta lectura de los encabezados, la alineación de las columnas y la presencia de datos esperados.\ntail(): Muestra las últimas filas del conjunto de datos, útil para verificar la integridad de los registros al final del archivo.\ndim(): Informa sobre el número de filas y columnas, lo que ayuda a confirmar que la cantidad de observaciones y variables coincide con lo esperado (Venables & Ripley, 2002).\n\n\n\n5.3.2 Evaluación de la estructura y los tipos de variables\nLa correcta interpretación de los tipos de variables es fundamental para evitar errores en el análisis estadístico. R ofrece herramientas para examinar la estructura interna del objeto de datos:\n\nstr(): Proporciona información detallada sobre el tipo de cada variable (numérica, carácter, factor, etc.), la cantidad de observaciones y la organización de las columnas. Esta función resulta esencial para detectar conversiones automáticas no deseadas, como la transformación de variables numéricas en factores o viceversa (Grolemund & Wickham, 2017).\nnames(): Permite consultar los nombres de las columnas, lo que facilita la identificación de posibles errores en la lectura de los encabezados o la presencia de nombres duplicados.\n\n\n\n5.3.3 Resumen estadístico y detección de inconsistencias\nEl análisis exploratorio inicial incluye la obtención de resúmenes estadísticos básicos, que permiten identificar valores atípicos, rangos inusuales o la presencia de datos faltantes:\n\nsummary(): Genera un resumen estadístico para cada variable, incluyendo medidas de tendencia central, dispersión y frecuencia de valores nulos. Esta función resulta útil para detectar anomalías y orientar las primeras etapas de limpieza de datos (Wickham & Bryan, 2023).\ntable(): Permite examinar la frecuencia de los valores en variables categóricas, facilitando la identificación de categorías inesperadas o errores de codificación.\n\n\n\n5.3.4 Verificación de la codificación de caracteres\nEn contextos donde los datos contienen caracteres especiales, como tildes o la letra “ñ”, es fundamental asegurarse de que la codificación utilizada durante la importación sea la adecuada. El argumento encoding en funciones como read.csv() permite especificar la codificación, por ejemplo, \"UTF-8\", para evitar la aparición de símbolos incorrectos o pérdida de información (R Core Team, 2023).\n\n\n5.3.5 Identificación de valores faltantes y duplicados\nLa presencia de valores faltantes o registros duplicados puede afectar la validez de los análisis. R ofrece funciones para detectar y cuantificar estos casos:\n\nis.na(): Permite identificar valores ausentes en el conjunto de datos.\nanyDuplicated(): Informa sobre la existencia de filas duplicadas, lo que resulta relevante para garantizar la unicidad de las observaciones (Venables & Ripley, 2002).\n\n\n\n5.3.6 Importancia de la verificación sistemática\nLa aplicación sistemática de estas herramientas y procedimientos permite detectar de manera temprana problemas que, de no ser corregidos, pueden comprometer la interpretación y la validez de los resultados. La verificación de la importación de datos debe considerarse una buena práctica en la gestión de información y un paso indispensable en cualquier proyecto de análisis estadístico (Bryan, 2018; R Core Team, 2023).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Importación de datos</span>"
    ]
  },
  {
    "objectID": "06.1_funciones.html#creación-de-funciones-en-r-sintaxis-y-elementos-fundamentales",
    "href": "06.1_funciones.html#creación-de-funciones-en-r-sintaxis-y-elementos-fundamentales",
    "title": "7  Funciones en R",
    "section": "7.3 Creación de funciones en R: Sintaxis y elementos fundamentales",
    "text": "7.3 Creación de funciones en R: Sintaxis y elementos fundamentales\nLa definición de funciones en R se realiza mediante una sintaxis clara y estructurada, lo que facilita la creación de procedimientos personalizados para resolver tareas específicas. Comprender la estructura básica de una función es fundamental para aprovechar al máximo la modularidad y reutilización del código en R (R Core Team, 2023; Wickham & Grolemund, 2017).\n\n7.3.1 Sintaxis general de una función en R\nLa sintaxis básica para crear una función en R consiste en asignar un nombre descriptivo a la función y utilizar la palabra reservada function, seguida de una lista de argumentos entre paréntesis. El cuerpo de la función, delimitado por llaves, contiene las instrucciones y operaciones que se ejecutarán al llamar a la función. El valor de retorno se especifica mediante la instrucción return(), aunque si no se utiliza explícitamente, R devolverá automáticamente el último valor evaluado en el cuerpo de la función. La estructura general es la siguiente:\n\nnombre_funcion &lt;- function(argumento1, argumento2, ...) {\n  # Instrucciones y operaciones\n  return(resultado)\n}\n\n\n\n7.3.2 Elementos clave de una función\nCada función en R se compone de los siguientes elementos:\n\nNombre de la función: que debe ser descriptivo y reflejar claramente la tarea que realiza.\nArgumentos: representan los valores de entrada requeridos por la función. Es posible asignar valores por defecto a estos argumentos para hacer la función más flexible.\nCuerpo de la función: contiene la lógica y las operaciones principales, y puede incluir validaciones y manejo de errores para asegurar la robustez del código.\nValor de retorno: es el resultado que la función entrega tras la ejecución de sus operaciones; este valor puede ser un dato simple o una estructura más compleja, dependiendo del propósito de la función.\n\n\n\n7.3.3 Ejemplo de función personalizada\nA continuación, se muestra un ejemplo de una función personalizada que convierte temperaturas de grados Celsius a Fahrenheit:\n\n# Definición de la función para convertir temperaturas de Celsius a Fahrenheit\ncelsius_a_fahrenheit &lt;- function(celsius) {\n    # Validación del tipo de dato del argumento de entrada\n    if (!is.numeric(celsius)) {\n        # Si el argumento no es numérico, \n        # detiene la ejecución y muestra un mensaje de error \n        stop(\"El argumento 'celsius' debe ser numérico\")\n    }\n    # Cálculo de la conversión de Celsius a Fahrenheit\n    fahrenheit &lt;- (celsius * 9/5) + 32\n    # Devuelve el resultado de la conversión\n    return(fahrenheit)\n}\n\n# Ejemplo de uso de la función\n# Se convierte 25 grados Celsius a Fahrenheit\ntemperatura_celsius &lt;- 25\nresultado &lt;- celsius_a_fahrenheit(temperatura_celsius)\n\nEsta función demuestra varios conceptos importantes en la programación con R:\n\nValidación de datos: La función verifica que el argumento de entrada sea del tipo correcto antes de realizar los cálculos, lo que previene errores y mejora la robustez del código.\nClaridad en la estructura: La función sigue una estructura lógica clara: validación, cálculo y retorno del resultado.\nDocumentación interna: Los comentarios explican el propósito de cada sección del código, facilitando su comprensión y mantenimiento.\nReutilización: La función puede ser utilizada con diferentes valores de entrada, incluyendo vectores de temperaturas, gracias a la vectorización inherente de R.\n\nPara ilustrar la versatilidad de la función, se puede utilizar con múltiples valores simultáneamente:\n\n# Ejemplo de uso con múltiples temperaturas\ntemperaturas_celsius &lt;- c(0, 25, 100)\ntemperaturas_fahrenheit &lt;- celsius_a_fahrenheit(temperaturas_celsius)\ntemperaturas_fahrenheit # Vector con los resultados:\n\n[1]  32  77 212\n\n\nLa inclusión de comentarios detallados y ejemplos de uso hace que el código sea más accesible para otros usuarios y facilita su mantenimiento a largo plazo, aspectos fundamentales en el desarrollo de software científico y análisis de datos (R Core Team, 2023).",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Funciones en R</span>"
    ]
  },
  {
    "objectID": "07_paquetes.html#fundamentos-de-los-paquetes-en-r-definición-y-alcance",
    "href": "07_paquetes.html#fundamentos-de-los-paquetes-en-r-definición-y-alcance",
    "title": "8  Paquetes en R: Expansión y Modularidad del Entorno",
    "section": "",
    "text": "8.1.1 Atributos Distintivos de los Paquetes en R\nLos paquetes en R presentan una serie de atributos fundamentales que los distinguen y potencian su utilidad:\n\nFunciones Especializadas: Cada paquete contiene funciones diseñadas para abordar tareas concretas, como la generación de gráficos, la realización de análisis estadísticos avanzados o la manipulación de grandes volúmenes de datos.\nDocumentación Integral: Los paquetes incluyen documentación detallada que describe el propósito de cada función, sus argumentos y ejemplos de uso, lo que facilita el aprendizaje autónomo y la correcta aplicación de las herramientas.\nConjuntos de Datos Ilustrativos: Muchos paquetes incorporan conjuntos de datos de ejemplo, que permiten a los usuarios practicar y comprender la funcionalidad ofrecida, así como reproducir ejemplos y casos de estudio presentados en la documentación (Grolemund & Wickham, 2017; Xie, Allaire & Grolemund, 2018).\n\n\n\n8.1.2 Relevancia Estratégica del Uso de Paquetes en R: Beneficios y Aplicaciones\nEl aprovechamiento de los paquetes es esencial para explotar al máximo el potencial de R, ya que proporcionan extensibilidad, eficiencia y especialización. Los paquetes permiten realizar tareas que no están disponibles en el entorno base, simplifican procesos complejos y ofrecen soluciones adaptadas a necesidades específicas en disciplinas como la agronomía, la biología, la economía y muchas otras. La existencia de una comunidad activa y colaborativa asegura la actualización constante y el soporte de una amplia variedad de paquetes, lo que contribuye a mantener a R como una herramienta de referencia en el análisis de datos y la investigación reproducible (Wickham & Grolemund, 2017; R Core Team, 2023). Esta dinámica de desarrollo y mantenimiento colectivo fomenta la innovación y la rápida incorporación de nuevas metodologías y tecnologías en el ecosistema de R.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Paquetes en R: Expansión y Modularidad del Entorno</span>"
    ]
  },
  {
    "objectID": "07_paquetes.html#gestión-de-paquetes-en-r-instalación-y-carga",
    "href": "07_paquetes.html#gestión-de-paquetes-en-r-instalación-y-carga",
    "title": "8  Paquetes en R: Expansión y Modularidad del Entorno",
    "section": "8.2 Gestión de Paquetes en R: Instalación y Carga",
    "text": "8.2 Gestión de Paquetes en R: Instalación y Carga\nLa gestión de paquetes en R es un proceso esencial para acceder a herramientas especializadas y ampliar las capacidades del entorno base. La mayoría de los paquetes se obtienen desde CRAN (Comprehensive R Archive Network), el repositorio oficial que alberga una amplia variedad de recursos para diferentes áreas de aplicación (R Core Team, 2023). Este proceso asegura que los usuarios puedan acceder a las funcionalidades necesarias para sus análisis y proyectos.\n\n8.2.1 Procedimiento de Instalación de Paquetes\nPara instalar un paquete desde CRAN, se utiliza la función install.packages(). Este proceso descarga e instala el paquete y sus dependencias en el sistema. Por ejemplo, para instalar el paquete ggplot2, ampliamente utilizado para la visualización de datos, se emplea la siguiente instrucción:\n\n# Instalación del paquete ggplot2\ninstall.packages(\"ggplot2\")\n\nLa instalación de un paquete es un proceso que solo debe realizarse una vez en el sistema, a menos que se requiera una versión específica o se reinstale el sistema operativo.\n\n\n8.2.2 Activación de Paquetes: Carga en la Sesión de Trabajo\nDespués de instalar un paquete, es necesario cargarlo en cada nueva sesión de trabajo para poder utilizar sus funciones. Esto se realiza mediante la función library():\n\n# Cargar el paquete ggplot2\nlibrary(ggplot2)\n\nLa carga de paquetes debe repetirse cada vez que se inicia una nueva sesión en R, ya que los paquetes no se cargan automáticamente al abrir el entorno. Este paso es crucial para que las funciones y los datos del paquete estén disponibles para su uso.\n\n\n8.2.3 Automatización de la instalación y carga\nPara asegurar que un paquete esté disponible y evitar errores al compartir scripts, es recomendable automatizar el proceso de verificación, instalación y carga. La siguiente estructura permite comprobar si el paquete está instalado y, en caso contrario, instalarlo y cargarlo automáticamente:\n\n# Verificar e instalar automáticamente un paquete\nif (!require(\"ggplot2\")) install.packages(\"ggplot2\")\n\nEste enfoque contribuye a la reproducibilidad del código y facilita el intercambio de scripts entre usuarios, garantizando que todas las dependencias necesarias estén disponibles en el entorno de trabajo (R Core Team, 2023).\n\n\n8.2.4 Alternativas para la Gestión de Paquetes\nAdemás de las funciones básicas install.packages() y library(), existen alternativas para la gestión de paquetes que ofrecen funcionalidades adicionales:\n\npacman: Este paquete simplifica la instalación y carga de múltiples paquetes con una sintaxis más concisa. Por ejemplo, p_load(ggplot2, dplyr) instala y carga ambos paquetes simultáneamente.\nrenv: Este paquete permite crear entornos de proyectos reproducibles, registrando las versiones exactas de los paquetes utilizados. Esto asegura que el código funcione correctamente incluso en diferentes sistemas o en el futuro.\ndevtools: Este paquete facilita la instalación de paquetes desde GitHub u otras fuentes no oficiales, lo que es útil para acceder a versiones en desarrollo o paquetes personalizados.\n\nEl uso de estas herramientas puede mejorar significativamente la eficiencia y la reproducibilidad en el manejo de paquetes en R (R Core Team, 2023).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Paquetes en R: Expansión y Modularidad del Entorno</span>"
    ]
  },
  {
    "objectID": "07_paquetes.html#paquetes-recomendados-para-tareas-específicas-en-r",
    "href": "07_paquetes.html#paquetes-recomendados-para-tareas-específicas-en-r",
    "title": "8  Paquetes en R: Expansión y Modularidad del Entorno",
    "section": "8.3 Paquetes recomendados para tareas específicas en R",
    "text": "8.3 Paquetes recomendados para tareas específicas en R\nEn el contexto del análisis estadístico y la manipulación de datos, R dispone de una amplia variedad de paquetes que optimizan tareas especializadas y permiten realizar análisis complejos de manera eficiente. La selección adecuada de paquetes facilita la automatización de procesos, la obtención de resultados reproducibles y la adaptación a diferentes áreas de aplicación (R Core Team, 2023).\nA continuación, se presenta una clasificación de los paquetes más relevantes, organizados por su área de aplicación y acompañados de una breve descripción de sus funcionalidades principales:\n\n\n\n\n\n\n\n\nÁrea\nPaquete\nDescripción\n\n\n\n\nManipulación de datos\ndplyr\nFacilita la transformación y manipulación de datos mediante funciones intuitivas\n\n\n\ntidyr\nPermite reorganizar datos entre formatos ancho y largo\n\n\n\ndata.table\nOptimizado para el manejo de grandes conjuntos de datos\n\n\nAnálisis exploratorio\nDataExplorer\nAutomatiza el análisis exploratorio de datos\n\n\n\nsummarytools\nGenera resúmenes estadísticos detallados\n\n\n\npsych\nProporciona funciones para análisis psicométricos y estadística descriptiva\n\n\nAnálisis estadístico\nstats\nIncluye funciones base para pruebas estadísticas comunes\n\n\n\nagricolae\nEspecializado en diseños experimentales y análisis agrícolas\n\n\n\nAgroR\nProporciona funciones y herramientas para análisis estadísticos en agronomía\n\n\n\ncar\nFacilita análisis de regresión avanzados\n\n\nVisualización\nggplot2\nPermite crear gráficos personalizados de alta calidad\n\n\n\nplotly\nGenera gráficos interactivos\n\n\n\nEsta clasificación permite identificar rápidamente los paquetes más adecuados para cada etapa del análisis de datos, desde la manipulación inicial hasta la visualización y el análisis especializado.\n\n8.3.1 Instalación y carga de paquetes esenciales\nPara facilitar el inicio de un proyecto de análisis estadístico, se recomienda instalar y cargar un conjunto básico de paquetes que cubran las principales necesidades de manipulación, exploración, análisis y visualización de datos. El siguiente código muestra cómo automatizar este proceso:\n\n# Paquetes para manipulación y visualización de datos\nif (!require(\"tidyverse\")) install.packages(\"tidyverse\")\nif (!require(\"data.table\")) install.packages(\"data.table\")\n\n# Paquetes para análisis exploratorio\nif (!require(\"DataExplorer\")) install.packages(\"DataExplorer\")\nif (!require(\"psych\")) install.packages(\"psych\")\n\n# Paquetes para análisis estadísticos especializados\nif (!require(\"agricolae\")) install.packages(\"agricolae\")\nif (!require(\"AgroR\")) install.packages(\"AgroR\")\nif (!require(\"car\")) install.packages(\"car\")\n\n# Paquetes para manejo de archivos\nif (!require(\"readxl\")) install.packages(\"readxl\")\nif (!require(\"writexl\")) install.packages(\"writexl\")\n\nEste conjunto de instrucciones garantiza que los paquetes esenciales estén disponibles en el entorno de trabajo, contribuyendo a la reproducibilidad y facilitando el intercambio de scripts entre usuarios. Además, la automatización de la instalación y carga de paquetes minimiza errores y asegura que todas las dependencias necesarias se encuentren correctamente configuradas (R Core Team, 2023).",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Paquetes en R: Expansión y Modularidad del Entorno</span>"
    ]
  },
  {
    "objectID": "08.3_manipulacion.html#configuración-del-entorno-y-datos-de-ejemplo",
    "href": "08.3_manipulacion.html#configuración-del-entorno-y-datos-de-ejemplo",
    "title": "11  Manipulación de datos con dplyr y tidyr",
    "section": "11.2 Configuración del Entorno y Datos de Ejemplo",
    "text": "11.2 Configuración del Entorno y Datos de Ejemplo\nSe emplea el mismo conjunto de datos simulado del experimento agrícola utilizado en el capítulo anterior, lo que permite comparar directamente los enfoques de R base y tidyverse.\n\n# Configuración inicial del entorno de análisis\nlibrary(dplyr)    # Carga del paquete para manipulación de datos\nlibrary(tidyr)    # Carga del paquete para reorganización de datos\n\n# Creación de datos simulados para experimento agrícola\nset.seed(123)     # Establecimiento de semilla para reproducibilidad\n\ndatos_cultivo &lt;- data.frame(\n    parcela = 1:20,    # Identificador único de cada parcela\n    tratamiento = rep(c(\"Control\", \"Fertilizante A\",\n                       \"Fertilizante B\", \"Fertilizante C\"), each = 5),\n    bloque = rep(1:5, times = 4),    # Estructura de bloques\n    altura_cm = round(rnorm(20, mean = 65, sd = 10), 1),    # Variable respuesta 1\n    peso_gr = round(rnorm(20, mean = 120, sd = 25), 1),     # Variable respuesta 2\n    daño_plaga = sample(c(\"Alto\", \"Medio\", \"Bajo\"), 20, replace = TRUE),\n    fecha_siembra = as.Date(\"2024-01-01\") + sample(1:10, 20, replace = TRUE)\n)\n\n# Introducción de valores faltantes para ejemplos didácticos\ndatos_cultivo$altura_cm[c(3, 15)] &lt;- NA\ndatos_cultivo$peso_gr[c(7, 18)] &lt;- NA\n\nEl conjunto de datos simulado representa un experimento agrícola con un diseño de bloques completamente aleatorizado. Los datos incluyen mediciones de altura y peso de plantas bajo diferentes tratamientos de fertilización, organizados en bloques para controlar la variabilidad ambiental. La estructura del experimento sigue los principios fundamentales del diseño experimental descritos por Montgomery et al. (2012), donde el control local mediante bloques permite una estimación más precisa de los efectos de los tratamientos.\nLas variables incluidas en el conjunto de datos son:\n\nparcela: Identificador único de cada unidad experimental\ntratamiento: Factor experimental con cuatro niveles (Control y tres tipos de fertilizantes)\nbloque: Factor de control local con cinco niveles\naltura_cm: Variable respuesta que mide el crecimiento vertical de las plantas\npeso_gr: Variable respuesta que mide la biomasa de las plantas\ndaño_plaga: Evaluación categórica del daño por plagas\nfecha_siembra: Registro temporal de la implementación del experimento\n\nLa inclusión deliberada de valores faltantes en las variables de respuesta (altura_cm y peso_gr) permite ilustrar técnicas comunes de manejo de datos incompletos, una situación frecuente en experimentos agrícolas (Field, 2013).",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Manipulación de datos con dplyr y tidyr</span>"
    ]
  },
  {
    "objectID": "09.2_visualizacion.html#arquitectura-del-sistema-gráfico-base",
    "href": "09.2_visualizacion.html#arquitectura-del-sistema-gráfico-base",
    "title": "13  Sistema Gráfico Base de R",
    "section": "",
    "text": "Modularidad: cada elemento (ejes, marcas, títulos, objetos geométricos) puede añadirse o modificarse sin rehacer el gráfico desde cero.\nJerarquía: los componentes se dibujan en capas sucesivas sobre un “lienzo” inicial.\nPersistencia: las modificaciones se aplican sobre el dispositivo gráfico activo hasta que este se cierra o se restablecen los parámetros originales (Murrell, 2018).",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Sistema Gráfico Base de R</span>"
    ]
  },
  {
    "objectID": "09.2_visualizacion.html#funciones-esenciales-para-la-exploración-de-datos",
    "href": "09.2_visualizacion.html#funciones-esenciales-para-la-exploración-de-datos",
    "title": "13  Sistema Gráfico Base de R",
    "section": "13.3 Funciones esenciales para la exploración de datos",
    "text": "13.3 Funciones esenciales para la exploración de datos\nLa exploración visual de los datos es una etapa fundamental en cualquier análisis estadístico, ya que permite identificar patrones, tendencias, anomalías y posibles errores en los datos antes de aplicar modelos formales. El sistema gráfico base de R proporciona funciones versátiles y personalizables para la creación de gráficos exploratorios, facilitando la comprensión y la comunicación de los resultados (Venables & Ripley, 2002; Murrell, 2018).\n\n13.3.1 Histogramas\nEl histograma es una herramienta gráfica que permite representar la distribución de una variable numérica, facilitando la identificación de asimetrías, curtosis, valores atípicos y posibles multimodalidades (Cleveland, 1993). En R, la función principal para crear histogramas es hist().\nSintaxis general:\n\nhist(x, \n     breaks = \"Sturges\", \n     freq = TRUE, \n     col = NULL, \n     border = NULL, \n     main = NULL, \n     xlab = NULL, \n     ylab = NULL, \n     ...)\n\nExplicación de los argumentos principales:\n\nx: Vector numérico con los datos a graficar.\nbreaks: Define el número de intervalos (bins) o el método para calcularlos. Puede ser un número, un vector de puntos de corte, o un método como “Sturges”, “Scott”, “FD”.\nfreq: Si es TRUE, el eje Y muestra frecuencias absolutas; si es FALSE, muestra densidades.\ncol: Color de las barras.\nborder: Color del borde de las barras.\nmain: Título principal del gráfico.\nxlab, ylab: Etiquetas de los ejes X e Y.\n...: Otros argumentos gráficos adicionales.\n\nEjemplo:\n\n# Simulación de datos: calificaciones de 200 estudiantes\nset.seed(123)\nnotas &lt;- rnorm(200, mean = 70, sd = 10)\n\n# Creación de un histograma personalizado\nhist(notas,\n     breaks = 15,                # Número de intervalos (bins)\n     freq = TRUE,                # Mostrar frecuencias absolutas en el eje Y\n     col = \"lightblue\",          # Color de las barras\n     border = \"darkblue\",        # Color del borde de las barras\n     main = \"Histograma de calificaciones\", # Título principal\n     xlab = \"Puntaje\",           # Etiqueta del eje X\n     ylab = \"Frecuencia\")        # Etiqueta del eje Y\n\n\n\n\n\n\n\n\nLa elección del número de intervalos (breaks) es crucial para evitar interpretaciones erróneas: intervalos muy amplios pueden ocultar detalles importantes, mientras que intervalos muy estrechos pueden generar ruido visual (Venables & Ripley, 2002).\n\n\n13.3.2 Diagramas de caja (boxplots)\nEl diagrama de caja, o boxplot, es una herramienta gráfica que resume la dispersión, la mediana y la presencia de valores atípicos en una o varias muestras. Es especialmente útil para comparar grupos y detectar asimetrías (Tukey, 1977).\nSintaxis general:\n\nboxplot(formula, \n        data = NULL, \n        main = NULL, \n        xlab = NULL, \n        ylab = NULL, \n        col = NULL, \n        border = NULL, \n        notch = FALSE, \n        outline = TRUE, \n        ...)\n\n\nformula: Expresión del tipo y ~ grupo para comparar grupos.\ndata: Data frame donde buscar las variables.\nmain, xlab, ylab: Títulos y etiquetas.\ncol: Colores de las cajas.\nborder: Color del borde de las cajas.\nnotch: Si es TRUE, añade una muesca para comparar medianas.\noutline: Si es TRUE, muestra valores atípicos.\n...: Otros argumentos gráficos.\n\nEjemplo:\n\n# Simulación de datos para dos grupos\nset.seed(123)\ngrupo &lt;- factor(rep(c(\"Control\", \"Tratamiento\"), each = 100))\nvalores &lt;- c(rnorm(100, 70, 8), rnorm(100, 75, 10))\n\n# Creación de un boxplot personalizado\nboxplot(valores ~ grupo,\n        main = \"Comparación entre Grupos\",\n        xlab = \"Grupo\",\n        ylab = \"Valores\",\n        col = c(\"lightgreen\", \"lightcoral\"), # Colores para cada grupo\n        border = \"darkgray\",                 # Color del borde\n        notch = TRUE,                        # Muesca para comparar medianas\n        outline = TRUE)                      # Mostrar valores atípicos\n\n\n\n\n\n\n\n\nLa muesca en el boxplot ayuda a comparar visualmente las medianas: si las muescas no se superponen, existe evidencia de diferencia significativa entre los grupos (Murrell, 2018).\n\n\n13.3.3 Gráficos de dispersión\nEl gráfico de dispersión es fundamental para analizar la relación entre dos variables cuantitativas, permitiendo identificar tendencias lineales, no lineales, agrupamientos y valores atípicos (Cleveland, 1993).\nSintaxis general:\n\nplot(x, y, \n     type = \"p\", \n     main = NULL, \n     sub = NULL, \n     xlab = NULL, \n     ylab = NULL, \n     pch = 1, \n     col = NULL, \n     cex = 1, \n     ...)\n\nExplicación de los argumentos principales:\n\nx, y: Vectores numéricos de igual longitud.\ntype: Tipo de gráfico (“p” para puntos, “l” para líneas, “b” para ambos).\nmain, sub: Título principal y subtítulo.\nxlab, ylab: Etiquetas de los ejes.\npch: Tipo de símbolo para los puntos (1: círculo, 16: círculo sólido, 17: triángulo, etc.).\ncol: Color de los puntos.\ncex: Tamaño relativo de los puntos.\n...: Otros argumentos gráficos.\n\nEjemplo:\n\n# Simulación de datos correlacionados\nset.seed(123)\nx &lt;- rnorm(100, mean = 10, sd = 2)\ny &lt;- 2 * x + rnorm(100, 0, 3)\n\n# Gráfico de dispersión personalizado\nplot(x, y,\n     type = \"p\",                  # Tipo de gráfico: puntos\n     main = \"Relación entre X e Y\",\n     sub = \"Datos simulados\",\n     xlab = \"Variable X\",\n     ylab = \"Variable Y\",\n     pch = 16,                    # Círculo sólido\n     col = \"navy\",                # Color de los puntos\n     cex = 1.2)                   # Tamaño de los puntos\n\n# Añadir línea de regresión lineal\nabline(lm(y ~ x), col = \"red\", lwd = 2, lty = 2)\n\n\n\n\n\n\n\n\nLa adición de la línea de regresión ayuda a identificar la dirección y fuerza de la relación entre las variables (Venables & Ripley, 2002).\n\n\n13.3.4 Gráficos de líneas\nLos gráficos de líneas son ideales para representar la evolución de una variable a lo largo del tiempo o en función de un orden secuencial, permitiendo detectar tendencias, ciclos y cambios abruptos (Murrell, 2018).\n\nplot(x, y, \n     type = \"l\", \n     main = NULL, \n     xlab = NULL, \n     ylab = NULL, \n     col = NULL, \n     lwd = 1, \n     ...)\n\n\ntype = \"l\": Dibuja una línea.\nlwd: Grosor de la línea.\n\nEjemplo:\n\n# Simulación de una serie temporal\nset.seed(123)\ntiempo &lt;- 1:50\nmedidas &lt;- cumsum(rnorm(50))\n\n# Gráfico de líneas\nplot(tiempo, medidas,\n     type = \"l\",                  # Tipo de gráfico: línea\n     main = \"Serie temporal simulada\",\n     xlab = \"Tiempo\",\n     ylab = \"Medida\",\n     col = \"darkred\",\n     lwd = 2)                     # Grosor de la línea\n\n# Añadir puntos sobre la línea para enfatizar cada observación\npoints(tiempo, medidas, pch = 16, col = \"black\")\n\n\n\n\n\n\n\n\nLa combinación de líneas y puntos facilita la identificación de valores individuales y la tendencia global de la serie.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Sistema Gráfico Base de R</span>"
    ]
  },
  {
    "objectID": "09.3_visualizacion.html#ventajas-principales-de-ggplot2",
    "href": "09.3_visualizacion.html#ventajas-principales-de-ggplot2",
    "title": "14  Visualización de datos con ggplot2",
    "section": "",
    "text": "Modularidad y gramática de gráficos: ggplot2 está basado en la “gramática de los gráficos” (Grammar of Graphics), lo que permite construir visualizaciones a partir de componentes independientes: datos, mapeos estéticos, geometrías, escalas, temas y capas adicionales. Esta modularidad facilita la creación de gráficos complejos de manera incremental, permitiendo añadir o modificar elementos sin rehacer el gráfico desde cero (Wilkinson, 2005; Wickham, 2016).\nFlexibilidad y personalización: A diferencia del sistema gráfico base de R, ggplot2 ofrece una amplia gama de opciones para personalizar cada aspecto del gráfico, desde los colores y tipos de símbolos hasta la disposición de leyendas, títulos y escalas. Esta flexibilidad es fundamental para adaptar las visualizaciones a los estándares de publicaciones científicas y a las necesidades específicas de cada análisis (Wickham, 2016).\nResultados visuales profesionales: Los gráficos generados con ggplot2 presentan una estética cuidada y profesional por defecto, lo que facilita su inclusión directa en artículos científicos, reportes técnicos y presentaciones académicas. Además, la posibilidad de aplicar temas predefinidos o personalizados permite mantener la coherencia visual en todos los productos gráficos de un proyecto (Wickham, 2016).\nReproducibilidad y transparencia: La sintaxis declarativa de ggplot2 favorece la reproducibilidad de los análisis, ya que cada gráfico puede ser reconstruido exactamente a partir del código utilizado. Esto es especialmente relevante en la investigación científica, donde la transparencia y la replicabilidad son principios fundamentales (Wickham et al., 2019).\nIntegración con el ecosistema tidyverse: ggplot2 forma parte del tidyverse, un conjunto de paquetes diseñados para el manejo, transformación y modelado de datos en R. Esta integración permite una transición fluida desde la manipulación de datos hasta la visualización, optimizando el flujo de trabajo y reduciendo la posibilidad de errores (Wickham et al., 2019).\nComunidad activa y abundancia de recursos: La amplia adopción de ggplot2 ha dado lugar a una comunidad activa de usuarios y desarrolladores, lo que se traduce en una gran cantidad de recursos, tutoriales, ejemplos y extensiones disponibles para resolver dudas y ampliar las capacidades del paquete.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Visualización de datos con ggplot2</span>"
    ]
  },
  {
    "objectID": "09.3_visualizacion.html#gramática-de-los-gráficos-en-ggplot2",
    "href": "09.3_visualizacion.html#gramática-de-los-gráficos-en-ggplot2",
    "title": "14  Visualización de datos con ggplot2",
    "section": "14.2 Gramática de los Gráficos en ggplot2",
    "text": "14.2 Gramática de los Gráficos en ggplot2\nLa visualización de datos es una etapa fundamental en el análisis estadístico, ya que permite identificar patrones, tendencias y anomalías que pueden pasar desapercibidos en una inspección numérica (Cleveland, 1993; Tufte, 2001). En este contexto, ggplot2 se destaca por su enfoque basado en la “gramática de los gráficos” (Grammar of Graphics), un marco conceptual que facilita la construcción de visualizaciones claras, reproducibles y adaptadas a los estándares de la comunicación científica (Wilkinson, 2005; Wickham, 2016).\n\n14.2.1 Principios conceptuales de la gramática de los gráficos\nLa gramática de los gráficos, propuesta originalmente por Wilkinson (2005), parte de la premisa de que toda visualización estadística puede descomponerse en un conjunto de componentes básicos. Este enfoque modular permite construir gráficos complejos a partir de la combinación sistemática de elementos independientes, lo que resulta especialmente útil para quienes se inician en la programación estadística, ya que reduce la complejidad y favorece la comprensión progresiva del proceso de visualización (Wickham, 2016).\nSegún Cleveland (1993), la claridad y la precisión en la representación gráfica son esenciales para evitar interpretaciones erróneas y comunicar los resultados de manera efectiva. Por ello, la gramática de los gráficos enfatiza la importancia de definir explícitamente cada elemento visual, asegurando que el gráfico resultante sea informativo y estéticamente adecuado (Tufte, 2001).\n\n\n14.2.2 Componentes esenciales de un gráfico en ggplot2\nA continuación se describen los principales componentes que conforman la gramática de los gráficos en ggplot2, siguiendo la estructura propuesta por Wilkinson (2005) y adaptada por Wickham (2016):\n\nDatos: Constituyen el insumo fundamental de cualquier gráfico. En R, los datos suelen organizarse en data frames, lo que facilita su manipulación y visualización (Wickham & Grolemund, 2017).\nMapeos estéticos (aesthetics): Son las correspondencias entre las variables de los datos y las propiedades visuales del gráfico, como la posición en los ejes, el color, el tamaño o la forma de los elementos. Definir correctamente los mapeos es crucial para garantizar que la visualización transmita la información deseada (Wickham, 2016).\nGeometrías (geoms): Representan los objetos gráficos que visualizan los datos, como puntos (geom_point()), líneas (geom_line()), barras (geom_bar()), cajas (geom_boxplot()), entre otros. La elección de la geometría depende del tipo de variable y del objetivo del análisis (Cleveland, 1993).\nTransformaciones estadísticas (stats): Permiten aplicar cálculos o resúmenes estadísticos antes de la representación gráfica, como medias, medianas, conteos o ajustes de modelos. Por ejemplo, geom_smooth() puede añadir una línea de tendencia basada en un modelo de regresión (Wickham, 2016).\nEscalas: Definen cómo se traducen los valores de las variables a propiedades visuales, por ejemplo, escalas de color, tamaño o forma. Las escalas permiten adaptar el gráfico a diferentes contextos y audiencias (Wilkinson, 2005).\nSistemas de coordenadas: Determinan el sistema de referencia utilizado para ubicar los elementos gráficos, siendo el cartesiano el más común, aunque también se pueden emplear coordenadas polares u otras transformaciones (Wickham, 2016).\nFacetas: Permiten dividir el gráfico en subgráficos según los valores de una o más variables categóricas, facilitando la comparación visual entre grupos o condiciones experimentales (Wickham, 2016).\nTemas: Controlan la apariencia general del gráfico, incluyendo el tipo y tamaño de fuente, colores de fondo, líneas de cuadrícula y otros elementos estéticos globales. La personalización de temas es fundamental para adaptar los gráficos a los estándares de publicaciones científicas (Tufte, 2001; Wickham, 2016).\n\n\n\n14.2.3 Construcción secuencial y sintaxis básica de gráficos en ggplot2\nLa sintaxis de ggplot2 se basa en la adición secuencial de capas, donde cada componente se incorpora mediante el operador +. Este enfoque modular permite construir gráficos de manera progresiva, añadiendo o modificando elementos según las necesidades del análisis (Wickham, 2016).\nEjemplo con datos simulados:\n\n# Cargar el paquete ggplot2\nlibrary(ggplot2)\n\n# Simulación de datos\nset.seed(123)\nx &lt;- rnorm(100, mean = 10, sd = 2)\ny &lt;- 2 * x + rnorm(100, 0, 3)\ndatos &lt;- data.frame(x = x, y = y)\n\n# Construcción secuencial de un gráfico de dispersión\nggplot(datos, aes(x = x, y = y)) +   # Inicialización y mapeo estético\n  geom_point()                       # Capa de geometría: puntos\n\n\n\n\n\n\n\n\nExplicación:\n\nggplot(datos, aes(x = x, y = y)) inicializa el objeto gráfico y define que la variable x se mapea al eje horizontal y y al eje vertical.\ngeom_point() añade la capa de puntos, representando cada observación como un símbolo en el plano cartesiano.\nEl operador + permite añadir más capas o personalizaciones de manera sencilla y ordenada.\n\n\n\n14.2.4 Ejemplo avanzado: Incorporación de capas y personalización\nLa verdadera potencia de ggplot2 se manifiesta al combinar múltiples capas y personalizaciones en un solo gráfico. A continuación se muestra cómo añadir una línea de tendencia y personalizar etiquetas y temas, siguiendo las recomendaciones de claridad y economía visual de Tufte (2001):\n\nggplot(datos, aes(x = x, y = y)) +\n  # Puntos personalizados\n  geom_point(color = \"navy\", size = 2) +                \n  # Línea de regresión\n  geom_smooth(method = \"lm\", color = \"red\", linetype = \"dashed\") + \n  labs(\n    title = \"Relación entre X e Y\",\n    subtitle = \"Ejemplo con datos simulados\",\n    x = \"Variable X\",\n    y = \"Variable Y\"\n  ) +\n  # Tema profesional y limpio\n  theme_minimal(base_size = 13)                        \n\n\n\n\n\n\n\n\nExplicación:\n\ngeom_smooth(method = \"lm\", ...) añade una línea de regresión lineal con estilo personalizado, facilitando la interpretación de la tendencia general de los datos (Cleveland, 1993).\nlabs() permite definir títulos y etiquetas descriptivas, mejorando la claridad del gráfico.\ntheme_minimal() aplica un tema visual adecuado para presentaciones y publicaciones, siguiendo los principios de economía visual (Tufte, 2001).\n\n\n\n14.2.5 Ventajas del enfoque modular y declarativo\nEl enfoque modular y declarativo de ggplot2 ofrece ventajas significativas para principiantes y usuarios avanzados:\n\nPermite construir gráficos complejos de manera incremental y reproducible, facilitando el aprendizaje progresivo (Wickham, 2016).\nFacilita la modificación y personalización de cada elemento visual, adaptando los gráficos a diferentes audiencias y contextos (Wilkinson, 2005).\nFavorece la claridad y la transparencia en la comunicación de resultados, aspectos esenciales en la investigación científica y la docencia (Cleveland, 1993; Tufte, 2001).",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Visualización de datos con ggplot2</span>"
    ]
  },
  {
    "objectID": "09.3_visualizacion.html#estructura-y-flujo-de-trabajo-para-la-construcción-de-gráficos-en-ggplot2",
    "href": "09.3_visualizacion.html#estructura-y-flujo-de-trabajo-para-la-construcción-de-gráficos-en-ggplot2",
    "title": "14  Visualización de datos con ggplot2",
    "section": "14.3 Estructura y Flujo de Trabajo para la Construcción de Gráficos en ggplot2",
    "text": "14.3 Estructura y Flujo de Trabajo para la Construcción de Gráficos en ggplot2\nLa creación de gráficos profesionales en ggplot2 sigue un flujo de trabajo sistemático y reproducible, que facilita tanto el aprendizaje para principiantes como la producción de visualizaciones de alta calidad para informes y publicaciones científicas (Wickham, 2016; Wilkinson, 2005). Comprender este workflow es esencial para aprovechar al máximo las capacidades del paquete y garantizar la claridad y la coherencia en la comunicación de resultados.\n\n14.3.1 Preparación y organización de los datos\nEl primer paso en cualquier proceso de visualización es la preparación de los datos. En R, los datos suelen organizarse en data frames, lo que permite una manipulación eficiente y una integración directa con ggplot2 (Wickham & Grolemund, 2017). Es fundamental asegurarse de que los datos estén limpios, estructurados y listos para ser mapeados a los elementos visuales del gráfico.\nEjemplo:\n\n# Simulación de datos para el ejemplo\nset.seed(123)\nx &lt;- rnorm(100, mean = 10, sd = 2)\ny &lt;- 2 * x + rnorm(100, 0, 3)\ndatos &lt;- data.frame(x = x, y = y)\n\nExplicación: Se simulan dos variables numéricas (x e y) y se almacenan en un data frame llamado datos, siguiendo las mejores prácticas de organización de datos para análisis estadístico (Wickham & Grolemund, 2017).\n\n\n14.3.2 Inicialización del objeto gráfico y definición de mapeos estéticos\nEl flujo de trabajo (workflow) de ggplot2 comienza con la inicialización del objeto gráfico mediante la función ggplot(), donde se especifica el data frame y los mapeos estéticos principales a través de la función aes(). Los mapeos estéticos determinan cómo se asignan las variables de los datos a las propiedades visuales del gráfico, como los ejes, el color, el tamaño o la forma (Wickham, 2016).\nEjemplo:\n\n# Inicialización del objeto gráfico con mapeos estéticos\ngrafico_base &lt;- ggplot(datos, aes(x = x, y = y))\ngrafico_base\n\n\n\n\n\n\n\n\nExplicación: Se crea un objeto gráfico vacío, donde se define que la variable x se ubicará en el eje horizontal y y en el eje vertical. Este objeto sirve como base para añadir capas adicionales.\n\n\n14.3.3 Adición de geometrías para la representación visual\nEl siguiente paso consiste en añadir una o más geometrías, que determinan cómo se visualizarán los datos. Las geometrías más comunes incluyen puntos (geom_point()), líneas (geom_line()), barras (geom_bar()) y cajas (geom_boxplot()). Cada geometría puede personalizarse mediante argumentos adicionales, como color, tamaño o forma (Cleveland, 1993; Wickham, 2016).\nEjemplo:\n\n# Adición de una geometría de puntos\ngrafico_dispersion &lt;- grafico_base + geom_point()\ngrafico_dispersion\n\n\n\n\n\n\n\n\nExplicación: geom_point() añade una capa de puntos, representando cada observación como un símbolo en el plano cartesiano. El operador + permite añadir más capas o personalizaciones de manera ordenada y progresiva.\n\n\n14.3.4 Personalización de etiquetas, títulos y leyendas\nPara mejorar la claridad y la interpretación del gráfico, es recomendable añadir títulos, subtítulos, etiquetas a los ejes y leyendas mediante la función labs(). Una correcta rotulación facilita la comunicación de los resultados y evita ambigüedades (Tufte, 2001).\nEjemplo:\n\n# Personalización de etiquetas y títulos\ngrafico_etiquetado &lt;- grafico_dispersion +\n  labs(\n    title = \"Gráfico de dispersión básico\",\n    subtitle = \"Ejemplo con datos simulados\",\n    x = \"Variable X\",\n    y = \"Variable Y\"\n  )\ngrafico_etiquetado\n\n\n\n\n\n\n\n\nExplicación: labs() permite definir el título principal, el subtítulo y las etiquetas de los ejes, mejorando la presentación y la comprensión del gráfico.\n\n\n14.3.5 Aplicación de temas y ajustes estéticos\nGgplot2 ofrece una variedad de temas predefinidos que modifican la apariencia general del gráfico, adaptándolo a diferentes contextos y estándares de publicación. Los temas controlan aspectos como el fondo, las fuentes, las líneas de cuadrícula y los colores (Wickham, 2016; Tufte, 2001).\nEjemplo:\n\n# Aplicación de un tema profesional\ngrafico_final &lt;- grafico_etiquetado +\n  theme_minimal(base_size = 13)\ngrafico_final\n\n\n\n\n\n\n\n\nExplicación: theme_minimal() aplica un estilo visual limpio y profesional, adecuado para presentaciones y publicaciones científicas. El argumento base_size ajusta el tamaño base de las fuentes, facilitando la lectura.\n\n\n14.3.6 Exportación y reutilización del gráfico\nUna vez finalizado el gráfico, es posible exportarlo a diferentes formatos (PNG, PDF, SVG) utilizando funciones como ggsave(), lo que facilita su inclusión en documentos formales, reportes técnicos y publicaciones científicas (Wickham, 2016).\nEjemplo:\n\n# Exportar el gráfico a un archivo PNG\nggsave(\"grafico_dispersión.png\", \n       plot = grafico_final, \n       width = 6, height = 4, dpi = 300)\n\nExplicación: ggsave() permite guardar el gráfico en un archivo con la resolución y dimensiones especificadas, asegurando la calidad necesaria para su uso profesional.\n\n\n14.3.7 Resumen del flujo de trabajo en ggplot2\nEl flujo de trabajo recomendado para la construcción de gráficos en ggplot2 puede resumirse en los siguientes pasos:\n\nPreparar y organizar los datos en un formato adecuado (data frame).\nInicializar el objeto gráfico con los mapeos estéticos principales.\nAñadir geometrías para representar los datos visualmente.\nPersonalizar etiquetas, títulos y leyendas para mejorar la claridad.\nAplicar temas y ajustes estéticos para adaptar el gráfico a los estándares profesionales.\nExportar y reutilizar el gráfico en diferentes formatos según las necesidades del proyecto.\n\nEste flujo de trabajo modular y progresivo no solo facilita el aprendizaje para principiantes, sino que también garantiza la reproducibilidad, la claridad y la calidad en la comunicación de resultados estadísticos (Wickham, 2016; Wilkinson, 2005; Tufte, 2001).",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Visualización de datos con ggplot2</span>"
    ]
  },
  {
    "objectID": "09.3_visualizacion.html#creación-de-gráficos-exploratorios-y-descriptivos-en-ggplot2",
    "href": "09.3_visualizacion.html#creación-de-gráficos-exploratorios-y-descriptivos-en-ggplot2",
    "title": "14  Visualización de datos con ggplot2",
    "section": "14.4 Creación de gráficos exploratorios y descriptivos en ggplot2",
    "text": "14.4 Creación de gráficos exploratorios y descriptivos en ggplot2\nEl paquete ggplot2 proporciona una sintaxis coherente y modular para la creación de diferentes tipos de gráficos estadísticos en R. Cada visualización requiere una estructura específica de datos, generalmente en formato data frame, y utiliza funciones geométricas particulares que determinan cómo se representarán los datos. La construcción de estos gráficos sigue el workflow profesional establecido, donde primero se preparan los datos, luego se inicializa el objeto gráfico con ggplot(), se añaden las geometrías correspondientes y finalmente se personalizan los elementos visuales según sea necesario (Wickham, 2016).\n\n14.4.1 Gráficos de barras con geom_bar()\nLa función geom_bar() es la geometría principal para crear gráficos de barras a partir de variables categóricas. Esta función cuenta automáticamente las frecuencias de cada categoría y las representa como barras verticales.\n\n# Simulación de datos categóricos\nset.seed(123)\ngrupo &lt;- sample(c(\"A\", \"B\", \"C\"), size = 200, replace = TRUE)\ndatos_cat &lt;- data.frame(grupo = grupo)\n\n# Gráfico de barras\nggplot(datos_cat, aes(x = grupo)) +\n  geom_bar()\n\n\n\n\n\n\n\n\nExplicación:\n\nSe crea un data frame con una variable categórica (grupo).\nggplot(datos_cat, aes(x = grupo)) inicializa el gráfico mapeando la variable al eje X.\ngeom_bar() añade las barras, calculando automáticamente las frecuencias.\n\n\n\n14.4.2 Histogramas con geom_histogram()\nLa función geom_histogram() genera histogramas para variables numéricas continuas, dividiendo los datos en intervalos (bins) y contando la frecuencia en cada uno.\n\n# Simulación de datos numéricos\nset.seed(123)\nvalores &lt;- rnorm(200, mean = 70, sd = 10)\ndatos_hist &lt;- data.frame(valores = valores)\n\n# Histograma\nggplot(datos_hist, aes(x = valores)) +\n  geom_histogram(bins = 15)\n\n\n\n\n\n\n\n\nExplicación:\n\nSe crea un data frame con una variable numérica (valores).\nggplot(datos_hist, aes(x = valores)) mapea la variable al eje X.\ngeom_histogram() crea el histograma, especificando el número de bins deseado.\n\n\n\n14.4.3 Gráficos de dispersión con geom_point()\nLa función geom_point() crea gráficos de dispersión para analizar la relación entre dos variables numéricas, representando cada observación como un punto en el plano cartesiano.\n\n# Simulación de datos correlacionados\nset.seed(123)\nx &lt;- rnorm(100, mean = 10, sd = 2)\ny &lt;- 2 * x + rnorm(100, 0, 3)\ndatos_disp &lt;- data.frame(x = x, y = y)\n\n# Gráfico de dispersión\nggplot(datos_disp, aes(x = x, y = y)) +\n  geom_point()\n\n\n\n\n\n\n\n\nExplicación:\n\nSe crea un data frame con dos variables numéricas (x e y).\nggplot(datos_disp, aes(x = x, y = y)) mapea las variables a los ejes X e Y.\ngeom_point() representa cada par de valores como un punto.\n\n\n\n14.4.4 Boxplots con geom_boxplot()\nLa función geom_boxplot() construye diagramas de caja para comparar la distribución de una variable numérica entre diferentes grupos o categorías.\n\n# Simulación de datos para dos grupos\nset.seed(123)\ngrupo &lt;- factor(rep(c(\"Control\", \"Tratamiento\"), each = 100))\nvalores &lt;- c(rnorm(100, 70, 8), rnorm(100, 75, 10))\ndatos_box &lt;- data.frame(grupo = grupo, valores = valores)\n\n# Boxplot\nggplot(datos_box, aes(x = grupo, y = valores)) +\n  geom_boxplot()\n\n\n\n\n\n\n\n\nExplicación:\n\nSe crea un data frame con una variable categórica (grupo) y una numérica (valores).\nggplot(datos_box, aes(x = grupo, y = valores)) mapea el grupo al eje X y los valores al eje Y.\ngeom_boxplot() genera los diagramas de caja para cada grupo.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Visualización de datos con ggplot2</span>"
    ]
  },
  {
    "objectID": "09.3_visualizacion.html#uso-de-facetas-para-comparación-de-grupos-en-ggplot2",
    "href": "09.3_visualizacion.html#uso-de-facetas-para-comparación-de-grupos-en-ggplot2",
    "title": "14  Visualización de datos con ggplot2",
    "section": "14.6 Uso de facetas para comparación de grupos en ggplot2",
    "text": "14.6 Uso de facetas para comparación de grupos en ggplot2\nLas facetas en ggplot2 permiten dividir un gráfico en múltiples paneles según los valores de una o más variables categóricas, facilitando la comparación visual entre subgrupos o condiciones experimentales. Esta funcionalidad es especialmente útil en el análisis exploratorio de datos, ya que permite identificar patrones, diferencias y tendencias específicas en cada grupo sin perder la coherencia visual y la escala común entre paneles (Wickham, 2016).\nEl flujo de trabajo para el uso de facetas en ggplot2 consiste en preparar los datos con las variables categóricas de interés, construir el gráfico base y añadir la capa de facetas mediante las funciones facet_wrap() o facet_grid(). Estas funciones ofrecen flexibilidad para organizar los paneles en filas, columnas o matrices, y permiten personalizar etiquetas, escalas y disposición de los subgráficos.\n\n14.6.1 Facetado simple con facet_wrap()\nLa función facet_wrap() divide el gráfico en paneles independientes según los valores de una sola variable categórica. Los paneles se organizan automáticamente en una cuadrícula, lo que resulta útil para comparar múltiples niveles de una variable.\n\n# Simulación de datos con una variable de facetas\nset.seed(123)\ngrupo &lt;- sample(c(\"A\", \"B\", \"C\"), size = 150, replace = TRUE)\ncategoria &lt;- sample(c(\"X\", \"Y\"), size = 150, replace = TRUE)\nvalor &lt;- rnorm(150, mean = 50, sd = 10)\ndatos_facet &lt;- data.frame(grupo = grupo, categoria = categoria, valor = valor)\n\n# Gráfico de dispersión facetado por grupo\nggplot(datos_facet, aes(x = categoria, y = valor)) +\n  geom_boxplot(fill = \"lightblue\") +\n  facet_wrap(~ grupo)\n\n\n\n\n\n\n\n\nExplicación\n\nSe crea un data frame con una variable categórica para las facetas (grupo), una variable categórica para el eje X (categoria) y una variable numérica (valor).\nggplot(datos_facet, aes(x = categoria, y = valor)) inicializa el gráfico.\ngeom_boxplot() representa los datos como diagramas de caja.\nfacet_wrap(~ grupo) divide el gráfico en paneles independientes para cada nivel de la variable grupo.\n\nEl argumento nrow o ncol en facet_wrap() permite controlar el número de filas o columnas de la cuadrícula de paneles.\n\n\n14.6.2 Facetado múltiple con facet_grid()\nLa función facet_grid() permite crear una matriz de paneles utilizando dos variables categóricas, una para las filas y otra para las columnas. Esta organización es ideal para comparar simultáneamente los efectos de dos factores sobre la variable de interés.\n\n# Gráfico de dispersión facetado por grupo y categoría\nggplot(datos_facet, aes(x = valor)) +\n  geom_histogram(bins = 10, fill = \"salmon\", color = \"white\") +\n  facet_grid(grupo ~ categoria)\n\n\n\n\n\n\n\n\nExplicación:\n\nSe utiliza el mismo data frame con dos variables categóricas (grupo y categoria).\ngeom_histogram() representa la distribución de la variable numérica.\nfacet_grid(grupo ~ categoria) crea una matriz de paneles, donde las filas corresponden a los niveles de grupo y las columnas a los niveles de categoria.\n\nEl uso de facet_grid() es especialmente útil cuando se desea analizar la interacción entre dos factores y observar cómo varía la distribución de la variable numérica en cada combinación de niveles.\n\n\n14.6.3 Personalización de facetas\nggplot2 permite personalizar las etiquetas de los paneles, la disposición de los subgráficos y la independencia de las escalas entre paneles. Los argumentos labeller, scales y strip.position en las funciones de facetas ofrecen un control detallado sobre la presentación final.\n\n# Personalización avanzada de facetas\nggplot(datos_facet, aes(x = categoria, y = valor, fill = categoria)) +\n  geom_boxplot() +\n  facet_wrap(~ grupo, nrow = 1, labeller = label_both,\n             strip.position = \"bottom\", scales = \"free_y\") +\n  theme(strip.background = element_rect(fill = \"gray90\"),\n        strip.text = element_text(face = \"bold\", color = \"navy\"))\n\n\n\n\n\n\n\n\nExplicación:\n\nnrow = 1 organiza los paneles en una sola fila.\nlabeller = label_both muestra el nombre de la variable y el valor en la etiqueta del panel.\nstrip.position = \"bottom\" coloca las etiquetas de los paneles en la parte inferior.\nscales = \"free_y\" permite que cada panel tenga su propia escala en el eje Y.\ntheme() personaliza el fondo y el texto de las etiquetas de los paneles.\n\nLa personalización de facetas es clave para mejorar la legibilidad y la interpretación de los gráficos comparativos, especialmente cuando se trabaja con conjuntos de datos complejos o con múltiples niveles de agrupamiento.\nEl uso de facetas en ggplot2, mediante facet_wrap() y facet_grid(), es una herramienta poderosa para la exploración visual y la comunicación de resultados en análisis estadístico, permitiendo comparar grupos de manera clara, eficiente y reproducible (Wickham, 2016).",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Visualización de datos con ggplot2</span>"
    ]
  },
  {
    "objectID": "09.3_visualizacion.html#material-de-apoyo-y-paquetes-adicionales-recomendados",
    "href": "09.3_visualizacion.html#material-de-apoyo-y-paquetes-adicionales-recomendados",
    "title": "14  Visualización de datos con ggplot2",
    "section": "14.8 Material de apoyo y paquetes adicionales recomendados",
    "text": "14.8 Material de apoyo y paquetes adicionales recomendados\nEl aprendizaje y perfeccionamiento en la visualización de datos con ggplot2 puede potenciarse mediante el acceso a recursos especializados y la integración de paquetes adicionales que amplían las capacidades del entorno gráfico en R. A continuación, se presentan recomendaciones de materiales de apoyo y herramientas complementarias, útiles tanto para quienes inician como para usuarios avanzados.\n\n14.8.1 Recursos para profundizar en ggplot2\nEl ecosistema de R y ggplot2 cuenta con una amplia variedad de materiales didácticos, manuales y cursos en línea que facilitan la adquisición de competencias en visualización de datos:\n\nDocumentación oficial de ggplot2: El sitio oficial del paquete ofrece una referencia completa de funciones, ejemplos y guías de uso, actualizadas conforme a las nuevas versiones (Wickham, 2016). La documentación es un recurso esencial para comprender a fondo las capacidades del paquete. https://ggplot2.tidyverse.org/\nLibros especializados: ggplot2: Elegant Graphics for Data Analysis (Wickham, 2016) es la obra de referencia para comprender la filosofía y el uso avanzado del paquete. Este libro proporciona una base sólida para la creación de gráficos complejos y personalizados.\nCursos y tutoriales en línea: Plataformas como DataCamp, Coursera y edX ofrecen cursos interactivos sobre visualización de datos con R y ggplot2, que incluyen ejercicios prácticos y proyectos aplicados (Healy, 2018).\n\n\n\n14.8.2 Paquetes adicionales recomendados\nEl uso de paquetes complementarios puede optimizar tareas específicas y ampliar las posibilidades de personalización y análisis gráfico:\n\nDataExplorer: Facilita la exploración inicial de datos mediante la generación automática de reportes gráficos y estadísticos, permitiendo identificar patrones, valores atípicos y distribuciones de manera eficiente (Cui, 2020). Este paquete es especialmente útil para el análisis exploratorio de datos.\nggthemes: Proporciona una colección de temas predefinidos inspirados en estilos de publicaciones y medios reconocidos, lo que permite adaptar la estética de los gráficos a diferentes contextos profesionales. Los temas predefinidos pueden ahorrar tiempo y mejorar la apariencia de los gráficos.\nplotly: Permite transformar gráficos estáticos de ggplot2 en visualizaciones interactivas, ideales para presentaciones y análisis exploratorio en entornos web. La interactividad puede mejorar la comunicación de los resultados.\nggrepel: Mejora la legibilidad de los gráficos al evitar el solapamiento de etiquetas, especialmente útil en gráficos de dispersión con muchos puntos o etiquetas extensas. Evitar el solapamiento de etiquetas es crucial para la claridad visual.\nviridis: Ofrece paletas de colores perceptualmente uniformes y accesibles para personas con daltonismo, recomendadas para mapas de calor y representaciones de datos continuos. El uso de paletas de colores accesibles es importante para la inclusión.\n\n\n\n14.8.3 Consideraciones finales\nLa visualización de datos es un campo en constante evolución. Se recomienda explorar de manera continua nuevas herramientas, técnicas y buenas prácticas, así como participar en comunidades y foros especializados para resolver dudas y compartir experiencias. La integración de ggplot2 con otros paquetes del ecosistema tidyverse y herramientas adicionales permite abordar desafíos complejos de visualización de manera eficiente, reproducible y profesional (Wickham et al., 2019).",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Visualización de datos con ggplot2</span>"
    ]
  }
]